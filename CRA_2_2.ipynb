{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Convolution1D, Conv2D, MaxPooling2D, MaxPooling1D, LSTM, Embedding\n",
    "from keras.optimizers import SGD\n",
    "import keras.backend as K\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# config_proto = tf.ConfigProto(log_device_placement=0,allow_soft_placement=0)\n",
    "# config_proto.gpu_options.allow_growth = True\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../../20_Galaxy/ANGRY/2HXT-train.npy')\n",
    "Y_train = np.load('../../20_Galaxy/ANGRY/2HXT-train_label.npy')\n",
    "\n",
    "X_valid = np.load('../../20_Galaxy/ANGRY/2HXT-val.npy')\n",
    "Y_valid = np.load('../../20_Galaxy/ANGRY/2HXT-val_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "one_hot = preprocessing.OneHotEncoder(sparse = False)\n",
    "y_train = one_hot.fit_transform(Y_train)\n",
    "y_valid = one_hot.fit_transform(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"attention_layer_3/div:0\", shape=(?, 169, 32), dtype=float32)\n",
      "Tensor(\"lstm_3/transpose_1:0\", shape=(?, ?, 32), dtype=float32)\n",
      "Tensor(\"attention_layer_3/mul:0\", shape=(?, 169, 32), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 174s 11ms/step - loss: 0.6915 - acc: 0.5277 - val_loss: 0.6919 - val_acc: 0.5085\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 167s 11ms/step - loss: 0.6664 - acc: 0.5752 - val_loss: 0.6130 - val_acc: 0.6810\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.5974 - acc: 0.6704 - val_loss: 0.5664 - val_acc: 0.7670\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 169s 11ms/step - loss: 0.5222 - acc: 0.7369 - val_loss: 0.5384 - val_acc: 0.8015\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 170s 11ms/step - loss: 0.4806 - acc: 0.7698 - val_loss: 0.5172 - val_acc: 0.8263\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 178s 11ms/step - loss: 0.4603 - acc: 0.7836 - val_loss: 0.5050 - val_acc: 0.8255\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 174s 11ms/step - loss: 0.4404 - acc: 0.7949 - val_loss: 0.4675 - val_acc: 0.8605\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.4247 - acc: 0.8051 - val_loss: 0.4949 - val_acc: 0.8310\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 163s 10ms/step - loss: 0.4127 - acc: 0.8111 - val_loss: 0.4663 - val_acc: 0.8528\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 167s 11ms/step - loss: 0.4195 - acc: 0.8063 - val_loss: 0.4578 - val_acc: 0.8680\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 164s 10ms/step - loss: 0.4046 - acc: 0.8157 - val_loss: 0.5073 - val_acc: 0.8273\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.3982 - acc: 0.8211 - val_loss: 0.4671 - val_acc: 0.8627\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 169s 11ms/step - loss: 0.3904 - acc: 0.8255 - val_loss: 0.4521 - val_acc: 0.8652\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 171s 11ms/step - loss: 0.3876 - acc: 0.8272 - val_loss: 0.4455 - val_acc: 0.8668\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.3846 - acc: 0.8283 - val_loss: 0.4470 - val_acc: 0.8665\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 169s 11ms/step - loss: 0.3819 - acc: 0.8317 - val_loss: 0.4342 - val_acc: 0.8772\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.3767 - acc: 0.8336 - val_loss: 0.4310 - val_acc: 0.8680\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 166s 11ms/step - loss: 0.3764 - acc: 0.8323 - val_loss: 0.4512 - val_acc: 0.8668\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 164s 10ms/step - loss: 0.3737 - acc: 0.8343 - val_loss: 0.4754 - val_acc: 0.8543\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 166s 11ms/step - loss: 0.3709 - acc: 0.8389 - val_loss: 0.4187 - val_acc: 0.8822\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 164s 10ms/step - loss: 0.3685 - acc: 0.8406 - val_loss: 0.4255 - val_acc: 0.8685\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 165s 11ms/step - loss: 0.3680 - acc: 0.8389 - val_loss: 0.4245 - val_acc: 0.8755\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 169s 11ms/step - loss: 0.3676 - acc: 0.8403 - val_loss: 0.4625 - val_acc: 0.8622\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 166s 11ms/step - loss: 0.3631 - acc: 0.8424 - val_loss: 0.4141 - val_acc: 0.8835\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 166s 11ms/step - loss: 0.3645 - acc: 0.8423 - val_loss: 0.4139 - val_acc: 0.8740\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 166s 11ms/step - loss: 0.3602 - acc: 0.8441 - val_loss: 0.4165 - val_acc: 0.8848\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.3579 - acc: 0.8446 - val_loss: 0.4058 - val_acc: 0.8725\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.3555 - acc: 0.8458 - val_loss: 0.4169 - val_acc: 0.8758\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.3557 - acc: 0.8469 - val_loss: 0.4110 - val_acc: 0.8755\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 170s 11ms/step - loss: 0.3555 - acc: 0.8446 - val_loss: 0.4575 - val_acc: 0.8465\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.3558 - acc: 0.8482 - val_loss: 0.4109 - val_acc: 0.8818\n",
      "Epoch 32/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.3511 - acc: 0.8490 - val_loss: 0.4126 - val_acc: 0.8828\n",
      "Epoch 00032: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe52a84e090>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "batch_size = 32\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2), input_shape=(12* 3640,1)))\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(7)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               batch_input_shape=(batch_size, 12, 3640)))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Attention_layer())\n",
    "\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])\n",
    "# score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "# print model.predict(x_test)\n",
    "# print K.eval(categorical_accuracy(y_test, model.predict(x_test)))\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 17s 1ms/step - loss: 0.6223 - acc: 0.6369 - val_loss: 0.5352 - val_acc: 0.8115\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 15s 975us/step - loss: 0.5028 - acc: 0.7608 - val_loss: 0.5246 - val_acc: 0.8615\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 15s 964us/step - loss: 0.4616 - acc: 0.7839 - val_loss: 0.5110 - val_acc: 0.8490\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 15s 966us/step - loss: 0.4479 - acc: 0.7934 - val_loss: 0.5128 - val_acc: 0.8480\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 16s 1ms/step - loss: 0.4324 - acc: 0.8032 - val_loss: 0.4536 - val_acc: 0.8635\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 16s 992us/step - loss: 0.4127 - acc: 0.8159 - val_loss: 0.5623 - val_acc: 0.8315\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 16s 993us/step - loss: 0.4098 - acc: 0.8209 - val_loss: 0.4376 - val_acc: 0.8758\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 16s 1ms/step - loss: 0.3945 - acc: 0.8275 - val_loss: 0.3518 - val_acc: 0.8810\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 16s 994us/step - loss: 0.3897 - acc: 0.8320 - val_loss: 0.3206 - val_acc: 0.8835\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 15s 982us/step - loss: 0.3771 - acc: 0.8366 - val_loss: 0.5189 - val_acc: 0.8162\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 15s 971us/step - loss: 0.3755 - acc: 0.8382 - val_loss: 0.2790 - val_acc: 0.8942\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 15s 967us/step - loss: 0.3689 - acc: 0.8409 - val_loss: 0.3099 - val_acc: 0.9023\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 15s 982us/step - loss: 0.3759 - acc: 0.8344 - val_loss: 0.2985 - val_acc: 0.8950\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 16s 1ms/step - loss: 0.3634 - acc: 0.8421 - val_loss: 0.3224 - val_acc: 0.8995\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 16s 992us/step - loss: 0.3679 - acc: 0.8416 - val_loss: 0.5246 - val_acc: 0.7702\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 16s 991us/step - loss: 0.3596 - acc: 0.8447 - val_loss: 0.2870 - val_acc: 0.8955\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe5297d8c90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "batch_size = 32\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2), input_shape=(12* 3640,1)))\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(7)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(LSTM(32, return_sequences=True,\n",
    "#                batch_input_shape=(batch_size, 12, 3640)))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(Attention_layer())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])\n",
    "# score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "# print model.predict(x_test)\n",
    "# print K.eval(categorical_accuracy(y_test, model.predict(x_test)))\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 167s 11ms/step - loss: 0.5899 - acc: 0.6572 - val_loss: 0.7344 - val_acc: 0.7270\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 165s 11ms/step - loss: 0.4692 - acc: 0.7783 - val_loss: 0.5785 - val_acc: 0.8207\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 164s 10ms/step - loss: 0.4364 - acc: 0.7993 - val_loss: 0.5373 - val_acc: 0.8642\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 163s 10ms/step - loss: 0.4078 - acc: 0.8170 - val_loss: 0.4429 - val_acc: 0.8685\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 163s 10ms/step - loss: 0.3916 - acc: 0.8242 - val_loss: 0.4277 - val_acc: 0.8778\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 163s 10ms/step - loss: 0.3763 - acc: 0.8354 - val_loss: 0.2913 - val_acc: 0.8955\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 163s 10ms/step - loss: 0.3700 - acc: 0.8377 - val_loss: 0.3351 - val_acc: 0.8980\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 164s 10ms/step - loss: 0.3686 - acc: 0.8384 - val_loss: 0.2791 - val_acc: 0.8972\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 163s 10ms/step - loss: 0.3587 - acc: 0.8447 - val_loss: 0.3528 - val_acc: 0.8678\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 164s 10ms/step - loss: 0.3553 - acc: 0.8475 - val_loss: 0.3111 - val_acc: 0.8888\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 164s 10ms/step - loss: 0.3535 - acc: 0.8474 - val_loss: 0.2862 - val_acc: 0.8955\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 164s 10ms/step - loss: 0.3524 - acc: 0.8497 - val_loss: 0.3597 - val_acc: 0.8885\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 164s 10ms/step - loss: 0.3480 - acc: 0.8512 - val_loss: 0.2630 - val_acc: 0.9040\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 164s 10ms/step - loss: 0.3436 - acc: 0.8519 - val_loss: 0.2685 - val_acc: 0.9020\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 163s 10ms/step - loss: 0.3441 - acc: 0.8525 - val_loss: 0.3288 - val_acc: 0.8908\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 165s 11ms/step - loss: 0.3420 - acc: 0.8511 - val_loss: 0.2662 - val_acc: 0.9052\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 166s 11ms/step - loss: 0.3405 - acc: 0.8542 - val_loss: 0.3768 - val_acc: 0.8918\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 164s 10ms/step - loss: 0.3417 - acc: 0.8539 - val_loss: 0.2733 - val_acc: 0.9008\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe4a40d3250>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "batch_size = 32\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2), input_shape=(12* 3640,1)))\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(7)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               batch_input_shape=(batch_size, 12, 3640)))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(Attention_layer())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])\n",
    "# score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "# print model.predict(x_test)\n",
    "# print K.eval(categorical_accuracy(y_test, model.predict(x_test)))\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 171s 11ms/step - loss: 0.5849 - acc: 0.6740 - val_loss: 0.4300 - val_acc: 0.8402\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 170s 11ms/step - loss: 0.4720 - acc: 0.7771 - val_loss: 0.4378 - val_acc: 0.8400\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 171s 11ms/step - loss: 0.4386 - acc: 0.7989 - val_loss: 0.4555 - val_acc: 0.8748\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 169s 11ms/step - loss: 0.4243 - acc: 0.8101 - val_loss: 0.4210 - val_acc: 0.8520\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.4155 - acc: 0.8138 - val_loss: 0.4082 - val_acc: 0.8765\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.3946 - acc: 0.8218 - val_loss: 0.4116 - val_acc: 0.8820\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 167s 11ms/step - loss: 0.3828 - acc: 0.8309 - val_loss: 0.3807 - val_acc: 0.8778\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 173s 11ms/step - loss: 0.3733 - acc: 0.8366 - val_loss: 0.3049 - val_acc: 0.8960\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 174s 11ms/step - loss: 0.3714 - acc: 0.8373 - val_loss: 0.3340 - val_acc: 0.8925\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 167s 11ms/step - loss: 0.3625 - acc: 0.8430 - val_loss: 0.3671 - val_acc: 0.8892\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.3572 - acc: 0.8465 - val_loss: 0.4181 - val_acc: 0.8930\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 167s 11ms/step - loss: 0.3573 - acc: 0.8451 - val_loss: 0.3471 - val_acc: 0.8982\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 165s 11ms/step - loss: 0.3503 - acc: 0.8479 - val_loss: 0.3261 - val_acc: 0.9090\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe2ca3aea10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "batch_size = 32\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2), input_shape=(12* 3640,1)))\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(7)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True,\n",
    "               batch_input_shape=(batch_size, 12, 3640)))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(Attention_layer())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])\n",
    "# score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "# print model.predict(x_test)\n",
    "# print K.eval(categorical_accuracy(y_test, model.predict(x_test)))\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 164s 10ms/step - loss: 0.6527 - acc: 0.5926 - val_loss: 0.5284 - val_acc: 0.7395\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 148s 9ms/step - loss: 0.5232 - acc: 0.7337 - val_loss: 0.4876 - val_acc: 0.8300\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 146s 9ms/step - loss: 0.4883 - acc: 0.7598 - val_loss: 0.4972 - val_acc: 0.8432\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 146s 9ms/step - loss: 0.4668 - acc: 0.7779 - val_loss: 0.5028 - val_acc: 0.8552\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 147s 9ms/step - loss: 0.4465 - acc: 0.7904 - val_loss: 0.5435 - val_acc: 0.8625\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 149s 9ms/step - loss: 0.4366 - acc: 0.7982 - val_loss: 0.5185 - val_acc: 0.8660\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.4252 - acc: 0.8038 - val_loss: 0.4872 - val_acc: 0.8680\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 152s 10ms/step - loss: 0.4210 - acc: 0.8072 - val_loss: 0.5485 - val_acc: 0.8640\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 153s 10ms/step - loss: 0.4110 - acc: 0.8103 - val_loss: 0.4897 - val_acc: 0.8752\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 145s 9ms/step - loss: 0.4014 - acc: 0.8198 - val_loss: 0.5012 - val_acc: 0.8698\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3879 - acc: 0.8265 - val_loss: 0.4656 - val_acc: 0.8832\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 144s 9ms/step - loss: 0.3819 - acc: 0.8334 - val_loss: 0.4777 - val_acc: 0.8820\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3718 - acc: 0.8394 - val_loss: 0.4617 - val_acc: 0.8665\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 152s 10ms/step - loss: 0.3689 - acc: 0.8397 - val_loss: 0.4215 - val_acc: 0.8838\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 153s 10ms/step - loss: 0.3668 - acc: 0.8394 - val_loss: 0.4418 - val_acc: 0.8715\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 150s 10ms/step - loss: 0.3595 - acc: 0.8462 - val_loss: 0.3968 - val_acc: 0.8820\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 154s 10ms/step - loss: 0.3559 - acc: 0.8472 - val_loss: 0.4009 - val_acc: 0.8825\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.3557 - acc: 0.8481 - val_loss: 0.3696 - val_acc: 0.8845\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.3550 - acc: 0.8485 - val_loss: 0.3914 - val_acc: 0.8858\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3524 - acc: 0.8469 - val_loss: 0.3672 - val_acc: 0.8928\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3506 - acc: 0.8484 - val_loss: 0.3485 - val_acc: 0.8930\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.3463 - acc: 0.8512 - val_loss: 0.3973 - val_acc: 0.8802\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 141s 9ms/step - loss: 0.3469 - acc: 0.8506 - val_loss: 0.3354 - val_acc: 0.8962\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 141s 9ms/step - loss: 0.3448 - acc: 0.8518 - val_loss: 0.3167 - val_acc: 0.8853\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 145s 9ms/step - loss: 0.3422 - acc: 0.8529 - val_loss: 0.3406 - val_acc: 0.8892\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 150s 10ms/step - loss: 0.3397 - acc: 0.8550 - val_loss: 0.3379 - val_acc: 0.8958\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 150s 10ms/step - loss: 0.3380 - acc: 0.8573 - val_loss: 0.3638 - val_acc: 0.8870\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 149s 10ms/step - loss: 0.3379 - acc: 0.8542 - val_loss: 0.3117 - val_acc: 0.8978\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 149s 10ms/step - loss: 0.3360 - acc: 0.8555 - val_loss: 0.3407 - val_acc: 0.8922\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 154s 10ms/step - loss: 0.3382 - acc: 0.8555 - val_loss: 0.3473 - val_acc: 0.8868\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 147s 9ms/step - loss: 0.3332 - acc: 0.8593 - val_loss: 0.3454 - val_acc: 0.8938\n",
      "Epoch 32/50\n",
      "15658/15658 [==============================] - 148s 9ms/step - loss: 0.3328 - acc: 0.8558 - val_loss: 0.3275 - val_acc: 0.8940\n",
      "Epoch 33/50\n",
      "15658/15658 [==============================] - 148s 9ms/step - loss: 0.3323 - acc: 0.8594 - val_loss: 0.3325 - val_acc: 0.8952\n",
      "Epoch 34/50\n",
      "15658/15658 [==============================] - 150s 10ms/step - loss: 0.3331 - acc: 0.8571 - val_loss: 0.2947 - val_acc: 0.8982\n",
      "Epoch 35/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.3283 - acc: 0.8595 - val_loss: 0.3216 - val_acc: 0.8938\n",
      "Epoch 36/50\n",
      "15658/15658 [==============================] - 147s 9ms/step - loss: 0.3334 - acc: 0.8560 - val_loss: 0.3336 - val_acc: 0.8905\n",
      "Epoch 37/50\n",
      "15658/15658 [==============================] - 147s 9ms/step - loss: 0.3295 - acc: 0.8573 - val_loss: 0.3030 - val_acc: 0.8940\n",
      "Epoch 38/50\n",
      "15658/15658 [==============================] - 149s 9ms/step - loss: 0.3272 - acc: 0.8579 - val_loss: 0.3351 - val_acc: 0.8945\n",
      "Epoch 39/50\n",
      "15658/15658 [==============================] - 148s 9ms/step - loss: 0.3256 - acc: 0.8615 - val_loss: 0.3192 - val_acc: 0.9020\n",
      "Epoch 40/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.3257 - acc: 0.8613 - val_loss: 0.3152 - val_acc: 0.8995\n",
      "Epoch 41/50\n",
      "15658/15658 [==============================] - 147s 9ms/step - loss: 0.3241 - acc: 0.8601 - val_loss: 0.3329 - val_acc: 0.8938\n",
      "Epoch 42/50\n",
      "15658/15658 [==============================] - 146s 9ms/step - loss: 0.3273 - acc: 0.8585 - val_loss: 0.3306 - val_acc: 0.8925\n",
      "Epoch 43/50\n",
      "15658/15658 [==============================] - 147s 9ms/step - loss: 0.3260 - acc: 0.8612 - val_loss: 0.2984 - val_acc: 0.9020\n",
      "Epoch 44/50\n",
      "15658/15658 [==============================] - 147s 9ms/step - loss: 0.3250 - acc: 0.8634 - val_loss: 0.3109 - val_acc: 0.8995\n",
      "Epoch 00044: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe2b57beb10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "batch_size = 64\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2), input_shape=(12* 3640,1)))\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(7)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "# model.add(LSTM(64, return_sequences=True,\n",
    "#                batch_input_shape=(batch_size, 12, 3640)))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(Attention_layer())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])\n",
    "# score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "# print model.predict(x_test)\n",
    "# print K.eval(categorical_accuracy(y_test, model.predict(x_test)))\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 166s 11ms/step - loss: 0.6481 - acc: 0.6021 - val_loss: 0.5342 - val_acc: 0.7360\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 159s 10ms/step - loss: 0.5531 - acc: 0.7091 - val_loss: 0.4947 - val_acc: 0.8297\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 157s 10ms/step - loss: 0.4907 - acc: 0.7572 - val_loss: 0.4781 - val_acc: 0.8425\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 157s 10ms/step - loss: 0.4515 - acc: 0.7841 - val_loss: 0.5140 - val_acc: 0.8678\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 162s 10ms/step - loss: 0.4293 - acc: 0.8023 - val_loss: 0.5263 - val_acc: 0.8737\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 153s 10ms/step - loss: 0.4170 - acc: 0.8095 - val_loss: 0.5568 - val_acc: 0.8792\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 149s 10ms/step - loss: 0.4008 - acc: 0.8169 - val_loss: 0.5843 - val_acc: 0.8848\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.3819 - acc: 0.8298 - val_loss: 0.5205 - val_acc: 0.8912\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 152s 10ms/step - loss: 0.3795 - acc: 0.8326 - val_loss: 0.5061 - val_acc: 0.8673\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.3725 - acc: 0.8389 - val_loss: 0.4677 - val_acc: 0.8768\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.3683 - acc: 0.8407 - val_loss: 0.4438 - val_acc: 0.8620\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.3637 - acc: 0.8417 - val_loss: 0.4362 - val_acc: 0.8798\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 156s 10ms/step - loss: 0.3629 - acc: 0.8404 - val_loss: 0.4231 - val_acc: 0.8805\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 157s 10ms/step - loss: 0.3573 - acc: 0.8444 - val_loss: 0.4413 - val_acc: 0.8905\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 162s 10ms/step - loss: 0.3579 - acc: 0.8444 - val_loss: 0.4087 - val_acc: 0.8858\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 163s 10ms/step - loss: 0.3583 - acc: 0.8443 - val_loss: 0.4198 - val_acc: 0.8938\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 152s 10ms/step - loss: 0.3539 - acc: 0.8450 - val_loss: 0.3811 - val_acc: 0.8898\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 152s 10ms/step - loss: 0.3541 - acc: 0.8474 - val_loss: 0.3406 - val_acc: 0.8908\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 153s 10ms/step - loss: 0.3522 - acc: 0.8462 - val_loss: 0.4343 - val_acc: 0.8890\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.3512 - acc: 0.8484 - val_loss: 0.3889 - val_acc: 0.8908\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 150s 10ms/step - loss: 0.3471 - acc: 0.8500 - val_loss: 0.3212 - val_acc: 0.8860\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 149s 10ms/step - loss: 0.3465 - acc: 0.8502 - val_loss: 0.3775 - val_acc: 0.8900\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 149s 9ms/step - loss: 0.3463 - acc: 0.8525 - val_loss: 0.4011 - val_acc: 0.8920\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 150s 10ms/step - loss: 0.3448 - acc: 0.8532 - val_loss: 0.3840 - val_acc: 0.8835\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.3447 - acc: 0.8516 - val_loss: 0.3755 - val_acc: 0.8918\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 150s 10ms/step - loss: 0.3400 - acc: 0.8539 - val_loss: 0.3201 - val_acc: 0.8912\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 149s 10ms/step - loss: 0.3426 - acc: 0.8521 - val_loss: 0.3914 - val_acc: 0.8932\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 149s 9ms/step - loss: 0.3419 - acc: 0.8537 - val_loss: 0.3608 - val_acc: 0.8910\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 149s 9ms/step - loss: 0.3389 - acc: 0.8552 - val_loss: 0.3190 - val_acc: 0.8950\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 148s 9ms/step - loss: 0.3429 - acc: 0.8513 - val_loss: 0.3541 - val_acc: 0.8915\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 149s 9ms/step - loss: 0.3400 - acc: 0.8552 - val_loss: 0.3426 - val_acc: 0.8892\n",
      "Epoch 32/50\n",
      "15658/15658 [==============================] - 155s 10ms/step - loss: 0.3359 - acc: 0.8570 - val_loss: 0.3420 - val_acc: 0.8842\n",
      "Epoch 33/50\n",
      "15658/15658 [==============================] - 155s 10ms/step - loss: 0.3359 - acc: 0.8562 - val_loss: 0.3479 - val_acc: 0.8978\n",
      "Epoch 34/50\n",
      "15658/15658 [==============================] - 154s 10ms/step - loss: 0.3365 - acc: 0.8547 - val_loss: 0.4308 - val_acc: 0.8882\n",
      "Epoch 35/50\n",
      "15658/15658 [==============================] - 150s 10ms/step - loss: 0.3364 - acc: 0.8579 - val_loss: 0.3161 - val_acc: 0.8958\n",
      "Epoch 36/50\n",
      "15658/15658 [==============================] - 149s 9ms/step - loss: 0.3328 - acc: 0.8578 - val_loss: 0.3409 - val_acc: 0.8940\n",
      "Epoch 37/50\n",
      "15658/15658 [==============================] - 149s 10ms/step - loss: 0.3352 - acc: 0.8557 - val_loss: 0.3997 - val_acc: 0.8918\n",
      "Epoch 38/50\n",
      "15658/15658 [==============================] - 149s 9ms/step - loss: 0.3327 - acc: 0.8557 - val_loss: 0.3599 - val_acc: 0.8922\n",
      "Epoch 39/50\n",
      "15658/15658 [==============================] - 147s 9ms/step - loss: 0.3310 - acc: 0.8592 - val_loss: 0.3420 - val_acc: 0.8853\n",
      "Epoch 40/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.3306 - acc: 0.8599 - val_loss: 0.3170 - val_acc: 0.8855\n",
      "Epoch 41/50\n",
      "15658/15658 [==============================] - 152s 10ms/step - loss: 0.3295 - acc: 0.8578 - val_loss: 0.3569 - val_acc: 0.8918\n",
      "Epoch 42/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.3288 - acc: 0.8607 - val_loss: 0.4290 - val_acc: 0.8882\n",
      "Epoch 43/50\n",
      "15658/15658 [==============================] - 149s 10ms/step - loss: 0.3306 - acc: 0.8597 - val_loss: 0.3324 - val_acc: 0.8938\n",
      "Epoch 44/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.3311 - acc: 0.8585 - val_loss: 0.3271 - val_acc: 0.8870\n",
      "Epoch 45/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.3278 - acc: 0.8603 - val_loss: 0.3193 - val_acc: 0.8888\n",
      "Epoch 00045: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc2e5bc0850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "batch_size = 64\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2), input_shape=(12* 3640,1)))\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(7)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "# model.add(LSTM(64, return_sequences=True,\n",
    "#                batch_input_shape=(batch_size, 12, 3640)))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(Attention_layer())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])\n",
    "# score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "# print model.predict(x_test)\n",
    "# print K.eval(categorical_accuracy(y_test, model.predict(x_test)))\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 152s 10ms/step - loss: 0.6429 - acc: 0.6068 - val_loss: 0.5259 - val_acc: 0.7468\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 148s 9ms/step - loss: 0.5485 - acc: 0.7116 - val_loss: 0.4551 - val_acc: 0.8393\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 150s 10ms/step - loss: 0.4960 - acc: 0.7542 - val_loss: 0.4584 - val_acc: 0.8488\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 158s 10ms/step - loss: 0.4730 - acc: 0.7746 - val_loss: 0.4395 - val_acc: 0.8638\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 158s 10ms/step - loss: 0.4525 - acc: 0.7863 - val_loss: 0.4533 - val_acc: 0.8423\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 151s 10ms/step - loss: 0.4400 - acc: 0.7944 - val_loss: 0.4487 - val_acc: 0.8695\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 146s 9ms/step - loss: 0.4320 - acc: 0.8040 - val_loss: 0.4503 - val_acc: 0.8565\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 145s 9ms/step - loss: 0.4234 - acc: 0.8063 - val_loss: 0.4661 - val_acc: 0.8408\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 145s 9ms/step - loss: 0.4166 - acc: 0.8136 - val_loss: 0.4660 - val_acc: 0.8770\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 144s 9ms/step - loss: 0.4113 - acc: 0.8140 - val_loss: 0.4763 - val_acc: 0.8848\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 150s 10ms/step - loss: 0.4048 - acc: 0.8189 - val_loss: 0.5273 - val_acc: 0.8452\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 152s 10ms/step - loss: 0.3957 - acc: 0.8250 - val_loss: 0.5008 - val_acc: 0.8748\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 150s 10ms/step - loss: 0.3925 - acc: 0.8267 - val_loss: 0.4631 - val_acc: 0.8822\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 149s 9ms/step - loss: 0.3904 - acc: 0.8278 - val_loss: 0.4634 - val_acc: 0.8860\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0b0f50b50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "batch_size = 64\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2), input_shape=(12* 3640,1)))\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(7)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "# model.add(LSTM(64, return_sequences=True,\n",
    "#                batch_input_shape=(batch_size, 12, 3640)))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(Attention_layer())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])\n",
    "# score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "# print model.predict(x_test)\n",
    "# print K.eval(categorical_accuracy(y_test, model.predict(x_test)))\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout_21/cond/Merge:0\", shape=(?, 169, 5), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.6266 - acc: 0.6305 - val_loss: 0.5893 - val_acc: 0.7827\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 15s 974us/step - loss: 0.5381 - acc: 0.7295 - val_loss: 0.5640 - val_acc: 0.8275\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 15s 958us/step - loss: 0.4917 - acc: 0.7694 - val_loss: 0.5613 - val_acc: 0.8430\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 15s 977us/step - loss: 0.4656 - acc: 0.7894 - val_loss: 0.6243 - val_acc: 0.8678\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 15s 969us/step - loss: 0.4486 - acc: 0.7940 - val_loss: 0.5309 - val_acc: 0.8518\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 15s 973us/step - loss: 0.4352 - acc: 0.8046 - val_loss: 0.5768 - val_acc: 0.8755\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 15s 960us/step - loss: 0.4206 - acc: 0.8145 - val_loss: 0.5653 - val_acc: 0.8752\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 15s 969us/step - loss: 0.4130 - acc: 0.8163 - val_loss: 0.5653 - val_acc: 0.8795\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 15s 971us/step - loss: 0.4089 - acc: 0.8219 - val_loss: 0.4784 - val_acc: 0.8808\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 15s 973us/step - loss: 0.3957 - acc: 0.8315 - val_loss: 0.5916 - val_acc: 0.8755\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 15s 971us/step - loss: 0.3886 - acc: 0.8317 - val_loss: 0.4110 - val_acc: 0.8928\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 15s 969us/step - loss: 0.3808 - acc: 0.8352 - val_loss: 0.3382 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 15s 986us/step - loss: 0.3799 - acc: 0.8334 - val_loss: 0.4594 - val_acc: 0.8962\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 15s 986us/step - loss: 0.3768 - acc: 0.8398 - val_loss: 0.3536 - val_acc: 0.8940\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 16s 991us/step - loss: 0.3722 - acc: 0.8384 - val_loss: 0.5054 - val_acc: 0.8947\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 16s 996us/step - loss: 0.3732 - acc: 0.8428 - val_loss: 0.4027 - val_acc: 0.9023\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 15s 988us/step - loss: 0.3685 - acc: 0.8412 - val_loss: 0.3330 - val_acc: 0.8975\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 16s 994us/step - loss: 0.3713 - acc: 0.8395 - val_loss: 0.3632 - val_acc: 0.8992\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 16s 1ms/step - loss: 0.3626 - acc: 0.8456 - val_loss: 0.4400 - val_acc: 0.8965\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 15s 979us/step - loss: 0.3658 - acc: 0.8432 - val_loss: 0.5186 - val_acc: 0.8878\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 15s 985us/step - loss: 0.3692 - acc: 0.8422 - val_loss: 0.3161 - val_acc: 0.8932\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0acea32d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    cnn1 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "    cnn2 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "    pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "    drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    cnn3 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "    cnn4 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "    pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    cnn5 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "    cnn6 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "    pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "    drop3 = Dropout(0.25)(pool3)\n",
    "    \n",
    "    print drop3\n",
    "    \n",
    "    flatten1 = Flatten()(drop3)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "# densenet_depth = 46\n",
    "# densenet_growth_rate = 8\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout_42/cond/Merge:0\", shape=(?, 169, 5), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 178s 11ms/step - loss: 0.6271 - acc: 0.6282 - val_loss: 0.5044 - val_acc: 0.8123\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 174s 11ms/step - loss: 0.4888 - acc: 0.7627 - val_loss: 0.5399 - val_acc: 0.8608\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 172s 11ms/step - loss: 0.4453 - acc: 0.7925 - val_loss: 0.5881 - val_acc: 0.8670\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 170s 11ms/step - loss: 0.4186 - acc: 0.8112 - val_loss: 0.6356 - val_acc: 0.8742\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 169s 11ms/step - loss: 0.3924 - acc: 0.8272 - val_loss: 0.5285 - val_acc: 0.8855\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 177s 11ms/step - loss: 0.3783 - acc: 0.8332 - val_loss: 0.5341 - val_acc: 0.8713\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 174s 11ms/step - loss: 0.3696 - acc: 0.8380 - val_loss: 0.4495 - val_acc: 0.8700\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 176s 11ms/step - loss: 0.3669 - acc: 0.8426 - val_loss: 0.3628 - val_acc: 0.8888\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 169s 11ms/step - loss: 0.3635 - acc: 0.8424 - val_loss: 0.3518 - val_acc: 0.8888\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 167s 11ms/step - loss: 0.3556 - acc: 0.8484 - val_loss: 0.3425 - val_acc: 0.8932\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 169s 11ms/step - loss: 0.3511 - acc: 0.8498 - val_loss: 0.3268 - val_acc: 0.8830\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 168s 11ms/step - loss: 0.3492 - acc: 0.8502 - val_loss: 0.3120 - val_acc: 0.9008\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 173s 11ms/step - loss: 0.3424 - acc: 0.8556 - val_loss: 0.2969 - val_acc: 0.8848\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 172s 11ms/step - loss: 0.3431 - acc: 0.8534 - val_loss: 0.2676 - val_acc: 0.9048\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 167s 11ms/step - loss: 0.3419 - acc: 0.8539 - val_loss: 0.2831 - val_acc: 0.9012\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 169s 11ms/step - loss: 0.3369 - acc: 0.8576 - val_loss: 0.2710 - val_acc: 0.9055\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 170s 11ms/step - loss: 0.3322 - acc: 0.8589 - val_loss: 0.2911 - val_acc: 0.8955\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 169s 11ms/step - loss: 0.3351 - acc: 0.8566 - val_loss: 0.2856 - val_acc: 0.8998\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 169s 11ms/step - loss: 0.3333 - acc: 0.8585 - val_loss: 0.3023 - val_acc: 0.8960\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 170s 11ms/step - loss: 0.3343 - acc: 0.8569 - val_loss: 0.2833 - val_acc: 0.9015\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 172s 11ms/step - loss: 0.3340 - acc: 0.8567 - val_loss: 0.2861 - val_acc: 0.8980\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0a9bc2950>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    cnn1 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "    cnn2 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "    pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "    drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    cnn3 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "    cnn4 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "    pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    cnn5 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "    cnn6 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "    pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "    drop3 = Dropout(0.25)(pool3)\n",
    "    \n",
    "    print drop3\n",
    "    \n",
    "    lstm_out1 = LSTM(32,return_sequences=True)(drop3)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "# densenet_depth = 46\n",
    "# densenet_growth_rate = 8\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_1/concat:0\", shape=(?, 10, 60), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 57s 4ms/step - loss: 0.5976 - acc: 0.6595 - val_loss: 0.4791 - val_acc: 0.8030\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 46s 3ms/step - loss: 0.4805 - acc: 0.7668 - val_loss: 0.4611 - val_acc: 0.8495\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 47s 3ms/step - loss: 0.4515 - acc: 0.7860 - val_loss: 0.6569 - val_acc: 0.7642\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 48s 3ms/step - loss: 0.4332 - acc: 0.7972 - val_loss: 0.5046 - val_acc: 0.8713\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 48s 3ms/step - loss: 0.4166 - acc: 0.8081 - val_loss: 0.4316 - val_acc: 0.8668\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 48s 3ms/step - loss: 0.3983 - acc: 0.8188 - val_loss: 0.4176 - val_acc: 0.8892\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 48s 3ms/step - loss: 0.3880 - acc: 0.8282 - val_loss: 0.3978 - val_acc: 0.8808\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 47s 3ms/step - loss: 0.3847 - acc: 0.8265 - val_loss: 0.3988 - val_acc: 0.8930\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 47s 3ms/step - loss: 0.3778 - acc: 0.8323 - val_loss: 0.4224 - val_acc: 0.8940\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 48s 3ms/step - loss: 0.3730 - acc: 0.8384 - val_loss: 0.4023 - val_acc: 0.8848\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 48s 3ms/step - loss: 0.3676 - acc: 0.8365 - val_loss: 0.4410 - val_acc: 0.8928\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 47s 3ms/step - loss: 0.3637 - acc: 0.8376 - val_loss: 0.4315 - val_acc: 0.8920\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 47s 3ms/step - loss: 0.3630 - acc: 0.8392 - val_loss: 0.4252 - val_acc: 0.8962\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 47s 3ms/step - loss: 0.3637 - acc: 0.8384 - val_loss: 0.3904 - val_acc: 0.8867\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 47s 3ms/step - loss: 0.3567 - acc: 0.8451 - val_loss: 0.3689 - val_acc: 0.8892\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 47s 3ms/step - loss: 0.3542 - acc: 0.8447 - val_loss: 0.3763 - val_acc: 0.8962\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 45s 3ms/step - loss: 0.3522 - acc: 0.8472 - val_loss: 0.4085 - val_acc: 0.8982\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 46s 3ms/step - loss: 0.3492 - acc: 0.8488 - val_loss: 0.3891 - val_acc: 0.8925\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 47s 3ms/step - loss: 0.3489 - acc: 0.8486 - val_loss: 0.4183 - val_acc: 0.9030\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 49s 3ms/step - loss: 0.3486 - acc: 0.8474 - val_loss: 0.3769 - val_acc: 0.8932\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 48s 3ms/step - loss: 0.3436 - acc: 0.8506 - val_loss: 0.4184 - val_acc: 0.8995\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 47s 3ms/step - loss: 0.3436 - acc: 0.8507 - val_loss: 0.3795 - val_acc: 0.9025\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 46s 3ms/step - loss: 0.3420 - acc: 0.8516 - val_loss: 0.3912 - val_acc: 0.8940\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 46s 3ms/step - loss: 0.3413 - acc: 0.8502 - val_loss: 0.3707 - val_acc: 0.8975\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0a3a01190>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(ROWS):\n",
    "        inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(32,return_sequences=True)(concat)\n",
    "\n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_13/concat:0\", shape=(?, 120, 40), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 232s 15ms/step - loss: 0.6566 - acc: 0.5912 - val_loss: 0.5443 - val_acc: 0.7390\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 201s 13ms/step - loss: 0.5279 - acc: 0.7320 - val_loss: 0.4743 - val_acc: 0.8005\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.4795 - acc: 0.7679 - val_loss: 0.5262 - val_acc: 0.8200\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 198s 13ms/step - loss: 0.4543 - acc: 0.7866 - val_loss: 0.4723 - val_acc: 0.8592\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.4408 - acc: 0.7959 - val_loss: 0.4633 - val_acc: 0.8605\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 201s 13ms/step - loss: 0.4255 - acc: 0.8036 - val_loss: 0.4941 - val_acc: 0.8645\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.4145 - acc: 0.8118 - val_loss: 0.5694 - val_acc: 0.8213\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 201s 13ms/step - loss: 0.4033 - acc: 0.8164 - val_loss: 0.4487 - val_acc: 0.8812\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 202s 13ms/step - loss: 0.3908 - acc: 0.8264 - val_loss: 0.4239 - val_acc: 0.8760\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 201s 13ms/step - loss: 0.3789 - acc: 0.8318 - val_loss: 0.4401 - val_acc: 0.8867\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.3680 - acc: 0.8394 - val_loss: 0.4575 - val_acc: 0.8947\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.3600 - acc: 0.8465 - val_loss: 0.4543 - val_acc: 0.8890\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 201s 13ms/step - loss: 0.3553 - acc: 0.8479 - val_loss: 0.4658 - val_acc: 0.8838\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 201s 13ms/step - loss: 0.3513 - acc: 0.8508 - val_loss: 0.4448 - val_acc: 0.8942\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 201s 13ms/step - loss: 0.3437 - acc: 0.8518 - val_loss: 0.4274 - val_acc: 0.8940\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.3403 - acc: 0.8521 - val_loss: 0.3668 - val_acc: 0.8960\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.3375 - acc: 0.8549 - val_loss: 0.3898 - val_acc: 0.8968\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.3350 - acc: 0.8543 - val_loss: 0.3758 - val_acc: 0.8928\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.3315 - acc: 0.8568 - val_loss: 0.3518 - val_acc: 0.8957\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 201s 13ms/step - loss: 0.3281 - acc: 0.8586 - val_loss: 0.3602 - val_acc: 0.9023\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.3236 - acc: 0.8628 - val_loss: 0.3718 - val_acc: 0.8982\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 198s 13ms/step - loss: 0.3228 - acc: 0.8611 - val_loss: 0.3229 - val_acc: 0.8953\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.3208 - acc: 0.8611 - val_loss: 0.3644 - val_acc: 0.8942\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.3196 - acc: 0.8628 - val_loss: 0.3736 - val_acc: 0.8968\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.3178 - acc: 0.8638 - val_loss: 0.3392 - val_acc: 0.9005\n",
      "Epoch 00025: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbfecb8b390>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(ROWS):\n",
    "        inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(48, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(48, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(56, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(56, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(40, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(40, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=1)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_14/concat:0\", shape=(?, 10, 480), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 127s 8ms/step - loss: 0.6484 - acc: 0.6037 - val_loss: 0.5259 - val_acc: 0.7775\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 101s 6ms/step - loss: 0.5181 - acc: 0.7448 - val_loss: 0.4817 - val_acc: 0.8063\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 101s 6ms/step - loss: 0.4713 - acc: 0.7756 - val_loss: 0.4874 - val_acc: 0.8423\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 101s 6ms/step - loss: 0.4482 - acc: 0.7897 - val_loss: 0.4920 - val_acc: 0.8645\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 101s 6ms/step - loss: 0.4277 - acc: 0.8048 - val_loss: 0.5027 - val_acc: 0.8637\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.4075 - acc: 0.8149 - val_loss: 0.4928 - val_acc: 0.8810\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 104s 7ms/step - loss: 0.3920 - acc: 0.8247 - val_loss: 0.4750 - val_acc: 0.8718\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3774 - acc: 0.8323 - val_loss: 0.4594 - val_acc: 0.8978\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3641 - acc: 0.8416 - val_loss: 0.4713 - val_acc: 0.8932\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3529 - acc: 0.8463 - val_loss: 0.4609 - val_acc: 0.8955\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 104s 7ms/step - loss: 0.3529 - acc: 0.8474 - val_loss: 0.4710 - val_acc: 0.8995\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3443 - acc: 0.8522 - val_loss: 0.4141 - val_acc: 0.8938\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.4387 - val_acc: 0.8962\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3414 - acc: 0.8543 - val_loss: 0.4550 - val_acc: 0.8972\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3334 - acc: 0.8579 - val_loss: 0.4058 - val_acc: 0.9008\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 104s 7ms/step - loss: 0.3342 - acc: 0.8587 - val_loss: 0.4264 - val_acc: 0.8968\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3332 - acc: 0.8577 - val_loss: 0.4168 - val_acc: 0.9038\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3273 - acc: 0.8606 - val_loss: 0.4087 - val_acc: 0.8990\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3282 - acc: 0.8601 - val_loss: 0.4277 - val_acc: 0.8988\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3264 - acc: 0.8612 - val_loss: 0.4354 - val_acc: 0.9045\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3243 - acc: 0.8624 - val_loss: 0.4191 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3219 - acc: 0.8645 - val_loss: 0.4240 - val_acc: 0.9042\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3191 - acc: 0.8660 - val_loss: 0.4404 - val_acc: 0.8982\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3165 - acc: 0.8651 - val_loss: 0.4304 - val_acc: 0.9035\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.3181 - acc: 0.8638 - val_loss: 0.4276 - val_acc: 0.9040\n",
      "Epoch 00025: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbfdd33c7d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(ROWS):\n",
    "        inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(48, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(48, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(56, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(56, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(40, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(40, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_15/concat:0\", shape=(?, 10, 480), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 116s 7ms/step - loss: 0.6242 - acc: 0.6399 - val_loss: 0.5417 - val_acc: 0.7445\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 91s 6ms/step - loss: 0.5138 - acc: 0.7471 - val_loss: 0.5240 - val_acc: 0.8393\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 92s 6ms/step - loss: 0.4751 - acc: 0.7728 - val_loss: 0.5293 - val_acc: 0.8520\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 92s 6ms/step - loss: 0.4543 - acc: 0.7898 - val_loss: 0.5672 - val_acc: 0.8570\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 92s 6ms/step - loss: 0.4364 - acc: 0.8044 - val_loss: 0.5922 - val_acc: 0.8652\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 92s 6ms/step - loss: 0.4226 - acc: 0.8054 - val_loss: 0.5741 - val_acc: 0.8680\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 92s 6ms/step - loss: 0.4129 - acc: 0.8150 - val_loss: 0.5421 - val_acc: 0.8750\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 92s 6ms/step - loss: 0.3979 - acc: 0.8244 - val_loss: 0.5762 - val_acc: 0.8790\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 92s 6ms/step - loss: 0.3889 - acc: 0.8303 - val_loss: 0.5425 - val_acc: 0.8682\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 92s 6ms/step - loss: 0.3736 - acc: 0.8394 - val_loss: 0.5642 - val_acc: 0.8912\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 92s 6ms/step - loss: 0.3656 - acc: 0.8410 - val_loss: 0.5695 - val_acc: 0.8925\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 92s 6ms/step - loss: 0.3591 - acc: 0.8451 - val_loss: 0.5510 - val_acc: 0.8795\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 92s 6ms/step - loss: 0.3580 - acc: 0.8466 - val_loss: 0.5355 - val_acc: 0.8972\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 92s 6ms/step - loss: 0.3475 - acc: 0.8491 - val_loss: 0.4366 - val_acc: 0.8835\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 93s 6ms/step - loss: 0.3444 - acc: 0.8501 - val_loss: 0.4982 - val_acc: 0.8970\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 93s 6ms/step - loss: 0.3423 - acc: 0.8537 - val_loss: 0.4634 - val_acc: 0.8950\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 93s 6ms/step - loss: 0.3395 - acc: 0.8530 - val_loss: 0.4323 - val_acc: 0.8972\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 93s 6ms/step - loss: 0.3316 - acc: 0.8564 - val_loss: 0.3959 - val_acc: 0.8988\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 93s 6ms/step - loss: 0.3324 - acc: 0.8584 - val_loss: 0.4526 - val_acc: 0.8955\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 93s 6ms/step - loss: 0.3285 - acc: 0.8575 - val_loss: 0.4091 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 93s 6ms/step - loss: 0.3265 - acc: 0.8597 - val_loss: 0.4031 - val_acc: 0.8980\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 93s 6ms/step - loss: 0.3215 - acc: 0.8612 - val_loss: 0.4381 - val_acc: 0.8992\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 93s 6ms/step - loss: 0.3207 - acc: 0.8610 - val_loss: 0.3768 - val_acc: 0.8982\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 93s 6ms/step - loss: 0.3185 - acc: 0.8631 - val_loss: 0.5060 - val_acc: 0.8713\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 92s 6ms/step - loss: 0.3142 - acc: 0.8629 - val_loss: 0.3909 - val_acc: 0.8957\n",
      "Epoch 00025: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbfce033f50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(ROWS):\n",
    "        inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(48, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(48, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(56, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(56, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(40, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(40, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "#     lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(concat)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_31/concat:0\", shape=(?, 120, 30), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 130s 8ms/step - loss: 0.6787 - acc: 0.5531 - val_loss: 0.6589 - val_acc: 0.6522\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 77s 5ms/step - loss: 0.6514 - acc: 0.6175 - val_loss: 0.6161 - val_acc: 0.6723\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.6281 - acc: 0.6450 - val_loss: 0.5773 - val_acc: 0.7145\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.5889 - acc: 0.6923 - val_loss: 0.5170 - val_acc: 0.7997\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.5402 - acc: 0.7318 - val_loss: 0.4760 - val_acc: 0.8223\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.5133 - acc: 0.7489 - val_loss: 0.4899 - val_acc: 0.7897\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 80s 5ms/step - loss: 0.4992 - acc: 0.7590 - val_loss: 0.4666 - val_acc: 0.8402\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4903 - acc: 0.7647 - val_loss: 0.4670 - val_acc: 0.8270\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4797 - acc: 0.7690 - val_loss: 0.4589 - val_acc: 0.8452\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4731 - acc: 0.7744 - val_loss: 0.4576 - val_acc: 0.8528\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4654 - acc: 0.7804 - val_loss: 0.4534 - val_acc: 0.8433\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4605 - acc: 0.7835 - val_loss: 0.4539 - val_acc: 0.8602\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4527 - acc: 0.7872 - val_loss: 0.4677 - val_acc: 0.8605\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4466 - acc: 0.7944 - val_loss: 0.4755 - val_acc: 0.8482\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4418 - acc: 0.7918 - val_loss: 0.4677 - val_acc: 0.8637\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4376 - acc: 0.7952 - val_loss: 0.4692 - val_acc: 0.8627\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4328 - acc: 0.8012 - val_loss: 0.4757 - val_acc: 0.8620\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4284 - acc: 0.8028 - val_loss: 0.4624 - val_acc: 0.8678\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4210 - acc: 0.8071 - val_loss: 0.4762 - val_acc: 0.8695\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4151 - acc: 0.8125 - val_loss: 0.4714 - val_acc: 0.8735\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4080 - acc: 0.8149 - val_loss: 0.4807 - val_acc: 0.8802\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.4039 - acc: 0.8205 - val_loss: 0.4638 - val_acc: 0.8688\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3937 - acc: 0.8251 - val_loss: 0.4754 - val_acc: 0.8865\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3913 - acc: 0.8260 - val_loss: 0.4413 - val_acc: 0.8778\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3837 - acc: 0.8294 - val_loss: 0.4510 - val_acc: 0.8765\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3806 - acc: 0.8318 - val_loss: 0.4828 - val_acc: 0.8873\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3740 - acc: 0.8362 - val_loss: 0.4379 - val_acc: 0.8818\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3762 - acc: 0.8352 - val_loss: 0.5085 - val_acc: 0.8905\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3724 - acc: 0.8375 - val_loss: 0.4518 - val_acc: 0.8782\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3678 - acc: 0.8428 - val_loss: 0.4942 - val_acc: 0.8922\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3655 - acc: 0.8419 - val_loss: 0.4622 - val_acc: 0.8938\n",
      "Epoch 32/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3631 - acc: 0.8421 - val_loss: 0.4434 - val_acc: 0.8925\n",
      "Epoch 33/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3591 - acc: 0.8481 - val_loss: 0.4363 - val_acc: 0.8885\n",
      "Epoch 34/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3603 - acc: 0.8447 - val_loss: 0.4303 - val_acc: 0.8915\n",
      "Epoch 35/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3558 - acc: 0.8465 - val_loss: 0.4366 - val_acc: 0.9000\n",
      "Epoch 36/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3522 - acc: 0.8488 - val_loss: 0.4258 - val_acc: 0.8975\n",
      "Epoch 37/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3533 - acc: 0.8466 - val_loss: 0.4737 - val_acc: 0.8982\n",
      "Epoch 38/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3497 - acc: 0.8493 - val_loss: 0.4191 - val_acc: 0.8970\n",
      "Epoch 39/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3496 - acc: 0.8506 - val_loss: 0.4243 - val_acc: 0.8902\n",
      "Epoch 40/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3479 - acc: 0.8526 - val_loss: 0.4272 - val_acc: 0.8995\n",
      "Epoch 41/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3477 - acc: 0.8507 - val_loss: 0.4326 - val_acc: 0.8957\n",
      "Epoch 42/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3508 - acc: 0.8499 - val_loss: 0.4534 - val_acc: 0.8982\n",
      "Epoch 43/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3429 - acc: 0.8535 - val_loss: 0.4241 - val_acc: 0.8970\n",
      "Epoch 44/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3459 - acc: 0.8502 - val_loss: 0.3784 - val_acc: 0.8900\n",
      "Epoch 45/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3447 - acc: 0.8517 - val_loss: 0.3717 - val_acc: 0.8932\n",
      "Epoch 46/50\n",
      "15658/15658 [==============================] - 78s 5ms/step - loss: 0.3438 - acc: 0.8539 - val_loss: 0.3880 - val_acc: 0.8910\n",
      "Epoch 47/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3396 - acc: 0.8576 - val_loss: 0.4263 - val_acc: 0.8980\n",
      "Epoch 48/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3420 - acc: 0.8536 - val_loss: 0.4154 - val_acc: 0.8980\n",
      "Epoch 49/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3391 - acc: 0.8559 - val_loss: 0.3724 - val_acc: 0.8938\n",
      "Epoch 50/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.3389 - acc: 0.8549 - val_loss: 0.4129 - val_acc: 0.8955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf144bd6d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(ROWS):\n",
    "        inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=1)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "#     lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(concat)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=20, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_32/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Tensor(\"attention_layer_3/div:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Tensor(\"concatenate_32/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Tensor(\"attention_layer_3/mul:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 200s 13ms/step - loss: 0.6708 - acc: 0.5710 - val_loss: 0.6338 - val_acc: 0.6585\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 138s 9ms/step - loss: 0.5684 - acc: 0.7036 - val_loss: 0.5869 - val_acc: 0.7825\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.4973 - acc: 0.7627 - val_loss: 0.4947 - val_acc: 0.8452\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.4659 - acc: 0.7844 - val_loss: 0.4530 - val_acc: 0.8645\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.4379 - acc: 0.8031 - val_loss: 0.4156 - val_acc: 0.8572\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.4110 - acc: 0.8189 - val_loss: 0.4769 - val_acc: 0.8550\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3999 - acc: 0.8283 - val_loss: 0.3876 - val_acc: 0.8915\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3878 - acc: 0.8326 - val_loss: 0.3811 - val_acc: 0.8900\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3874 - acc: 0.8336 - val_loss: 0.4273 - val_acc: 0.8898\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3827 - acc: 0.8331 - val_loss: 0.3797 - val_acc: 0.8890\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3755 - acc: 0.8376 - val_loss: 0.3685 - val_acc: 0.8935\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3711 - acc: 0.8370 - val_loss: 0.3929 - val_acc: 0.8840\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3656 - acc: 0.8420 - val_loss: 0.3539 - val_acc: 0.8918\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3626 - acc: 0.8474 - val_loss: 0.3606 - val_acc: 0.8978\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 138s 9ms/step - loss: 0.3598 - acc: 0.8474 - val_loss: 0.3979 - val_acc: 0.8935\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 138s 9ms/step - loss: 0.3580 - acc: 0.8453 - val_loss: 0.3685 - val_acc: 0.8953\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3547 - acc: 0.8454 - val_loss: 0.3765 - val_acc: 0.8940\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3516 - acc: 0.8512 - val_loss: 0.3581 - val_acc: 0.8932\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3508 - acc: 0.8483 - val_loss: 0.4060 - val_acc: 0.8902\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3491 - acc: 0.8520 - val_loss: 0.3408 - val_acc: 0.8940\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3498 - acc: 0.8502 - val_loss: 0.3471 - val_acc: 0.9015\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3441 - acc: 0.8515 - val_loss: 0.4062 - val_acc: 0.8918\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3448 - acc: 0.8527 - val_loss: 0.3551 - val_acc: 0.8878\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3413 - acc: 0.8528 - val_loss: 0.3638 - val_acc: 0.8908\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3419 - acc: 0.8517 - val_loss: 0.3853 - val_acc: 0.8990\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3390 - acc: 0.8543 - val_loss: 0.3375 - val_acc: 0.8975\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3353 - acc: 0.8592 - val_loss: 0.3606 - val_acc: 0.8925\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3373 - acc: 0.8563 - val_loss: 0.3367 - val_acc: 0.8975\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3351 - acc: 0.8555 - val_loss: 0.3182 - val_acc: 0.8935\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3340 - acc: 0.8587 - val_loss: 0.4069 - val_acc: 0.8918\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3594 - acc: 0.8428 - val_loss: 0.3314 - val_acc: 0.8972\n",
      "Epoch 00031: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf06d88f10>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "#     lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "    flatten1 = Attention_layer()(concat)\n",
    "\n",
    "#     flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_33/concat:0\", shape=(?, 338, 30), dtype=float32)\n",
      "Tensor(\"attention_layer_4/div:0\", shape=(?, 338, 30), dtype=float32)\n",
      "Tensor(\"concatenate_33/concat:0\", shape=(?, 338, 30), dtype=float32)\n",
      "Tensor(\"attention_layer_4/mul:0\", shape=(?, 338, 30), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 202s 13ms/step - loss: 0.6837 - acc: 0.5393 - val_loss: 0.6644 - val_acc: 0.5677\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 136s 9ms/step - loss: 0.6173 - acc: 0.6519 - val_loss: 0.6293 - val_acc: 0.7847\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.5246 - acc: 0.7375 - val_loss: 0.5577 - val_acc: 0.8435\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.4849 - acc: 0.7680 - val_loss: 0.6092 - val_acc: 0.8250\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.4655 - acc: 0.7815 - val_loss: 0.5383 - val_acc: 0.8633\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.4480 - acc: 0.7912 - val_loss: 0.5020 - val_acc: 0.8752\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.4249 - acc: 0.8076 - val_loss: 0.4423 - val_acc: 0.8790\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.4063 - acc: 0.8201 - val_loss: 0.4108 - val_acc: 0.8850\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3964 - acc: 0.8290 - val_loss: 0.3967 - val_acc: 0.8930\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3915 - acc: 0.8276 - val_loss: 0.4028 - val_acc: 0.8978\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3830 - acc: 0.8324 - val_loss: 0.3746 - val_acc: 0.8895\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3783 - acc: 0.8377 - val_loss: 0.3858 - val_acc: 0.8892\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3751 - acc: 0.8381 - val_loss: 0.3726 - val_acc: 0.8930\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3700 - acc: 0.8393 - val_loss: 0.4241 - val_acc: 0.8865\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3656 - acc: 0.8417 - val_loss: 0.3437 - val_acc: 0.8982\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3627 - acc: 0.8417 - val_loss: 0.3471 - val_acc: 0.8960\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3610 - acc: 0.8440 - val_loss: 0.3727 - val_acc: 0.8932\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3606 - acc: 0.8446 - val_loss: 0.3849 - val_acc: 0.8928\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3571 - acc: 0.8473 - val_loss: 0.3354 - val_acc: 0.8925\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3551 - acc: 0.8457 - val_loss: 0.3574 - val_acc: 0.8932\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3515 - acc: 0.8469 - val_loss: 0.4301 - val_acc: 0.8878\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3494 - acc: 0.8513 - val_loss: 0.3738 - val_acc: 0.8915\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3504 - acc: 0.8497 - val_loss: 0.3401 - val_acc: 0.8930\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3498 - acc: 0.8477 - val_loss: 0.3648 - val_acc: 0.8950\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3472 - acc: 0.8545 - val_loss: 0.3949 - val_acc: 0.8898\n",
      "Epoch 00025: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf01e3e710>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=1)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "#     lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "    flatten1 = Attention_layer()(concat)\n",
    "\n",
    "#     flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_36/concat:0\", shape=(?, 169, 90), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/100\n",
      "15658/15658 [==============================] - 487s 31ms/step - loss: 0.6546 - acc: 0.5865 - val_loss: 0.5624 - val_acc: 0.7165\n",
      "Epoch 2/100\n",
      "15658/15658 [==============================] - 391s 25ms/step - loss: 0.5358 - acc: 0.7228 - val_loss: 0.5172 - val_acc: 0.7935\n",
      "Epoch 3/100\n",
      "15658/15658 [==============================] - 392s 25ms/step - loss: 0.4809 - acc: 0.7664 - val_loss: 0.5177 - val_acc: 0.8465\n",
      "Epoch 4/100\n",
      "15658/15658 [==============================] - 412s 26ms/step - loss: 0.4574 - acc: 0.7817 - val_loss: 0.4864 - val_acc: 0.8500\n",
      "Epoch 5/100\n",
      "15658/15658 [==============================] - 410s 26ms/step - loss: 0.4435 - acc: 0.7923 - val_loss: 0.4946 - val_acc: 0.8508\n",
      "Epoch 6/100\n",
      "15658/15658 [==============================] - 412s 26ms/step - loss: 0.4314 - acc: 0.8000 - val_loss: 0.5062 - val_acc: 0.8698\n",
      "Epoch 7/100\n",
      "15658/15658 [==============================] - 412s 26ms/step - loss: 0.4239 - acc: 0.8049 - val_loss: 0.4930 - val_acc: 0.8652\n",
      "Epoch 8/100\n",
      "15658/15658 [==============================] - 412s 26ms/step - loss: 0.4125 - acc: 0.8113 - val_loss: 0.5221 - val_acc: 0.8635\n",
      "Epoch 9/100\n",
      "15658/15658 [==============================] - 412s 26ms/step - loss: 0.4049 - acc: 0.8162 - val_loss: 0.5003 - val_acc: 0.8718\n",
      "Epoch 10/100\n",
      "15658/15658 [==============================] - 408s 26ms/step - loss: 0.3958 - acc: 0.8230 - val_loss: 0.5085 - val_acc: 0.8838\n",
      "Epoch 11/100\n",
      "15658/15658 [==============================] - 379s 24ms/step - loss: 0.3843 - acc: 0.8290 - val_loss: 0.5870 - val_acc: 0.8530\n",
      "Epoch 12/100\n",
      "15658/15658 [==============================] - 377s 24ms/step - loss: 0.3772 - acc: 0.8354 - val_loss: 0.5120 - val_acc: 0.8655\n",
      "Epoch 13/100\n",
      "15658/15658 [==============================] - 378s 24ms/step - loss: 0.3615 - acc: 0.8426 - val_loss: 0.5161 - val_acc: 0.8902\n",
      "Epoch 14/100\n",
      "15658/15658 [==============================] - 383s 24ms/step - loss: 0.3538 - acc: 0.8507 - val_loss: 0.5649 - val_acc: 0.8380\n",
      "Epoch 15/100\n",
      "15658/15658 [==============================] - 381s 24ms/step - loss: 0.3454 - acc: 0.8508 - val_loss: 0.4784 - val_acc: 0.9012\n",
      "Epoch 16/100\n",
      "15658/15658 [==============================] - 391s 25ms/step - loss: 0.3415 - acc: 0.8547 - val_loss: 0.4196 - val_acc: 0.8955\n",
      "Epoch 17/100\n",
      "15658/15658 [==============================] - 412s 26ms/step - loss: 0.3351 - acc: 0.8581 - val_loss: 0.4589 - val_acc: 0.8922\n",
      "Epoch 18/100\n",
      "15658/15658 [==============================] - 412s 26ms/step - loss: 0.3331 - acc: 0.8603 - val_loss: 0.4538 - val_acc: 0.8955\n",
      "Epoch 19/100\n",
      "15658/15658 [==============================] - 414s 26ms/step - loss: 0.3293 - acc: 0.8579 - val_loss: 0.4046 - val_acc: 0.9005\n",
      "Epoch 20/100\n",
      "15658/15658 [==============================] - 410s 26ms/step - loss: 0.3200 - acc: 0.8626 - val_loss: 0.4284 - val_acc: 0.8932\n",
      "Epoch 21/100\n",
      "15658/15658 [==============================] - 413s 26ms/step - loss: 0.3222 - acc: 0.8621 - val_loss: 0.3806 - val_acc: 0.8968\n",
      "Epoch 22/100\n",
      "15658/15658 [==============================] - 411s 26ms/step - loss: 0.3212 - acc: 0.8643 - val_loss: 0.4481 - val_acc: 0.9055\n",
      "Epoch 23/100\n",
      "15658/15658 [==============================] - 418s 27ms/step - loss: 0.3160 - acc: 0.8659 - val_loss: 0.4810 - val_acc: 0.8900\n",
      "Epoch 24/100\n",
      "15658/15658 [==============================] - 414s 26ms/step - loss: 0.3138 - acc: 0.8679 - val_loss: 0.4183 - val_acc: 0.9000\n",
      "Epoch 25/100\n",
      "15658/15658 [==============================] - 412s 26ms/step - loss: 0.3113 - acc: 0.8691 - val_loss: 0.3691 - val_acc: 0.8978\n",
      "Epoch 26/100\n",
      "15658/15658 [==============================] - 412s 26ms/step - loss: 0.3079 - acc: 0.8679 - val_loss: 0.4253 - val_acc: 0.9033\n",
      "Epoch 27/100\n",
      "15658/15658 [==============================] - 413s 26ms/step - loss: 0.3063 - acc: 0.8707 - val_loss: 0.4102 - val_acc: 0.8828\n",
      "Epoch 28/100\n",
      "15658/15658 [==============================] - 412s 26ms/step - loss: 0.3033 - acc: 0.8718 - val_loss: 0.4271 - val_acc: 0.9030\n",
      "Epoch 29/100\n",
      "15658/15658 [==============================] - 413s 26ms/step - loss: 0.3003 - acc: 0.8753 - val_loss: 0.3478 - val_acc: 0.8945\n",
      "Epoch 30/100\n",
      "15658/15658 [==============================] - 412s 26ms/step - loss: 0.2994 - acc: 0.8727 - val_loss: 0.3537 - val_acc: 0.8995\n",
      "Epoch 31/100\n",
      "15658/15658 [==============================] - 407s 26ms/step - loss: 0.2984 - acc: 0.8718 - val_loss: 0.3769 - val_acc: 0.9048\n",
      "Epoch 32/100\n",
      "15658/15658 [==============================] - 398s 25ms/step - loss: 0.2979 - acc: 0.8727 - val_loss: 0.3769 - val_acc: 0.9025\n",
      "Epoch 00032: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbef457c950>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(3):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=100,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_37/concat:0\", shape=(?, 507, 30), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/100\n",
      "15658/15658 [==============================] - 851s 54ms/step - loss: 0.6653 - acc: 0.5771 - val_loss: 0.6032 - val_acc: 0.6937\n",
      "Epoch 2/100\n",
      "15658/15658 [==============================] - 719s 46ms/step - loss: 0.5802 - acc: 0.6841 - val_loss: 0.4751 - val_acc: 0.8110\n",
      "Epoch 3/100\n",
      "15658/15658 [==============================] - 725s 46ms/step - loss: 0.5030 - acc: 0.7467 - val_loss: 0.4760 - val_acc: 0.8205\n",
      "Epoch 4/100\n",
      "15658/15658 [==============================] - 719s 46ms/step - loss: 0.4681 - acc: 0.7739 - val_loss: 0.4599 - val_acc: 0.8630\n",
      "Epoch 5/100\n",
      "15658/15658 [==============================] - 726s 46ms/step - loss: 0.4421 - acc: 0.7947 - val_loss: 0.4731 - val_acc: 0.8670\n",
      "Epoch 6/100\n",
      "15658/15658 [==============================] - 717s 46ms/step - loss: 0.4275 - acc: 0.8003 - val_loss: 0.4803 - val_acc: 0.8598\n",
      "Epoch 7/100\n",
      "15658/15658 [==============================] - 823s 53ms/step - loss: 0.4171 - acc: 0.8106 - val_loss: 0.4709 - val_acc: 0.8755\n",
      "Epoch 8/100\n",
      "15658/15658 [==============================] - 944s 60ms/step - loss: 0.4068 - acc: 0.8140 - val_loss: 0.4344 - val_acc: 0.8732\n",
      "Epoch 9/100\n",
      "15658/15658 [==============================] - 871s 56ms/step - loss: 0.3974 - acc: 0.8223 - val_loss: 0.4049 - val_acc: 0.8662\n",
      "Epoch 10/100\n",
      "15658/15658 [==============================] - 957s 61ms/step - loss: 0.3889 - acc: 0.8280 - val_loss: 0.4432 - val_acc: 0.8700\n",
      "Epoch 11/100\n",
      "15658/15658 [==============================] - 944s 60ms/step - loss: 0.3758 - acc: 0.8332 - val_loss: 0.4096 - val_acc: 0.8785\n",
      "Epoch 12/100\n",
      "15658/15658 [==============================] - 966s 62ms/step - loss: 0.3711 - acc: 0.8351 - val_loss: 0.4124 - val_acc: 0.8938\n",
      "Epoch 13/100\n",
      "15658/15658 [==============================] - 782s 50ms/step - loss: 0.3643 - acc: 0.8391 - val_loss: 0.4277 - val_acc: 0.8918\n",
      "Epoch 14/100\n",
      "15658/15658 [==============================] - 716s 46ms/step - loss: 0.3563 - acc: 0.8426 - val_loss: 0.4160 - val_acc: 0.8950\n",
      "Epoch 15/100\n",
      "15658/15658 [==============================] - 704s 45ms/step - loss: 0.3519 - acc: 0.8471 - val_loss: 0.3652 - val_acc: 0.9002\n",
      "Epoch 16/100\n",
      "15658/15658 [==============================] - 704s 45ms/step - loss: 0.3456 - acc: 0.8492 - val_loss: 0.3421 - val_acc: 0.9008\n",
      "Epoch 17/100\n",
      "15658/15658 [==============================] - 699s 45ms/step - loss: 0.3414 - acc: 0.8520 - val_loss: 0.3355 - val_acc: 0.8990\n",
      "Epoch 18/100\n",
      "15658/15658 [==============================] - 699s 45ms/step - loss: 0.3360 - acc: 0.8547 - val_loss: 0.3419 - val_acc: 0.9030\n",
      "Epoch 19/100\n",
      "15658/15658 [==============================] - 696s 44ms/step - loss: 0.3325 - acc: 0.8568 - val_loss: 0.3714 - val_acc: 0.9015\n",
      "Epoch 20/100\n",
      "15658/15658 [==============================] - 702s 45ms/step - loss: 0.3277 - acc: 0.8618 - val_loss: 0.3132 - val_acc: 0.9065\n",
      "Epoch 21/100\n",
      "15658/15658 [==============================] - 698s 45ms/step - loss: 0.3232 - acc: 0.8606 - val_loss: 0.4121 - val_acc: 0.8895\n",
      "Epoch 22/100\n",
      "15658/15658 [==============================] - 698s 45ms/step - loss: 0.3250 - acc: 0.8602 - val_loss: 0.3523 - val_acc: 0.8960\n",
      "Epoch 23/100\n",
      "15658/15658 [==============================] - 700s 45ms/step - loss: 0.3216 - acc: 0.8625 - val_loss: 0.3257 - val_acc: 0.8947\n",
      "Epoch 24/100\n",
      "15658/15658 [==============================] - 698s 45ms/step - loss: 0.3184 - acc: 0.8617 - val_loss: 0.3214 - val_acc: 0.8970\n",
      "Epoch 25/100\n",
      "15658/15658 [==============================] - 699s 45ms/step - loss: 0.3119 - acc: 0.8665 - val_loss: 0.3196 - val_acc: 0.9052\n",
      "Epoch 26/100\n",
      "15658/15658 [==============================] - 724s 46ms/step - loss: 0.3136 - acc: 0.8652 - val_loss: 0.3021 - val_acc: 0.9040\n",
      "Epoch 27/100\n",
      "15658/15658 [==============================] - 719s 46ms/step - loss: 0.3089 - acc: 0.8697 - val_loss: 0.3169 - val_acc: 0.9090\n",
      "Epoch 28/100\n",
      "15658/15658 [==============================] - 725s 46ms/step - loss: 0.3070 - acc: 0.8685 - val_loss: 0.3134 - val_acc: 0.9075\n",
      "Epoch 29/100\n",
      "15658/15658 [==============================] - 718s 46ms/step - loss: 0.3028 - acc: 0.8710 - val_loss: 0.3216 - val_acc: 0.9125\n",
      "Epoch 30/100\n",
      "15658/15658 [==============================] - 718s 46ms/step - loss: 0.3040 - acc: 0.8705 - val_loss: 0.3045 - val_acc: 0.9090\n",
      "Epoch 31/100\n",
      "15658/15658 [==============================] - 719s 46ms/step - loss: 0.2999 - acc: 0.8716 - val_loss: 0.3531 - val_acc: 0.9077\n",
      "Epoch 32/100\n",
      "15658/15658 [==============================] - 722s 46ms/step - loss: 0.3012 - acc: 0.8721 - val_loss: 0.2887 - val_acc: 0.9080\n",
      "Epoch 33/100\n",
      "15658/15658 [==============================] - 734s 47ms/step - loss: 0.2974 - acc: 0.8723 - val_loss: 0.2920 - val_acc: 0.9067\n",
      "Epoch 34/100\n",
      "15658/15658 [==============================] - 729s 47ms/step - loss: 0.2971 - acc: 0.8744 - val_loss: 0.3123 - val_acc: 0.9123\n",
      "Epoch 35/100\n",
      "15658/15658 [==============================] - 720s 46ms/step - loss: 0.2931 - acc: 0.8736 - val_loss: 0.3159 - val_acc: 0.9055\n",
      "Epoch 36/100\n",
      "15658/15658 [==============================] - 716s 46ms/step - loss: 0.2903 - acc: 0.8739 - val_loss: 0.2797 - val_acc: 0.9083\n",
      "Epoch 37/100\n",
      "15658/15658 [==============================] - 768s 49ms/step - loss: 0.2877 - acc: 0.8778 - val_loss: 0.3139 - val_acc: 0.9090\n",
      "Epoch 38/100\n",
      "15658/15658 [==============================] - 775s 50ms/step - loss: 0.2881 - acc: 0.8776 - val_loss: 0.3137 - val_acc: 0.9008\n",
      "Epoch 39/100\n",
      "15658/15658 [==============================] - 777s 50ms/step - loss: 0.2851 - acc: 0.8792 - val_loss: 0.3418 - val_acc: 0.8815\n",
      "Epoch 00039: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbeef65a190>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(3):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=1)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=100,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_40/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/100\n",
      "15658/15658 [==============================] - 545s 35ms/step - loss: 0.6520 - acc: 0.5991 - val_loss: 0.5544 - val_acc: 0.7518\n",
      "Epoch 2/100\n",
      "15658/15658 [==============================] - 468s 30ms/step - loss: 0.5237 - acc: 0.7349 - val_loss: 0.5231 - val_acc: 0.7768\n",
      "Epoch 3/100\n",
      "15658/15658 [==============================] - 468s 30ms/step - loss: 0.4720 - acc: 0.7716 - val_loss: 0.4618 - val_acc: 0.8595\n",
      "Epoch 4/100\n",
      "15658/15658 [==============================] - 465s 30ms/step - loss: 0.4405 - acc: 0.7940 - val_loss: 0.4651 - val_acc: 0.8445\n",
      "Epoch 5/100\n",
      "15658/15658 [==============================] - 466s 30ms/step - loss: 0.4201 - acc: 0.8055 - val_loss: 0.4449 - val_acc: 0.8793\n",
      "Epoch 6/100\n",
      "15658/15658 [==============================] - 468s 30ms/step - loss: 0.3967 - acc: 0.8249 - val_loss: 0.4498 - val_acc: 0.8660\n",
      "Epoch 7/100\n",
      "15658/15658 [==============================] - 471s 30ms/step - loss: 0.3835 - acc: 0.8308 - val_loss: 0.4140 - val_acc: 0.8975\n",
      "Epoch 8/100\n",
      "15658/15658 [==============================] - 471s 30ms/step - loss: 0.3701 - acc: 0.8362 - val_loss: 0.4157 - val_acc: 0.8900\n",
      "Epoch 9/100\n",
      "15658/15658 [==============================] - 469s 30ms/step - loss: 0.3619 - acc: 0.8431 - val_loss: 0.4843 - val_acc: 0.8863\n",
      "Epoch 10/100\n",
      "15658/15658 [==============================] - 472s 30ms/step - loss: 0.3586 - acc: 0.8461 - val_loss: 0.4542 - val_acc: 0.9018\n",
      "Epoch 11/100\n",
      "15658/15658 [==============================] - 470s 30ms/step - loss: 0.3512 - acc: 0.8488 - val_loss: 0.4255 - val_acc: 0.9008\n",
      "Epoch 12/100\n",
      "15658/15658 [==============================] - 473s 30ms/step - loss: 0.3473 - acc: 0.8517 - val_loss: 0.4718 - val_acc: 0.9035\n",
      "Epoch 13/100\n",
      "15658/15658 [==============================] - 471s 30ms/step - loss: 0.3449 - acc: 0.8513 - val_loss: 0.3872 - val_acc: 0.9027\n",
      "Epoch 14/100\n",
      "15658/15658 [==============================] - 468s 30ms/step - loss: 0.3376 - acc: 0.8565 - val_loss: 0.3816 - val_acc: 0.9048\n",
      "Epoch 15/100\n",
      "15658/15658 [==============================] - 468s 30ms/step - loss: 0.3346 - acc: 0.8571 - val_loss: 0.4141 - val_acc: 0.8995\n",
      "Epoch 16/100\n",
      "15658/15658 [==============================] - 467s 30ms/step - loss: 0.3344 - acc: 0.8541 - val_loss: 0.3349 - val_acc: 0.8998\n",
      "Epoch 17/100\n",
      "15658/15658 [==============================] - 467s 30ms/step - loss: 0.3339 - acc: 0.8566 - val_loss: 0.3901 - val_acc: 0.8953\n",
      "Epoch 18/100\n",
      "15658/15658 [==============================] - 473s 30ms/step - loss: 0.3276 - acc: 0.8603 - val_loss: 0.3692 - val_acc: 0.9058\n",
      "Epoch 19/100\n",
      "15658/15658 [==============================] - 469s 30ms/step - loss: 0.3254 - acc: 0.8592 - val_loss: 0.4163 - val_acc: 0.9023\n",
      "Epoch 20/100\n",
      "15658/15658 [==============================] - 465s 30ms/step - loss: 0.3223 - acc: 0.8589 - val_loss: 0.3495 - val_acc: 0.9020\n",
      "Epoch 21/100\n",
      "15658/15658 [==============================] - 469s 30ms/step - loss: 0.3208 - acc: 0.8620 - val_loss: 0.3873 - val_acc: 0.8985\n",
      "Epoch 22/100\n",
      "15658/15658 [==============================] - 467s 30ms/step - loss: 0.3188 - acc: 0.8622 - val_loss: 0.3713 - val_acc: 0.8980\n",
      "Epoch 23/100\n",
      "15658/15658 [==============================] - 471s 30ms/step - loss: 0.3177 - acc: 0.8633 - val_loss: 0.4497 - val_acc: 0.8873\n",
      "Epoch 24/100\n",
      "15658/15658 [==============================] - 469s 30ms/step - loss: 0.3139 - acc: 0.8632 - val_loss: 0.3424 - val_acc: 0.9085\n",
      "Epoch 25/100\n",
      "15658/15658 [==============================] - 469s 30ms/step - loss: 0.3135 - acc: 0.8614 - val_loss: 0.3561 - val_acc: 0.9020\n",
      "Epoch 26/100\n",
      "15658/15658 [==============================] - 468s 30ms/step - loss: 0.3115 - acc: 0.8663 - val_loss: 0.3643 - val_acc: 0.8965\n",
      "Epoch 27/100\n",
      "15658/15658 [==============================] - 467s 30ms/step - loss: 0.3088 - acc: 0.8671 - val_loss: 0.3916 - val_acc: 0.9027\n",
      "Epoch 28/100\n",
      "15658/15658 [==============================] - 468s 30ms/step - loss: 0.3065 - acc: 0.8667 - val_loss: 0.3224 - val_acc: 0.9067\n",
      "Epoch 29/100\n",
      "15658/15658 [==============================] - 469s 30ms/step - loss: 0.3051 - acc: 0.8711 - val_loss: 0.3191 - val_acc: 0.9048\n",
      "Epoch 30/100\n",
      "15658/15658 [==============================] - 468s 30ms/step - loss: 0.3007 - acc: 0.8695 - val_loss: 0.4074 - val_acc: 0.8870\n",
      "Epoch 31/100\n",
      "15658/15658 [==============================] - 464s 30ms/step - loss: 0.3000 - acc: 0.8718 - val_loss: 0.3401 - val_acc: 0.8982\n",
      "Epoch 32/100\n",
      "15658/15658 [==============================] - 470s 30ms/step - loss: 0.2957 - acc: 0.8749 - val_loss: 0.3025 - val_acc: 0.9055\n",
      "Epoch 33/100\n",
      "15658/15658 [==============================] - 466s 30ms/step - loss: 0.2979 - acc: 0.8728 - val_loss: 0.3835 - val_acc: 0.8978\n",
      "Epoch 34/100\n",
      "15658/15658 [==============================] - 465s 30ms/step - loss: 0.2946 - acc: 0.8727 - val_loss: 0.3349 - val_acc: 0.9025\n",
      "Epoch 35/100\n",
      "15658/15658 [==============================] - 461s 29ms/step - loss: 0.2911 - acc: 0.8734 - val_loss: 0.2780 - val_acc: 0.9092\n",
      "Epoch 36/100\n",
      "15658/15658 [==============================] - 464s 30ms/step - loss: 0.2878 - acc: 0.8767 - val_loss: 0.3501 - val_acc: 0.9015\n",
      "Epoch 37/100\n",
      "15658/15658 [==============================] - 462s 29ms/step - loss: 0.2877 - acc: 0.8757 - val_loss: 0.3916 - val_acc: 0.8895\n",
      "Epoch 38/100\n",
      "15658/15658 [==============================] - 463s 30ms/step - loss: 0.2856 - acc: 0.8776 - val_loss: 0.3462 - val_acc: 0.9080\n",
      "Epoch 39/100\n",
      "15658/15658 [==============================] - 464s 30ms/step - loss: 0.2829 - acc: 0.8818 - val_loss: 0.3112 - val_acc: 0.9058\n",
      "Epoch 40/100\n",
      "15658/15658 [==============================] - 466s 30ms/step - loss: 0.2816 - acc: 0.8806 - val_loss: 0.3627 - val_acc: 0.8940\n",
      "Epoch 41/100\n",
      "15658/15658 [==============================] - 468s 30ms/step - loss: 0.2761 - acc: 0.8817 - val_loss: 0.4114 - val_acc: 0.8968\n",
      "Epoch 42/100\n",
      "15658/15658 [==============================] - 467s 30ms/step - loss: 0.2806 - acc: 0.8778 - val_loss: 0.3424 - val_acc: 0.8932\n",
      "Epoch 43/100\n",
      "15658/15658 [==============================] - 461s 29ms/step - loss: 0.2761 - acc: 0.8796 - val_loss: 0.3617 - val_acc: 0.9058\n",
      "Epoch 44/100\n",
      "15658/15658 [==============================] - 459s 29ms/step - loss: 0.2765 - acc: 0.8791 - val_loss: 0.3949 - val_acc: 0.9050\n",
      "Epoch 45/100\n",
      "15658/15658 [==============================] - 460s 29ms/step - loss: 0.2709 - acc: 0.8826 - val_loss: 0.3279 - val_acc: 0.9027\n",
      "Epoch 46/100\n",
      "15658/15658 [==============================] - 461s 29ms/step - loss: 0.2669 - acc: 0.8843 - val_loss: 0.3836 - val_acc: 0.8880\n",
      "Epoch 47/100\n",
      "15658/15658 [==============================] - 465s 30ms/step - loss: 0.2676 - acc: 0.8827 - val_loss: 0.3109 - val_acc: 0.9060\n",
      "Epoch 48/100\n",
      "15658/15658 [==============================] - 461s 29ms/step - loss: 0.2641 - acc: 0.8866 - val_loss: 0.4093 - val_acc: 0.8975\n",
      "Epoch 49/100\n",
      "15658/15658 [==============================] - 461s 29ms/step - loss: 0.2610 - acc: 0.8892 - val_loss: 0.3331 - val_acc: 0.9062\n",
      "Epoch 50/100\n",
      "15658/15658 [==============================] - 462s 30ms/step - loss: 0.2593 - acc: 0.8878 - val_loss: 0.3779 - val_acc: 0.9002\n",
      "Epoch 51/100\n",
      "15658/15658 [==============================] - 465s 30ms/step - loss: 0.2578 - acc: 0.8898 - val_loss: 0.3743 - val_acc: 0.9023\n",
      "Epoch 52/100\n",
      "15658/15658 [==============================] - 463s 30ms/step - loss: 0.2542 - acc: 0.8910 - val_loss: 0.4058 - val_acc: 0.9008\n",
      "Epoch 53/100\n",
      "15658/15658 [==============================] - 464s 30ms/step - loss: 0.2551 - acc: 0.8890 - val_loss: 0.3947 - val_acc: 0.8992\n",
      "Epoch 54/100\n",
      "15658/15658 [==============================] - 461s 29ms/step - loss: 0.2494 - acc: 0.8926 - val_loss: 0.3650 - val_acc: 0.8863\n",
      "Epoch 55/100\n",
      "15658/15658 [==============================] - 464s 30ms/step - loss: 0.2443 - acc: 0.8975 - val_loss: 0.3897 - val_acc: 0.9010\n",
      "Epoch 00055: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbede3b0550>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = Bidirectional(LSTM(64,return_sequences=True))(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=20, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=100,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
