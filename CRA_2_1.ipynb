{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Convolution1D, Conv2D, MaxPooling2D, MaxPooling1D, LSTM, Embedding\n",
    "from keras.optimizers import SGD\n",
    "import keras.backend as K\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# config_proto = tf.ConfigProto(log_device_placement=0,allow_soft_placement=0)\n",
    "# config_proto.gpu_options.allow_growth = True\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../../20_Galaxy/ANGRY/2HXT-train.npy')\n",
    "Y_train = np.load('../../20_Galaxy/ANGRY/2HXT-train_label.npy')\n",
    "\n",
    "X_valid = np.load('../../20_Galaxy/ANGRY/2HXT-val.npy')\n",
    "Y_valid = np.load('../../20_Galaxy/ANGRY/2HXT-val_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "one_hot = preprocessing.OneHotEncoder(sparse = False)\n",
    "y_train = one_hot.fit_transform(Y_train)\n",
    "y_valid = one_hot.fit_transform(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_108:0\", shape=(?, 3640, 1), dtype=float32)\n",
      "(?, 910, 448)\n",
      "Tensor(\"strided_slice_110:0\", shape=(?, 3640, 1), dtype=float32)\n",
      "(?, 910, 1328)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_inbound_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0ec40df5cbfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim,depth=densenet_depth,\n\u001b[0;32m--> 105\u001b[0;31m                   growth_rate = densenet_growth_rate)\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-0ec40df5cbfa>\u001b[0m in \u001b[0;36mcreateDenseNet\u001b[0;34m(nb_classes, img_dim, depth, nb_dense_block, growth_rate, nb_filter, dropout_rate, weight_decay, verbose)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mdensenet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 231\u001b[0;31m             self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1364\u001b[0m                   \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m                   \u001b[0mnode_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m                   tensor_index=tensor_index)\n\u001b[0m\u001b[1;32m   1367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_in_decreasing_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1353\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcycle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdetected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \"\"\"\n\u001b[0;32m-> 1325\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \u001b[0;31m# Prevent cycles.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_inbound_nodes'"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "def conv_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    x = Activation('relu')(input)\n",
    "    x = Conv1D(nb_filter, (9), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    feature_list = [x]\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        x = conv_block(x, growth_rate, dropout_rate, weight_decay)\n",
    "        feature_list.append(x)\n",
    "        x = Concatenate(axis=concat_axis)(feature_list)\n",
    "        nb_filter += growth_rate\n",
    "\n",
    "    return x, nb_filter\n",
    "\n",
    "def transition_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    x = Conv1D(nb_filter, (1), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(input)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    x = AveragePooling1D((2), strides=(2))(x)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                           beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=16, dropout_rate=None,\n",
    "                     weight_decay=1E-4, verbose=True):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    for i in range(2):\n",
    "        model_input0 = model_input[:,i]\n",
    "        print model_input[:,i]\n",
    "    \n",
    "        concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "        assert (depth - 4) % 3 == 0, \"Depth must be 3 N + 4\"\n",
    "\n",
    "        # layers in each dense block\n",
    "        nb_layers = int((depth - 4) / 3)\n",
    "\n",
    "        # Initial convolution\n",
    "        x = Conv1D(nb_filter, (9), kernel_initializer=\"he_uniform\", padding=\"same\", name=\"initial_conv1D\", use_bias=False,\n",
    "                          kernel_regularizer=l2(weight_decay))(model_input0)\n",
    "\n",
    "        x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                                beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "        # Add dense blocks\n",
    "        for block_idx in range(nb_dense_block - 1):\n",
    "            x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                                       weight_decay=weight_decay)\n",
    "            # add transition_block\n",
    "            x = transition_block(x, nb_filter, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "        # The last dense_block does not have a transition_block\n",
    "        x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                                   weight_decay=weight_decay)\n",
    "\n",
    "        x = Activation('relu')(x)\n",
    "        if i == 0:\n",
    "            out = x\n",
    "        else:\n",
    "            out = Concatenate(axis=concat_axis)([out,x])\n",
    "        print out.shape\n",
    "#         x = LSTM(128,return_sequences=True)(x)\n",
    "    x = Flatten()(x)\n",
    "#     print x.shape\n",
    "#         x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(nb_classes, activation='softmax', kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(out)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    if verbose: \n",
    "        print(\"DenseNet-%d-%d created.\" % (depth, growth_rate))\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "densenet_depth = 40\n",
    "densenet_growth_rate = 12\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim,depth=densenet_depth,\n",
    "                  growth_rate = densenet_growth_rate)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=12, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 42s 3ms/step - loss: 0.5908 - acc: 0.6749 - val_loss: 0.5553 - val_acc: 0.8088\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 17s 1ms/step - loss: 0.4880 - acc: 0.7686 - val_loss: 0.5640 - val_acc: 0.8605\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.4562 - acc: 0.7916 - val_loss: 0.4511 - val_acc: 0.8633\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.4427 - acc: 0.8021 - val_loss: 0.4642 - val_acc: 0.8710\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.4298 - acc: 0.8064 - val_loss: 0.4687 - val_acc: 0.8390\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.4225 - acc: 0.8114 - val_loss: 0.5927 - val_acc: 0.8670\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.4099 - acc: 0.8173 - val_loss: 0.4624 - val_acc: 0.8568\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.4142 - acc: 0.8134 - val_loss: 0.4367 - val_acc: 0.8810\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 17s 1ms/step - loss: 0.3967 - acc: 0.8225 - val_loss: 0.3764 - val_acc: 0.8920\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.3913 - acc: 0.8306 - val_loss: 0.3676 - val_acc: 0.8828\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.3893 - acc: 0.8315 - val_loss: 0.3949 - val_acc: 0.8880\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.3835 - acc: 0.8338 - val_loss: 0.3685 - val_acc: 0.8665\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.3811 - acc: 0.8357 - val_loss: 0.4358 - val_acc: 0.8885\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.3760 - acc: 0.8394 - val_loss: 0.3316 - val_acc: 0.8955\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.3741 - acc: 0.8375 - val_loss: 0.3418 - val_acc: 0.8895\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.3725 - acc: 0.8393 - val_loss: 0.3460 - val_acc: 0.8925\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 19s 1ms/step - loss: 0.3717 - acc: 0.8368 - val_loss: 0.3410 - val_acc: 0.8972\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.3637 - acc: 0.8430 - val_loss: 0.3704 - val_acc: 0.8838\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 17s 1ms/step - loss: 0.3668 - acc: 0.8419 - val_loss: 0.3810 - val_acc: 0.8852\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.3594 - acc: 0.8462 - val_loss: 0.3883 - val_acc: 0.9090\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.3607 - acc: 0.8460 - val_loss: 0.4133 - val_acc: 0.8900\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 17s 1ms/step - loss: 0.3618 - acc: 0.8463 - val_loss: 0.3229 - val_acc: 0.8895\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.3589 - acc: 0.8493 - val_loss: 0.3272 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.3559 - acc: 0.8466 - val_loss: 0.3621 - val_acc: 0.8982\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 18s 1ms/step - loss: 0.3611 - acc: 0.8477 - val_loss: 0.3263 - val_acc: 0.8930\n",
      "Epoch 00025: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff3f468d7d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    cnn1 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "    cnn2 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "    pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "    drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    cnn3 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "    cnn4 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "    pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    cnn5 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "    cnn6 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "    pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "    drop3 = Dropout(0.25)(pool3)\n",
    "    \n",
    "    flatten1 = Flatten()(drop3)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "# densenet_depth = 46\n",
    "# densenet_growth_rate = 8\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"pre_4/strided_slice:0\", shape=(?, 3640, 1), dtype=float32)\n",
      "Tensor(\"input_62:0\", shape=(?, 12, 3640, 1), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 39s 2ms/step - loss: 0.6924 - acc: 0.5236 - val_loss: 0.6939 - val_acc: 0.5015\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 11s 674us/step - loss: 0.6911 - acc: 0.5287 - val_loss: 0.6941 - val_acc: 0.5008\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 10s 665us/step - loss: 0.6909 - acc: 0.5289 - val_loss: 0.6929 - val_acc: 0.5005\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 10s 662us/step - loss: 0.6904 - acc: 0.5287 - val_loss: 0.6914 - val_acc: 0.5000\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 10s 659us/step - loss: 0.6895 - acc: 0.5291 - val_loss: 0.6896 - val_acc: 0.4998\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 10s 650us/step - loss: 0.6879 - acc: 0.5284 - val_loss: 0.6855 - val_acc: 0.4998\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 11s 696us/step - loss: 0.6856 - acc: 0.5339 - val_loss: 0.6795 - val_acc: 0.5978\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 11s 719us/step - loss: 0.6809 - acc: 0.5477 - val_loss: 0.6647 - val_acc: 0.6630\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 11s 674us/step - loss: 0.6638 - acc: 0.5955 - val_loss: 0.5979 - val_acc: 0.7435\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 11s 708us/step - loss: 0.6240 - acc: 0.6575 - val_loss: 0.5298 - val_acc: 0.7698\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 11s 681us/step - loss: 0.5970 - acc: 0.6846 - val_loss: 0.5221 - val_acc: 0.7722\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 11s 685us/step - loss: 0.5787 - acc: 0.6986 - val_loss: 0.4892 - val_acc: 0.8010\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 11s 695us/step - loss: 0.5667 - acc: 0.7083 - val_loss: 0.4922 - val_acc: 0.8027\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 11s 679us/step - loss: 0.5503 - acc: 0.7200 - val_loss: 0.4846 - val_acc: 0.8197\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 12s 740us/step - loss: 0.5376 - acc: 0.7265 - val_loss: 0.4882 - val_acc: 0.8227\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 12s 738us/step - loss: 0.5276 - acc: 0.7311 - val_loss: 0.4942 - val_acc: 0.8267\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 11s 703us/step - loss: 0.5228 - acc: 0.7364 - val_loss: 0.5020 - val_acc: 0.8200\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 11s 724us/step - loss: 0.5126 - acc: 0.7434 - val_loss: 0.5296 - val_acc: 0.8255\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 11s 713us/step - loss: 0.5086 - acc: 0.7476 - val_loss: 0.4847 - val_acc: 0.8398\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 12s 735us/step - loss: 0.5027 - acc: 0.7489 - val_loss: 0.5449 - val_acc: 0.8090\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 10s 641us/step - loss: 0.4958 - acc: 0.7562 - val_loss: 0.5011 - val_acc: 0.8438\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 10s 670us/step - loss: 0.4905 - acc: 0.7583 - val_loss: 0.4991 - val_acc: 0.8477\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 10s 632us/step - loss: 0.4875 - acc: 0.7625 - val_loss: 0.5044 - val_acc: 0.8502\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 10s 643us/step - loss: 0.4807 - acc: 0.7669 - val_loss: 0.4717 - val_acc: 0.8532\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 10s 628us/step - loss: 0.4793 - acc: 0.7671 - val_loss: 0.4818 - val_acc: 0.8485\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 10s 629us/step - loss: 0.4797 - acc: 0.7687 - val_loss: 0.5298 - val_acc: 0.8568\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 10s 646us/step - loss: 0.4704 - acc: 0.7771 - val_loss: 0.5168 - val_acc: 0.8582\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 10s 629us/step - loss: 0.4731 - acc: 0.7772 - val_loss: 0.4703 - val_acc: 0.8655\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 10s 625us/step - loss: 0.4675 - acc: 0.7804 - val_loss: 0.5306 - val_acc: 0.8620\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 10s 627us/step - loss: 0.4608 - acc: 0.7815 - val_loss: 0.4846 - val_acc: 0.8665\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 10s 630us/step - loss: 0.4620 - acc: 0.7806 - val_loss: 0.4895 - val_acc: 0.8680\n",
      "Epoch 32/50\n",
      "15658/15658 [==============================] - 10s 634us/step - loss: 0.4625 - acc: 0.7807 - val_loss: 0.4836 - val_acc: 0.8700\n",
      "Epoch 33/50\n",
      "15658/15658 [==============================] - 10s 627us/step - loss: 0.4559 - acc: 0.7862 - val_loss: 0.5132 - val_acc: 0.8672\n",
      "Epoch 34/50\n",
      "15658/15658 [==============================] - 10s 623us/step - loss: 0.4537 - acc: 0.7891 - val_loss: 0.4577 - val_acc: 0.8707\n",
      "Epoch 35/50\n",
      "15658/15658 [==============================] - 10s 631us/step - loss: 0.4523 - acc: 0.7869 - val_loss: 0.4725 - val_acc: 0.8718\n",
      "Epoch 36/50\n",
      "15658/15658 [==============================] - 10s 627us/step - loss: 0.4470 - acc: 0.7912 - val_loss: 0.4733 - val_acc: 0.8785\n",
      "Epoch 37/50\n",
      "15658/15658 [==============================] - 10s 630us/step - loss: 0.4449 - acc: 0.7942 - val_loss: 0.4528 - val_acc: 0.8780\n",
      "Epoch 38/50\n",
      "15658/15658 [==============================] - 10s 633us/step - loss: 0.4432 - acc: 0.7961 - val_loss: 0.4549 - val_acc: 0.8782\n",
      "Epoch 39/50\n",
      "15658/15658 [==============================] - 10s 632us/step - loss: 0.4431 - acc: 0.7946 - val_loss: 0.4565 - val_acc: 0.8775\n",
      "Epoch 40/50\n",
      "15658/15658 [==============================] - 10s 627us/step - loss: 0.4404 - acc: 0.7953 - val_loss: 0.4220 - val_acc: 0.8835\n",
      "Epoch 41/50\n",
      "15658/15658 [==============================] - 10s 619us/step - loss: 0.4341 - acc: 0.7990 - val_loss: 0.4102 - val_acc: 0.8840\n",
      "Epoch 42/50\n",
      "15658/15658 [==============================] - 10s 628us/step - loss: 0.4325 - acc: 0.8021 - val_loss: 0.4027 - val_acc: 0.8828\n",
      "Epoch 43/50\n",
      "15658/15658 [==============================] - 10s 631us/step - loss: 0.4335 - acc: 0.8013 - val_loss: 0.3952 - val_acc: 0.8808\n",
      "Epoch 44/50\n",
      "15658/15658 [==============================] - 10s 631us/step - loss: 0.4280 - acc: 0.8031 - val_loss: 0.4496 - val_acc: 0.8858\n",
      "Epoch 45/50\n",
      "15658/15658 [==============================] - 10s 618us/step - loss: 0.4324 - acc: 0.7992 - val_loss: 0.4431 - val_acc: 0.8793\n",
      "Epoch 46/50\n",
      "15658/15658 [==============================] - 10s 628us/step - loss: 0.4282 - acc: 0.8054 - val_loss: 0.4216 - val_acc: 0.8795\n",
      "Epoch 47/50\n",
      "15658/15658 [==============================] - 10s 621us/step - loss: 0.4289 - acc: 0.8042 - val_loss: 0.4071 - val_acc: 0.8928\n",
      "Epoch 48/50\n",
      "15658/15658 [==============================] - 10s 620us/step - loss: 0.4274 - acc: 0.8030 - val_loss: 0.3706 - val_acc: 0.8873\n",
      "Epoch 49/50\n",
      "15658/15658 [==============================] - 10s 629us/step - loss: 0.4250 - acc: 0.8034 - val_loss: 0.3986 - val_acc: 0.8902\n",
      "Epoch 50/50\n",
      "15658/15658 [==============================] - 10s 629us/step - loss: 0.4228 - acc: 0.8068 - val_loss: 0.3889 - val_acc: 0.8892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff3d8e0cb90>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def pre(x):\n",
    "    x = x[:,0]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "#     for i in range(1):\n",
    "    inputs = Lambda(pre, name='pre')(model_input)\n",
    "    print inputs\n",
    "    print model_input\n",
    "    \n",
    "    cnn1 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "    cnn2 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "    pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "    drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    cnn3 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "    cnn4 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "    pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    cnn5 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "    cnn6 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "    pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "    drop3 = Dropout(0.25)(pool3)\n",
    "    \n",
    "    flatten1 = Flatten()(drop3)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=25, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lambda_1/strided_slice:0\", shape=(?, 3640, 1), dtype=float32)\n",
      "Tensor(\"input_67:0\", shape=(?, 12, 3640, 1), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.6926 - acc: 0.5158 - val_loss: 0.6943 - val_acc: 0.5010\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 10s 646us/step - loss: 0.6910 - acc: 0.5289 - val_loss: 0.6930 - val_acc: 0.5010\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 10s 651us/step - loss: 0.6899 - acc: 0.5293 - val_loss: 0.6910 - val_acc: 0.5010\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 10s 660us/step - loss: 0.6877 - acc: 0.5349 - val_loss: 0.6867 - val_acc: 0.5230\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 10s 644us/step - loss: 0.6831 - acc: 0.5570 - val_loss: 0.6825 - val_acc: 0.5573\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 10s 634us/step - loss: 0.6690 - acc: 0.6022 - val_loss: 0.6582 - val_acc: 0.5955\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 10s 641us/step - loss: 0.6421 - acc: 0.6352 - val_loss: 0.6307 - val_acc: 0.6225\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 10s 637us/step - loss: 0.6101 - acc: 0.6714 - val_loss: 0.5984 - val_acc: 0.7033\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 10s 651us/step - loss: 0.5931 - acc: 0.6839 - val_loss: 0.5450 - val_acc: 0.7718\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 11s 673us/step - loss: 0.5778 - acc: 0.6992 - val_loss: 0.5327 - val_acc: 0.7808\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 10s 663us/step - loss: 0.5665 - acc: 0.7058 - val_loss: 0.5250 - val_acc: 0.7995\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 10s 669us/step - loss: 0.5588 - acc: 0.7187 - val_loss: 0.4977 - val_acc: 0.8150\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 10s 669us/step - loss: 0.5437 - acc: 0.7232 - val_loss: 0.4936 - val_acc: 0.8210\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 10s 661us/step - loss: 0.5318 - acc: 0.7344 - val_loss: 0.5483 - val_acc: 0.7887\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 10s 666us/step - loss: 0.5184 - acc: 0.7431 - val_loss: 0.4831 - val_acc: 0.8303\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 10s 660us/step - loss: 0.5139 - acc: 0.7482 - val_loss: 0.5013 - val_acc: 0.8387\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 10s 639us/step - loss: 0.5058 - acc: 0.7548 - val_loss: 0.5066 - val_acc: 0.8395\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 10s 643us/step - loss: 0.4970 - acc: 0.7621 - val_loss: 0.5058 - val_acc: 0.8520\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 10s 656us/step - loss: 0.4915 - acc: 0.7630 - val_loss: 0.4834 - val_acc: 0.8452\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 10s 646us/step - loss: 0.4827 - acc: 0.7677 - val_loss: 0.4996 - val_acc: 0.8538\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 11s 673us/step - loss: 0.4818 - acc: 0.7683 - val_loss: 0.5006 - val_acc: 0.8595\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 10s 667us/step - loss: 0.4790 - acc: 0.7671 - val_loss: 0.5431 - val_acc: 0.8575\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 11s 674us/step - loss: 0.4731 - acc: 0.7735 - val_loss: 0.5954 - val_acc: 0.8450\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 10s 669us/step - loss: 0.4683 - acc: 0.7774 - val_loss: 0.5291 - val_acc: 0.8610\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 10s 659us/step - loss: 0.4682 - acc: 0.7777 - val_loss: 0.5010 - val_acc: 0.8645\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 10s 661us/step - loss: 0.4607 - acc: 0.7824 - val_loss: 0.5221 - val_acc: 0.8668\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 10s 655us/step - loss: 0.4601 - acc: 0.7816 - val_loss: 0.5576 - val_acc: 0.8615\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 10s 654us/step - loss: 0.4543 - acc: 0.7880 - val_loss: 0.5172 - val_acc: 0.8688\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 10s 656us/step - loss: 0.4557 - acc: 0.7856 - val_loss: 0.5308 - val_acc: 0.8675\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 10s 646us/step - loss: 0.4517 - acc: 0.7845 - val_loss: 0.5343 - val_acc: 0.8725\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 10s 645us/step - loss: 0.4510 - acc: 0.7915 - val_loss: 0.5234 - val_acc: 0.8688\n",
      "Epoch 32/50\n",
      "15658/15658 [==============================] - 10s 662us/step - loss: 0.4474 - acc: 0.7907 - val_loss: 0.5210 - val_acc: 0.8735\n",
      "Epoch 33/50\n",
      "15658/15658 [==============================] - 10s 662us/step - loss: 0.4422 - acc: 0.7944 - val_loss: 0.5020 - val_acc: 0.8745\n",
      "Epoch 34/50\n",
      "15658/15658 [==============================] - 10s 665us/step - loss: 0.4416 - acc: 0.7980 - val_loss: 0.5281 - val_acc: 0.8707\n",
      "Epoch 35/50\n",
      "15658/15658 [==============================] - 10s 670us/step - loss: 0.4417 - acc: 0.7972 - val_loss: 0.5145 - val_acc: 0.8760\n",
      "Epoch 36/50\n",
      "15658/15658 [==============================] - 10s 660us/step - loss: 0.4422 - acc: 0.7958 - val_loss: 0.5146 - val_acc: 0.8765\n",
      "Epoch 37/50\n",
      "15658/15658 [==============================] - 10s 661us/step - loss: 0.4384 - acc: 0.7980 - val_loss: 0.4867 - val_acc: 0.8690\n",
      "Epoch 38/50\n",
      "15658/15658 [==============================] - 10s 667us/step - loss: 0.4363 - acc: 0.7995 - val_loss: 0.5081 - val_acc: 0.8728\n",
      "Epoch 39/50\n",
      "15658/15658 [==============================] - 10s 667us/step - loss: 0.4319 - acc: 0.7973 - val_loss: 0.4964 - val_acc: 0.8770\n",
      "Epoch 40/50\n",
      "15658/15658 [==============================] - 11s 685us/step - loss: 0.4353 - acc: 0.8002 - val_loss: 0.4964 - val_acc: 0.8720\n",
      "Epoch 41/50\n",
      "15658/15658 [==============================] - 10s 670us/step - loss: 0.4305 - acc: 0.8046 - val_loss: 0.5057 - val_acc: 0.8635\n",
      "Epoch 42/50\n",
      "15658/15658 [==============================] - 10s 660us/step - loss: 0.4294 - acc: 0.8048 - val_loss: 0.5088 - val_acc: 0.8750\n",
      "Epoch 43/50\n",
      "15658/15658 [==============================] - 11s 679us/step - loss: 0.4287 - acc: 0.8062 - val_loss: 0.5097 - val_acc: 0.8778\n",
      "Epoch 44/50\n",
      "15658/15658 [==============================] - 10s 667us/step - loss: 0.4225 - acc: 0.8069 - val_loss: 0.5183 - val_acc: 0.8772\n",
      "Epoch 45/50\n",
      "15658/15658 [==============================] - 10s 665us/step - loss: 0.4258 - acc: 0.8048 - val_loss: 0.5482 - val_acc: 0.8795\n",
      "Epoch 46/50\n",
      "15658/15658 [==============================] - 11s 680us/step - loss: 0.4221 - acc: 0.8080 - val_loss: 0.5266 - val_acc: 0.8805\n",
      "Epoch 47/50\n",
      "15658/15658 [==============================] - 10s 669us/step - loss: 0.4207 - acc: 0.8073 - val_loss: 0.5011 - val_acc: 0.8830\n",
      "Epoch 48/50\n",
      "15658/15658 [==============================] - 10s 669us/step - loss: 0.4192 - acc: 0.8091 - val_loss: 0.5393 - val_acc: 0.8850\n",
      "Epoch 49/50\n",
      "15658/15658 [==============================] - 10s 661us/step - loss: 0.4196 - acc: 0.8098 - val_loss: 0.4945 - val_acc: 0.8665\n",
      "Epoch 50/50\n",
      "15658/15658 [==============================] - 10s 671us/step - loss: 0.4211 - acc: 0.8108 - val_loss: 0.4997 - val_acc: 0.8782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff3d83d0310>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "#     for i in range(1):\n",
    "    inputs = Lambda(cut, arguments={'index':0})(model_input)\n",
    "    print inputs\n",
    "    print model_input\n",
    "    \n",
    "    cnn1 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "    cnn2 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "    pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "    drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    cnn3 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "    cnn4 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "    pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    cnn5 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "    cnn6 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "    pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "    drop3 = Dropout(0.25)(pool3)\n",
    "    \n",
    "    flatten1 = Flatten()(drop3)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=25, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_13/concat:0\", shape=(?, 10, 60), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 63s 4ms/step - loss: 0.6231 - acc: 0.6334 - val_loss: 0.5033 - val_acc: 0.7688\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.5270 - acc: 0.7360 - val_loss: 0.4671 - val_acc: 0.8203\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.4951 - acc: 0.7599 - val_loss: 0.4753 - val_acc: 0.8367\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.4789 - acc: 0.7682 - val_loss: 0.4689 - val_acc: 0.8227\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.4644 - acc: 0.7786 - val_loss: 0.5169 - val_acc: 0.8590\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 38s 2ms/step - loss: 0.4506 - acc: 0.7865 - val_loss: 0.5555 - val_acc: 0.8707\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.4303 - acc: 0.8025 - val_loss: 0.6081 - val_acc: 0.8660\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.4174 - acc: 0.8108 - val_loss: 0.6506 - val_acc: 0.8745\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.4108 - acc: 0.8150 - val_loss: 0.5826 - val_acc: 0.8732\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.4068 - acc: 0.8184 - val_loss: 0.5869 - val_acc: 0.8590\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 38s 2ms/step - loss: 0.3983 - acc: 0.8219 - val_loss: 0.4772 - val_acc: 0.8892\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 38s 2ms/step - loss: 0.3913 - acc: 0.8247 - val_loss: 0.4839 - val_acc: 0.8842\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.3859 - acc: 0.8324 - val_loss: 0.4650 - val_acc: 0.8925\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.3784 - acc: 0.8306 - val_loss: 0.5097 - val_acc: 0.8850\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.3819 - acc: 0.8340 - val_loss: 0.4137 - val_acc: 0.8878\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.3737 - acc: 0.8385 - val_loss: 0.5254 - val_acc: 0.8910\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.3736 - acc: 0.8356 - val_loss: 0.4562 - val_acc: 0.8758\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.3695 - acc: 0.8385 - val_loss: 0.3666 - val_acc: 0.8805\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1ecd482290>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(ROWS):\n",
    "        inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    flatten1 = Flatten()(concat)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_18/concat:0\", shape=(?, 6, 60), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.6035 - acc: 0.6653 - val_loss: 0.4721 - val_acc: 0.8290\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 38s 2ms/step - loss: 0.5068 - acc: 0.7574 - val_loss: 0.4005 - val_acc: 0.8345\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 39s 2ms/step - loss: 0.4567 - acc: 0.7889 - val_loss: 0.3656 - val_acc: 0.8768\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 39s 2ms/step - loss: 0.4243 - acc: 0.8105 - val_loss: 0.3522 - val_acc: 0.8800\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 39s 2ms/step - loss: 0.4065 - acc: 0.8203 - val_loss: 0.3052 - val_acc: 0.8908\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 39s 2ms/step - loss: 0.3949 - acc: 0.8289 - val_loss: 0.2905 - val_acc: 0.8915\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 39s 2ms/step - loss: 0.3895 - acc: 0.8299 - val_loss: 0.3171 - val_acc: 0.8892\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 39s 2ms/step - loss: 0.3887 - acc: 0.8332 - val_loss: 0.3011 - val_acc: 0.8912\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3799 - acc: 0.8360 - val_loss: 0.2880 - val_acc: 0.8970\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 39s 2ms/step - loss: 0.3776 - acc: 0.8347 - val_loss: 0.2937 - val_acc: 0.8928\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 39s 3ms/step - loss: 0.3699 - acc: 0.8417 - val_loss: 0.2897 - val_acc: 0.8922\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3684 - acc: 0.8403 - val_loss: 0.2819 - val_acc: 0.8995\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3638 - acc: 0.8449 - val_loss: 0.3063 - val_acc: 0.8895\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 39s 2ms/step - loss: 0.3626 - acc: 0.8465 - val_loss: 0.2817 - val_acc: 0.8988\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 38s 2ms/step - loss: 0.3636 - acc: 0.8431 - val_loss: 0.3134 - val_acc: 0.9002\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 38s 2ms/step - loss: 0.3606 - acc: 0.8430 - val_loss: 0.3101 - val_acc: 0.8930\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 38s 2ms/step - loss: 0.3607 - acc: 0.8451 - val_loss: 0.2928 - val_acc: 0.8922\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 37s 2ms/step - loss: 0.3551 - acc: 0.8495 - val_loss: 0.3195 - val_acc: 0.8860\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 38s 2ms/step - loss: 0.3579 - acc: 0.8483 - val_loss: 0.3043 - val_acc: 0.8950\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 38s 2ms/step - loss: 0.3561 - acc: 0.8472 - val_loss: 0.3141 - val_acc: 0.8982\n",
      "Epoch 00020: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e84813150>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(ROWS):\n",
    "        inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(8))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(7))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(7))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    flatten1 = Flatten()(concat)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_25/concat:0\", shape=(?, 120, 5), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 628s 40ms/step - loss: 0.5684 - acc: 0.6874 - val_loss: 0.4242 - val_acc: 0.8458\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 587s 37ms/step - loss: 0.4549 - acc: 0.7869 - val_loss: 0.4177 - val_acc: 0.8748\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 552s 35ms/step - loss: 0.4275 - acc: 0.8028 - val_loss: 0.4810 - val_acc: 0.8758\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 605s 39ms/step - loss: 0.4067 - acc: 0.8176 - val_loss: 0.4531 - val_acc: 0.8608\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 602s 38ms/step - loss: 0.3982 - acc: 0.8215 - val_loss: 0.3703 - val_acc: 0.8868\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 558s 36ms/step - loss: 0.3910 - acc: 0.8263 - val_loss: 0.3263 - val_acc: 0.8788\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 547s 35ms/step - loss: 0.3837 - acc: 0.8285 - val_loss: 0.4206 - val_acc: 0.8778\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 560s 36ms/step - loss: 0.3790 - acc: 0.8345 - val_loss: 0.3404 - val_acc: 0.8858\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 559s 36ms/step - loss: 0.3804 - acc: 0.8348 - val_loss: 0.3516 - val_acc: 0.8795\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 558s 36ms/step - loss: 0.3769 - acc: 0.8334 - val_loss: 0.3987 - val_acc: 0.8865\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e1c7cc1d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(ROWS):\n",
    "        inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(6, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(7, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(5, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=1)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(32,return_sequences=True)(concat)\n",
    "\n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 8\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 79s 5ms/step - loss: 0.5721 - acc: 0.6871 - val_loss: 0.5640 - val_acc: 0.8340\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 39s 3ms/step - loss: 0.4426 - acc: 0.8010 - val_loss: 0.4232 - val_acc: 0.8772\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.4058 - acc: 0.8215 - val_loss: 0.4238 - val_acc: 0.8852\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3766 - acc: 0.8358 - val_loss: 0.3791 - val_acc: 0.8878\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3639 - acc: 0.8422 - val_loss: 0.3318 - val_acc: 0.8960\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3535 - acc: 0.8479 - val_loss: 0.3634 - val_acc: 0.8910\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3458 - acc: 0.8532 - val_loss: 0.3292 - val_acc: 0.8995\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3409 - acc: 0.8552 - val_loss: 0.3028 - val_acc: 0.9020\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3355 - acc: 0.8575 - val_loss: 0.3454 - val_acc: 0.9045\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3363 - acc: 0.8563 - val_loss: 0.3004 - val_acc: 0.9092\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3330 - acc: 0.8560 - val_loss: 0.3626 - val_acc: 0.9067\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3275 - acc: 0.8631 - val_loss: 0.3298 - val_acc: 0.9055\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3310 - acc: 0.8602 - val_loss: 0.4217 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3289 - acc: 0.8601 - val_loss: 0.2966 - val_acc: 0.9038\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 40s 3ms/step - loss: 0.3254 - acc: 0.8626 - val_loss: 0.2674 - val_acc: 0.9075\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e0a675950>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    cnn1 = Convolution1D(24, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "    cnn2 = Convolution1D(24, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "    pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "    drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    cnn3 = Convolution1D(28, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "    cnn4 = Convolution1D(28, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "    pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    cnn5 = Convolution1D(20, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "    cnn6 = Convolution1D(20, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "    pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "    drop3 = Dropout(0.25)(pool3)\n",
    "    \n",
    "    flatten1 = Flatten()(drop3)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "# densenet_depth = 46\n",
    "# densenet_growth_rate = 8\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 114s 7ms/step - loss: 0.5684 - acc: 0.6943 - val_loss: 0.4515 - val_acc: 0.8285\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 73s 5ms/step - loss: 0.4610 - acc: 0.7850 - val_loss: 0.4700 - val_acc: 0.8582\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.4138 - acc: 0.8157 - val_loss: 0.4492 - val_acc: 0.8838\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3878 - acc: 0.8304 - val_loss: 0.3525 - val_acc: 0.8915\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3687 - acc: 0.8396 - val_loss: 0.3950 - val_acc: 0.8978\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3571 - acc: 0.8467 - val_loss: 0.2954 - val_acc: 0.9040\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3469 - acc: 0.8518 - val_loss: 0.3125 - val_acc: 0.8942\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3423 - acc: 0.8523 - val_loss: 0.5149 - val_acc: 0.7935\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3381 - acc: 0.8564 - val_loss: 0.2989 - val_acc: 0.8842\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3350 - acc: 0.8581 - val_loss: 0.4078 - val_acc: 0.8718\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3284 - acc: 0.8584 - val_loss: 0.3013 - val_acc: 0.9075\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3327 - acc: 0.8630 - val_loss: 0.2991 - val_acc: 0.9115\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3235 - acc: 0.8621 - val_loss: 0.3836 - val_acc: 0.8898\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3189 - acc: 0.8656 - val_loss: 0.2759 - val_acc: 0.9123\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 73s 5ms/step - loss: 0.3153 - acc: 0.8664 - val_loss: 0.3273 - val_acc: 0.9048\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3147 - acc: 0.8684 - val_loss: 0.3265 - val_acc: 0.9010\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3113 - acc: 0.8672 - val_loss: 0.2958 - val_acc: 0.9073\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3088 - acc: 0.8696 - val_loss: 0.2967 - val_acc: 0.9105\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3032 - acc: 0.8725 - val_loss: 0.4311 - val_acc: 0.8637\n",
      "Epoch 00019: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e0897b750>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "    cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "    pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "    drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "    cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "    pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "    cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "    pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "    drop3 = Dropout(0.25)(pool3)\n",
    "    \n",
    "    flatten1 = Flatten()(drop3)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "# densenet_depth = 46\n",
    "# densenet_growth_rate = 8\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 118s 8ms/step - loss: 0.6048 - acc: 0.6558 - val_loss: 0.4894 - val_acc: 0.8240\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.4799 - acc: 0.7718 - val_loss: 0.4241 - val_acc: 0.8622\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.4371 - acc: 0.7993 - val_loss: 0.3705 - val_acc: 0.8728\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.4065 - acc: 0.8202 - val_loss: 0.4089 - val_acc: 0.8790\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3865 - acc: 0.8313 - val_loss: 0.4643 - val_acc: 0.8665\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3690 - acc: 0.8382 - val_loss: 0.3088 - val_acc: 0.8835\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3570 - acc: 0.8459 - val_loss: 0.3499 - val_acc: 0.8990\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3441 - acc: 0.8523 - val_loss: 0.3118 - val_acc: 0.8978\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3383 - acc: 0.8575 - val_loss: 0.3378 - val_acc: 0.8947\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3306 - acc: 0.8592 - val_loss: 0.3253 - val_acc: 0.9012\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3234 - acc: 0.8638 - val_loss: 0.3667 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3210 - acc: 0.8659 - val_loss: 0.3275 - val_acc: 0.9010\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3195 - acc: 0.8627 - val_loss: 0.3044 - val_acc: 0.9038\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3140 - acc: 0.8664 - val_loss: 0.2840 - val_acc: 0.9113\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3109 - acc: 0.8664 - val_loss: 0.2817 - val_acc: 0.9115\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3051 - acc: 0.8668 - val_loss: 0.3509 - val_acc: 0.9040\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3048 - acc: 0.8679 - val_loss: 0.4322 - val_acc: 0.8543\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.3004 - acc: 0.8719 - val_loss: 0.2559 - val_acc: 0.9123\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.2956 - acc: 0.8731 - val_loss: 0.3006 - val_acc: 0.9035\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.2908 - acc: 0.8774 - val_loss: 0.3163 - val_acc: 0.8890\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.2910 - acc: 0.8754 - val_loss: 0.2439 - val_acc: 0.9165\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.2871 - acc: 0.8764 - val_loss: 0.3217 - val_acc: 0.9090\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 71s 5ms/step - loss: 0.2902 - acc: 0.8762 - val_loss: 0.3227 - val_acc: 0.8985\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.2839 - acc: 0.8795 - val_loss: 0.3356 - val_acc: 0.9083\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.2832 - acc: 0.8762 - val_loss: 0.3484 - val_acc: 0.9035\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.2832 - acc: 0.8785 - val_loss: 0.4373 - val_acc: 0.8863\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.2752 - acc: 0.8828 - val_loss: 0.3360 - val_acc: 0.9010\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.2755 - acc: 0.8822 - val_loss: 0.3544 - val_acc: 0.8998\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.2728 - acc: 0.8822 - val_loss: 0.3470 - val_acc: 0.8945\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.2706 - acc: 0.8804 - val_loss: 0.4095 - val_acc: 0.8980\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 72s 5ms/step - loss: 0.2683 - acc: 0.8833 - val_loss: 0.3278 - val_acc: 0.9067\n",
      "Epoch 00031: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1dfcebc910>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "    cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "    pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "    drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "    cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "    pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "    cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "    pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "    drop3 = Dropout(0.25)(pool3)\n",
    "    \n",
    "    flatten1 = Flatten()(drop3)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "# densenet_depth = 46\n",
    "# densenet_growth_rate = 8\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 191s 12ms/step - loss: 0.6046 - acc: 0.6434 - val_loss: 0.4563 - val_acc: 0.8183\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 141s 9ms/step - loss: 0.4709 - acc: 0.7755 - val_loss: 0.4471 - val_acc: 0.8675\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 141s 9ms/step - loss: 0.4230 - acc: 0.8092 - val_loss: 0.4463 - val_acc: 0.8848\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.3898 - acc: 0.8262 - val_loss: 0.4102 - val_acc: 0.8880\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.3709 - acc: 0.8387 - val_loss: 0.4229 - val_acc: 0.8955\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3562 - acc: 0.8497 - val_loss: 0.4178 - val_acc: 0.8990\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3496 - acc: 0.8501 - val_loss: 0.3685 - val_acc: 0.9045\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 146s 9ms/step - loss: 0.3394 - acc: 0.8565 - val_loss: 0.3670 - val_acc: 0.9055\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 141s 9ms/step - loss: 0.3356 - acc: 0.8565 - val_loss: 0.3469 - val_acc: 0.9020\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 141s 9ms/step - loss: 0.3291 - acc: 0.8593 - val_loss: 0.3540 - val_acc: 0.9028\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3272 - acc: 0.8582 - val_loss: 0.2974 - val_acc: 0.9080\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3212 - acc: 0.8629 - val_loss: 0.2860 - val_acc: 0.9087\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3203 - acc: 0.8637 - val_loss: 0.2746 - val_acc: 0.9107\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3164 - acc: 0.8662 - val_loss: 0.3437 - val_acc: 0.9050\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3136 - acc: 0.8656 - val_loss: 0.3070 - val_acc: 0.8960\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.3091 - acc: 0.8697 - val_loss: 0.3163 - val_acc: 0.9068\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.3034 - acc: 0.8693 - val_loss: 0.3346 - val_acc: 0.9080\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.3030 - acc: 0.8697 - val_loss: 0.2850 - val_acc: 0.9137\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.3023 - acc: 0.8705 - val_loss: 0.3571 - val_acc: 0.9068\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.2971 - acc: 0.8746 - val_loss: 0.2895 - val_acc: 0.9137\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.2972 - acc: 0.8713 - val_loss: 0.2961 - val_acc: 0.9120\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.2942 - acc: 0.8737 - val_loss: 0.2918 - val_acc: 0.9060\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.2912 - acc: 0.8759 - val_loss: 0.2865 - val_acc: 0.9008\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.2895 - acc: 0.8755 - val_loss: 0.3924 - val_acc: 0.9073\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.2887 - acc: 0.8758 - val_loss: 0.2601 - val_acc: 0.9092\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 141s 9ms/step - loss: 0.2866 - acc: 0.8780 - val_loss: 0.3120 - val_acc: 0.9103\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 141s 9ms/step - loss: 0.2839 - acc: 0.8805 - val_loss: 0.2999 - val_acc: 0.9103\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 141s 9ms/step - loss: 0.2810 - acc: 0.8802 - val_loss: 0.2973 - val_acc: 0.9115\n",
      "Epoch 00028: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1ddc60f5d0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "    cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "    pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "    drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "    cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "    pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "    cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "    pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "    drop3 = Dropout(0.25)(pool3)\n",
    "    \n",
    "    drop3 = LSTM(64,return_sequences=True)(drop3)\n",
    "    \n",
    "    flatten1 = Flatten()(drop3)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 64\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "# densenet_depth = 46\n",
    "# densenet_growth_rate = 8\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 191s 12ms/step - loss: 0.6176 - acc: 0.6370 - val_loss: 0.5732 - val_acc: 0.7113\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 141s 9ms/step - loss: 0.4820 - acc: 0.7673 - val_loss: 0.5084 - val_acc: 0.8645\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.4301 - acc: 0.8043 - val_loss: 0.4753 - val_acc: 0.8760\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3989 - acc: 0.8219 - val_loss: 0.4674 - val_acc: 0.8877\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3774 - acc: 0.8332 - val_loss: 0.4233 - val_acc: 0.8930\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3635 - acc: 0.8432 - val_loss: 0.4139 - val_acc: 0.8962\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3533 - acc: 0.8474 - val_loss: 0.4063 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3452 - acc: 0.8512 - val_loss: 0.3721 - val_acc: 0.8978\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 144s 9ms/step - loss: 0.3394 - acc: 0.8528 - val_loss: 0.3495 - val_acc: 0.9015\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3299 - acc: 0.8580 - val_loss: 0.3554 - val_acc: 0.8990\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3327 - acc: 0.8546 - val_loss: 0.3735 - val_acc: 0.9018\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3211 - acc: 0.8645 - val_loss: 0.3090 - val_acc: 0.9068\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3190 - acc: 0.8662 - val_loss: 0.3627 - val_acc: 0.8985\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3138 - acc: 0.8658 - val_loss: 0.3302 - val_acc: 0.9018\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 144s 9ms/step - loss: 0.3134 - acc: 0.8665 - val_loss: 0.3467 - val_acc: 0.9060\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3086 - acc: 0.8692 - val_loss: 0.4148 - val_acc: 0.8832\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3072 - acc: 0.8702 - val_loss: 0.3343 - val_acc: 0.9090\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 143s 9ms/step - loss: 0.3011 - acc: 0.8718 - val_loss: 0.3663 - val_acc: 0.9045\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.2977 - acc: 0.8726 - val_loss: 0.3200 - val_acc: 0.8985\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.2973 - acc: 0.8748 - val_loss: 0.3483 - val_acc: 0.8975\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.2950 - acc: 0.8751 - val_loss: 0.3578 - val_acc: 0.9085\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 141s 9ms/step - loss: 0.2881 - acc: 0.8770 - val_loss: 0.3195 - val_acc: 0.9068\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.2857 - acc: 0.8765 - val_loss: 0.3188 - val_acc: 0.8998\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.2842 - acc: 0.8780 - val_loss: 0.3343 - val_acc: 0.9077\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 141s 9ms/step - loss: 0.2836 - acc: 0.8804 - val_loss: 0.3503 - val_acc: 0.8958\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 142s 9ms/step - loss: 0.2789 - acc: 0.8799 - val_loss: 0.3281 - val_acc: 0.9055\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 141s 9ms/step - loss: 0.2774 - acc: 0.8815 - val_loss: 0.3096 - val_acc: 0.9060\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1dd1b3b690>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "    cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "    pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "    drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "    cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "    pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "    drop2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "    cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                  activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "    pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "    drop3 = Dropout(0.25)(pool3)\n",
    "    \n",
    "    drop3 = LSTM(64,return_sequences=True)(drop3)\n",
    "    \n",
    "    flatten1 = Flatten()(drop3)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 64\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "# densenet_depth = 46\n",
    "# densenet_growth_rate = 8\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_26/concat:0\", shape=(?, 338, 30), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 594s 38ms/step - loss: 0.6003 - acc: 0.6506 - val_loss: 0.5385 - val_acc: 0.7800\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 522s 33ms/step - loss: 0.4698 - acc: 0.7766 - val_loss: 0.5017 - val_acc: 0.8340\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 496s 32ms/step - loss: 0.4319 - acc: 0.8027 - val_loss: 0.4476 - val_acc: 0.8568\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 471s 30ms/step - loss: 0.4037 - acc: 0.8186 - val_loss: 0.3745 - val_acc: 0.8808\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 469s 30ms/step - loss: 0.3739 - acc: 0.8379 - val_loss: 0.4018 - val_acc: 0.8873\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 471s 30ms/step - loss: 0.3607 - acc: 0.8463 - val_loss: 0.3288 - val_acc: 0.9005\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 469s 30ms/step - loss: 0.3475 - acc: 0.8504 - val_loss: 0.3947 - val_acc: 0.9018\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 472s 30ms/step - loss: 0.3393 - acc: 0.8564 - val_loss: 0.2854 - val_acc: 0.9038\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 474s 30ms/step - loss: 0.3357 - acc: 0.8585 - val_loss: 0.3158 - val_acc: 0.9010\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 472s 30ms/step - loss: 0.3290 - acc: 0.8577 - val_loss: 0.4037 - val_acc: 0.9065\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 460s 29ms/step - loss: 0.3232 - acc: 0.8628 - val_loss: 0.3465 - val_acc: 0.9002\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 463s 30ms/step - loss: 0.3180 - acc: 0.8649 - val_loss: 0.3116 - val_acc: 0.9035\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 471s 30ms/step - loss: 0.3143 - acc: 0.8642 - val_loss: 0.3121 - val_acc: 0.9090\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 470s 30ms/step - loss: 0.3121 - acc: 0.8672 - val_loss: 0.3328 - val_acc: 0.9052\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 501s 32ms/step - loss: 0.3089 - acc: 0.8709 - val_loss: 0.3168 - val_acc: 0.9045\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 532s 34ms/step - loss: 0.3065 - acc: 0.8707 - val_loss: 0.2783 - val_acc: 0.9107\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 525s 34ms/step - loss: 0.3016 - acc: 0.8719 - val_loss: 0.3612 - val_acc: 0.9060\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 519s 33ms/step - loss: 0.2984 - acc: 0.8698 - val_loss: 0.4008 - val_acc: 0.8975\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 521s 33ms/step - loss: 0.2944 - acc: 0.8755 - val_loss: 0.3145 - val_acc: 0.9115\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 515s 33ms/step - loss: 0.2923 - acc: 0.8764 - val_loss: 0.3463 - val_acc: 0.9102\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 518s 33ms/step - loss: 0.2916 - acc: 0.8747 - val_loss: 0.3744 - val_acc: 0.9058\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 525s 34ms/step - loss: 0.2937 - acc: 0.8762 - val_loss: 0.3542 - val_acc: 0.9110\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 521s 33ms/step - loss: 0.2868 - acc: 0.8791 - val_loss: 0.3675 - val_acc: 0.9040\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 519s 33ms/step - loss: 0.2821 - acc: 0.8796 - val_loss: 0.3266 - val_acc: 0.9105\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 517s 33ms/step - loss: 0.2851 - acc: 0.8796 - val_loss: 0.3761 - val_acc: 0.9065\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 520s 33ms/step - loss: 0.2773 - acc: 0.8810 - val_loss: 0.4127 - val_acc: 0.8830\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 481s 31ms/step - loss: 0.2744 - acc: 0.8836 - val_loss: 0.3418 - val_acc: 0.8990\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 465s 30ms/step - loss: 0.2748 - acc: 0.8829 - val_loss: 0.4473 - val_acc: 0.8968\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 470s 30ms/step - loss: 0.2733 - acc: 0.8842 - val_loss: 0.3133 - val_acc: 0.9087\n",
      "Epoch 00029: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1dcf545890>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=1)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_30/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Tensor(\"attention_layer_1/div:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Tensor(\"concatenate_30/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Tensor(\"attention_layer_1/mul:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 211s 13ms/step - loss: 0.6704 - acc: 0.5741 - val_loss: 0.6437 - val_acc: 0.6805\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.5703 - acc: 0.6971 - val_loss: 0.5325 - val_acc: 0.8193\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.4959 - acc: 0.7629 - val_loss: 0.5549 - val_acc: 0.8345\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.4670 - acc: 0.7795 - val_loss: 0.4517 - val_acc: 0.8660\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.4352 - acc: 0.8033 - val_loss: 0.4350 - val_acc: 0.8782\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.4116 - acc: 0.8175 - val_loss: 0.4567 - val_acc: 0.8720\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3989 - acc: 0.8272 - val_loss: 0.3847 - val_acc: 0.8885\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3897 - acc: 0.8304 - val_loss: 0.3884 - val_acc: 0.8882\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3851 - acc: 0.8325 - val_loss: 0.4103 - val_acc: 0.8953\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3797 - acc: 0.8364 - val_loss: 0.3727 - val_acc: 0.8925\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3781 - acc: 0.8367 - val_loss: 0.3760 - val_acc: 0.8940\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3719 - acc: 0.8384 - val_loss: 0.4007 - val_acc: 0.8938\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3681 - acc: 0.8420 - val_loss: 0.3539 - val_acc: 0.8982\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3650 - acc: 0.8450 - val_loss: 0.3655 - val_acc: 0.8957\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3624 - acc: 0.8419 - val_loss: 0.3372 - val_acc: 0.8895\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3592 - acc: 0.8457 - val_loss: 0.3157 - val_acc: 0.8982\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3564 - acc: 0.8477 - val_loss: 0.3204 - val_acc: 0.9008\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3557 - acc: 0.8471 - val_loss: 0.3207 - val_acc: 0.8882\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 140s 9ms/step - loss: 0.3511 - acc: 0.8478 - val_loss: 0.3243 - val_acc: 0.8992\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3468 - acc: 0.8522 - val_loss: 0.3314 - val_acc: 0.8922\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3479 - acc: 0.8486 - val_loss: 0.3249 - val_acc: 0.8972\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3465 - acc: 0.8512 - val_loss: 0.3118 - val_acc: 0.8928\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3464 - acc: 0.8502 - val_loss: 0.3603 - val_acc: 0.8960\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3450 - acc: 0.8512 - val_loss: 0.3319 - val_acc: 0.8918\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3421 - acc: 0.8518 - val_loss: 0.3138 - val_acc: 0.8932\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 138s 9ms/step - loss: 0.3424 - acc: 0.8513 - val_loss: 0.3056 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 139s 9ms/step - loss: 0.3403 - acc: 0.8546 - val_loss: 0.2966 - val_acc: 0.8972\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1dcbc71a50>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "#     lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "#     print lstm_out1\n",
    "    \n",
    "    flatten1 = Attention_layer()(concat)\n",
    "\n",
    "#     flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_32/concat:0\", shape=(?, 169, 90), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 481s 31ms/step - loss: 0.6135 - acc: 0.6412 - val_loss: 0.4682 - val_acc: 0.8210\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 403s 26ms/step - loss: 0.4820 - acc: 0.7650 - val_loss: 0.4769 - val_acc: 0.8640\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 405s 26ms/step - loss: 0.4437 - acc: 0.7942 - val_loss: 0.4975 - val_acc: 0.8770\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 403s 26ms/step - loss: 0.4200 - acc: 0.8092 - val_loss: 0.4809 - val_acc: 0.8810\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 405s 26ms/step - loss: 0.3942 - acc: 0.8249 - val_loss: 0.4326 - val_acc: 0.8975\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 410s 26ms/step - loss: 0.3701 - acc: 0.8398 - val_loss: 0.3873 - val_acc: 0.8998\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 408s 26ms/step - loss: 0.3540 - acc: 0.8494 - val_loss: 0.3837 - val_acc: 0.8930\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 400s 26ms/step - loss: 0.3473 - acc: 0.8507 - val_loss: 0.2951 - val_acc: 0.9092\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 405s 26ms/step - loss: 0.3396 - acc: 0.8550 - val_loss: 0.3526 - val_acc: 0.9035\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 406s 26ms/step - loss: 0.3364 - acc: 0.8566 - val_loss: 0.3252 - val_acc: 0.9060\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 409s 26ms/step - loss: 0.3325 - acc: 0.8592 - val_loss: 0.3705 - val_acc: 0.8805\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 408s 26ms/step - loss: 0.3253 - acc: 0.8621 - val_loss: 0.3413 - val_acc: 0.9073\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 406s 26ms/step - loss: 0.3211 - acc: 0.8612 - val_loss: 0.3239 - val_acc: 0.9115\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 406s 26ms/step - loss: 0.3183 - acc: 0.8656 - val_loss: 0.3143 - val_acc: 0.9077\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 409s 26ms/step - loss: 0.3146 - acc: 0.8673 - val_loss: 0.3105 - val_acc: 0.9125\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 383s 24ms/step - loss: 0.3110 - acc: 0.8663 - val_loss: 0.3366 - val_acc: 0.8947\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 376s 24ms/step - loss: 0.3091 - acc: 0.8691 - val_loss: 0.2934 - val_acc: 0.9058\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 378s 24ms/step - loss: 0.3076 - acc: 0.8691 - val_loss: 0.3079 - val_acc: 0.9055\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 374s 24ms/step - loss: 0.3027 - acc: 0.8688 - val_loss: 0.3114 - val_acc: 0.9052\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 372s 24ms/step - loss: 0.2997 - acc: 0.8709 - val_loss: 0.2973 - val_acc: 0.9033\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 374s 24ms/step - loss: 0.2973 - acc: 0.8715 - val_loss: 0.2558 - val_acc: 0.9127\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 373s 24ms/step - loss: 0.2935 - acc: 0.8744 - val_loss: 0.2772 - val_acc: 0.9070\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 373s 24ms/step - loss: 0.2889 - acc: 0.8751 - val_loss: 0.2531 - val_acc: 0.9147\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 372s 24ms/step - loss: 0.2855 - acc: 0.8780 - val_loss: 0.3173 - val_acc: 0.8932\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 390s 25ms/step - loss: 0.2832 - acc: 0.8796 - val_loss: 0.3076 - val_acc: 0.9123\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 391s 25ms/step - loss: 0.2807 - acc: 0.8792 - val_loss: 0.3768 - val_acc: 0.8912\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 389s 25ms/step - loss: 0.2775 - acc: 0.8824 - val_loss: 0.2876 - val_acc: 0.9115\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 388s 25ms/step - loss: 0.2760 - acc: 0.8825 - val_loss: 0.3044 - val_acc: 0.8870\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 390s 25ms/step - loss: 0.2704 - acc: 0.8849 - val_loss: 0.3133 - val_acc: 0.8858\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 390s 25ms/step - loss: 0.2668 - acc: 0.8863 - val_loss: 0.3440 - val_acc: 0.8825\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 391s 25ms/step - loss: 0.2668 - acc: 0.8864 - val_loss: 0.3147 - val_acc: 0.8860\n",
      "Epoch 32/50\n",
      "15658/15658 [==============================] - 391s 25ms/step - loss: 0.2620 - acc: 0.8879 - val_loss: 0.3106 - val_acc: 0.9038\n",
      "Epoch 33/50\n",
      "15658/15658 [==============================] - 391s 25ms/step - loss: 0.2588 - acc: 0.8890 - val_loss: 0.3470 - val_acc: 0.8902\n",
      "Epoch 00033: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1dc42f9910>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(3):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(128,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_34/concat:0\", shape=(?, 55, 90), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 372s 24ms/step - loss: 0.6602 - acc: 0.5865 - val_loss: 0.5372 - val_acc: 0.7480\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 281s 18ms/step - loss: 0.5079 - acc: 0.7470 - val_loss: 0.4277 - val_acc: 0.8320\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.3928 - acc: 0.8258 - val_loss: 0.3362 - val_acc: 0.8890\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.3633 - acc: 0.8439 - val_loss: 0.3434 - val_acc: 0.8960\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.3501 - acc: 0.8511 - val_loss: 0.2985 - val_acc: 0.8928\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3457 - acc: 0.8543 - val_loss: 0.3438 - val_acc: 0.8995\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3392 - acc: 0.8575 - val_loss: 0.2844 - val_acc: 0.8955\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3400 - acc: 0.8550 - val_loss: 0.3384 - val_acc: 0.8982\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3332 - acc: 0.8601 - val_loss: 0.3517 - val_acc: 0.8975\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 271s 17ms/step - loss: 0.3327 - acc: 0.8610 - val_loss: 0.2616 - val_acc: 0.9067\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 272s 17ms/step - loss: 0.3278 - acc: 0.8608 - val_loss: 0.2821 - val_acc: 0.9105\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3264 - acc: 0.8626 - val_loss: 0.3036 - val_acc: 0.9015\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3235 - acc: 0.8654 - val_loss: 0.2810 - val_acc: 0.8992\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3217 - acc: 0.8654 - val_loss: 0.3048 - val_acc: 0.9012\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 281s 18ms/step - loss: 0.3197 - acc: 0.8662 - val_loss: 0.2856 - val_acc: 0.9090\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.3178 - acc: 0.8674 - val_loss: 0.2852 - val_acc: 0.9107\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.3175 - acc: 0.8669 - val_loss: 0.2814 - val_acc: 0.9090\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3142 - acc: 0.8697 - val_loss: 0.2716 - val_acc: 0.9083\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3137 - acc: 0.8677 - val_loss: 0.2555 - val_acc: 0.9065\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.3116 - acc: 0.8687 - val_loss: 0.2729 - val_acc: 0.9102\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.3100 - acc: 0.8703 - val_loss: 0.2676 - val_acc: 0.9113\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 278s 18ms/step - loss: 0.3097 - acc: 0.8699 - val_loss: 0.2619 - val_acc: 0.9133\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3097 - acc: 0.8718 - val_loss: 0.2946 - val_acc: 0.9008\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3088 - acc: 0.8694 - val_loss: 0.2731 - val_acc: 0.9075\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3067 - acc: 0.8718 - val_loss: 0.2551 - val_acc: 0.9125\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3053 - acc: 0.8720 - val_loss: 0.2992 - val_acc: 0.9073\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 277s 18ms/step - loss: 0.3051 - acc: 0.8709 - val_loss: 0.2529 - val_acc: 0.9070\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3039 - acc: 0.8723 - val_loss: 0.2517 - val_acc: 0.9123\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3013 - acc: 0.8720 - val_loss: 0.2353 - val_acc: 0.9145\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 278s 18ms/step - loss: 0.3019 - acc: 0.8739 - val_loss: 0.2807 - val_acc: 0.9035\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3022 - acc: 0.8727 - val_loss: 0.3000 - val_acc: 0.9092\n",
      "Epoch 32/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3003 - acc: 0.8747 - val_loss: 0.2828 - val_acc: 0.9145\n",
      "Epoch 33/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.2980 - acc: 0.8751 - val_loss: 0.3065 - val_acc: 0.9065\n",
      "Epoch 34/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.2962 - acc: 0.8757 - val_loss: 0.3525 - val_acc: 0.8832\n",
      "Epoch 35/50\n",
      "15658/15658 [==============================] - 278s 18ms/step - loss: 0.2954 - acc: 0.8741 - val_loss: 0.3048 - val_acc: 0.9105\n",
      "Epoch 36/50\n",
      "15658/15658 [==============================] - 276s 18ms/step - loss: 0.2949 - acc: 0.8764 - val_loss: 0.2521 - val_acc: 0.9160\n",
      "Epoch 37/50\n",
      "15658/15658 [==============================] - 277s 18ms/step - loss: 0.2932 - acc: 0.8762 - val_loss: 0.3108 - val_acc: 0.9030\n",
      "Epoch 38/50\n",
      "15658/15658 [==============================] - 281s 18ms/step - loss: 0.2925 - acc: 0.8759 - val_loss: 0.2690 - val_acc: 0.9083\n",
      "Epoch 39/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.2927 - acc: 0.8773 - val_loss: 0.2868 - val_acc: 0.9035\n",
      "Epoch 40/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.2911 - acc: 0.8780 - val_loss: 0.2575 - val_acc: 0.9130\n",
      "Epoch 41/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.2872 - acc: 0.8792 - val_loss: 0.2363 - val_acc: 0.9207\n",
      "Epoch 42/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.2899 - acc: 0.8795 - val_loss: 0.2255 - val_acc: 0.9140\n",
      "Epoch 43/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.2875 - acc: 0.8790 - val_loss: 0.2644 - val_acc: 0.9102\n",
      "Epoch 44/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.2844 - acc: 0.8797 - val_loss: 0.2811 - val_acc: 0.9123\n",
      "Epoch 45/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.2849 - acc: 0.8802 - val_loss: 0.2614 - val_acc: 0.9133\n",
      "Epoch 46/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.2860 - acc: 0.8776 - val_loss: 0.2642 - val_acc: 0.9100\n",
      "Epoch 47/50\n",
      "15658/15658 [==============================] - 273s 17ms/step - loss: 0.2830 - acc: 0.8817 - val_loss: 0.2552 - val_acc: 0.9117\n",
      "Epoch 48/50\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.2792 - acc: 0.8809 - val_loss: 0.3183 - val_acc: 0.9052\n",
      "Epoch 49/50\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.2791 - acc: 0.8815 - val_loss: 0.2604 - val_acc: 0.9105\n",
      "Epoch 50/50\n",
      "15658/15658 [==============================] - 266s 17ms/step - loss: 0.2783 - acc: 0.8818 - val_loss: 0.2404 - val_acc: 0.9155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1db949c1d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(3):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "        cnn7 = Convolution1D(30, (3), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop3)\n",
    "        cnn8 = Convolution1D(30, (3), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn7)\n",
    "        pool4 = MaxPooling1D(pool_size=(3))(cnn8)\n",
    "        drop4 = Dropout(0.25)(pool4)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop4)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(128,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_38/concat:0\", shape=(?, 55, 90), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/200\n",
      "15658/15658 [==============================] - 352s 23ms/step - loss: 0.6843 - acc: 0.5297 - val_loss: 0.6778 - val_acc: 0.6278\n",
      "Epoch 2/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.6600 - acc: 0.5924 - val_loss: 0.6352 - val_acc: 0.6478\n",
      "Epoch 3/200\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.6354 - acc: 0.6246 - val_loss: 0.6029 - val_acc: 0.6720\n",
      "Epoch 4/200\n",
      "15658/15658 [==============================] - 268s 17ms/step - loss: 0.5999 - acc: 0.6639 - val_loss: 0.5167 - val_acc: 0.7732\n",
      "Epoch 5/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.5510 - acc: 0.7099 - val_loss: 0.4929 - val_acc: 0.7632\n",
      "Epoch 6/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.5042 - acc: 0.7475 - val_loss: 0.4498 - val_acc: 0.8242\n",
      "Epoch 7/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.4719 - acc: 0.7724 - val_loss: 0.4190 - val_acc: 0.8448\n",
      "Epoch 8/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.4403 - acc: 0.7977 - val_loss: 0.3943 - val_acc: 0.8347\n",
      "Epoch 9/200\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.4183 - acc: 0.8113 - val_loss: 0.3764 - val_acc: 0.8772\n",
      "Epoch 10/200\n",
      "15658/15658 [==============================] - 268s 17ms/step - loss: 0.4018 - acc: 0.8226 - val_loss: 0.3928 - val_acc: 0.8882\n",
      "Epoch 11/200\n",
      "15658/15658 [==============================] - 267s 17ms/step - loss: 0.3927 - acc: 0.8297 - val_loss: 0.3776 - val_acc: 0.8825\n",
      "Epoch 12/200\n",
      "15658/15658 [==============================] - 268s 17ms/step - loss: 0.3796 - acc: 0.8357 - val_loss: 0.3653 - val_acc: 0.8705\n",
      "Epoch 13/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3707 - acc: 0.8425 - val_loss: 0.3431 - val_acc: 0.8762\n",
      "Epoch 14/200\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3643 - acc: 0.8416 - val_loss: 0.3392 - val_acc: 0.8668\n",
      "Epoch 15/200\n",
      "15658/15658 [==============================] - 268s 17ms/step - loss: 0.3592 - acc: 0.8486 - val_loss: 0.3858 - val_acc: 0.9018\n",
      "Epoch 16/200\n",
      "15658/15658 [==============================] - 268s 17ms/step - loss: 0.3545 - acc: 0.8483 - val_loss: 0.3411 - val_acc: 0.8888\n",
      "Epoch 17/200\n",
      "15658/15658 [==============================] - 268s 17ms/step - loss: 0.3502 - acc: 0.8520 - val_loss: 0.3264 - val_acc: 0.8910\n",
      "Epoch 18/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3470 - acc: 0.8513 - val_loss: 0.3279 - val_acc: 0.8975\n",
      "Epoch 19/200\n",
      "15658/15658 [==============================] - 267s 17ms/step - loss: 0.3437 - acc: 0.8545 - val_loss: 0.3160 - val_acc: 0.8915\n",
      "Epoch 20/200\n",
      "15658/15658 [==============================] - 268s 17ms/step - loss: 0.3410 - acc: 0.8568 - val_loss: 0.3116 - val_acc: 0.8960\n",
      "Epoch 21/200\n",
      "15658/15658 [==============================] - 268s 17ms/step - loss: 0.3398 - acc: 0.8578 - val_loss: 0.3058 - val_acc: 0.8995\n",
      "Epoch 22/200\n",
      "15658/15658 [==============================] - 268s 17ms/step - loss: 0.3359 - acc: 0.8576 - val_loss: 0.3057 - val_acc: 0.9015\n",
      "Epoch 23/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3355 - acc: 0.8579 - val_loss: 0.2912 - val_acc: 0.9087\n",
      "Epoch 24/200\n",
      "15658/15658 [==============================] - 268s 17ms/step - loss: 0.3337 - acc: 0.8580 - val_loss: 0.2785 - val_acc: 0.9025\n",
      "Epoch 25/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3320 - acc: 0.8598 - val_loss: 0.2918 - val_acc: 0.9092\n",
      "Epoch 26/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3324 - acc: 0.8617 - val_loss: 0.3013 - val_acc: 0.9065\n",
      "Epoch 27/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3282 - acc: 0.8631 - val_loss: 0.3036 - val_acc: 0.9042\n",
      "Epoch 28/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3299 - acc: 0.8614 - val_loss: 0.3021 - val_acc: 0.9060\n",
      "Epoch 29/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3260 - acc: 0.8633 - val_loss: 0.2770 - val_acc: 0.9105\n",
      "Epoch 30/200\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3256 - acc: 0.8655 - val_loss: 0.2740 - val_acc: 0.9033\n",
      "Epoch 31/200\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3256 - acc: 0.8629 - val_loss: 0.2637 - val_acc: 0.9010\n",
      "Epoch 32/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3226 - acc: 0.8646 - val_loss: 0.2719 - val_acc: 0.9038\n",
      "Epoch 33/200\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3232 - acc: 0.8663 - val_loss: 0.2771 - val_acc: 0.9033\n",
      "Epoch 34/200\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3232 - acc: 0.8657 - val_loss: 0.2575 - val_acc: 0.9040\n",
      "Epoch 35/200\n",
      "15658/15658 [==============================] - 268s 17ms/step - loss: 0.3207 - acc: 0.8644 - val_loss: 0.2718 - val_acc: 0.9038\n",
      "Epoch 36/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3215 - acc: 0.8640 - val_loss: 0.2830 - val_acc: 0.9065\n",
      "Epoch 37/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3209 - acc: 0.8665 - val_loss: 0.2739 - val_acc: 0.9045\n",
      "Epoch 38/200\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3186 - acc: 0.8652 - val_loss: 0.2710 - val_acc: 0.9073\n",
      "Epoch 39/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3173 - acc: 0.8681 - val_loss: 0.2805 - val_acc: 0.9083\n",
      "Epoch 40/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3168 - acc: 0.8684 - val_loss: 0.2725 - val_acc: 0.9083\n",
      "Epoch 41/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3168 - acc: 0.8689 - val_loss: 0.2626 - val_acc: 0.9035\n",
      "Epoch 42/200\n",
      "15658/15658 [==============================] - 268s 17ms/step - loss: 0.3149 - acc: 0.8680 - val_loss: 0.3129 - val_acc: 0.9020\n",
      "Epoch 43/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3140 - acc: 0.8697 - val_loss: 0.2626 - val_acc: 0.9055\n",
      "Epoch 44/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3133 - acc: 0.8690 - val_loss: 0.2594 - val_acc: 0.9067\n",
      "Epoch 45/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3149 - acc: 0.8670 - val_loss: 0.2831 - val_acc: 0.9100\n",
      "Epoch 46/200\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3135 - acc: 0.8679 - val_loss: 0.2520 - val_acc: 0.9065\n",
      "Epoch 47/200\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3124 - acc: 0.8707 - val_loss: 0.2843 - val_acc: 0.9095\n",
      "Epoch 48/200\n",
      "15658/15658 [==============================] - 269s 17ms/step - loss: 0.3128 - acc: 0.8693 - val_loss: 0.3028 - val_acc: 0.9092\n",
      "Epoch 49/200\n",
      "15658/15658 [==============================] - 270s 17ms/step - loss: 0.3095 - acc: 0.8721 - val_loss: 0.2503 - val_acc: 0.9025\n",
      "Epoch 00049: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d9fff9290>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(3):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "        cnn7 = Convolution1D(30, (3), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop3)\n",
    "        cnn8 = Convolution1D(30, (3), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn7)\n",
    "        pool4 = MaxPooling1D(pool_size=(3))(cnn8)\n",
    "        drop4 = Dropout(0.25)(pool4)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop4)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(128,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=20, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=200,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
