{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359796, 1700)\n",
      "(29983, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load('../ECGdata/normal.npy')\n",
    "temp = np.load('../ECGdata/abnormal.npy')\n",
    "X_train = np.vstack((X_train,temp))\n",
    "\n",
    "Y_train = np.load('../ECGdata/label_1.npy')\n",
    "temp = np.load('../ECGdata/label_0.npy')\n",
    "Y_train = np.vstack((Y_train,temp))\n",
    "\n",
    "print X_train.shape\n",
    "print Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29983, 20400)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1,12*1700)\n",
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23986, 20400)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size=0.2, random_state=2018)\n",
    "print x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_layer(Layer):\n",
    "    \"\"\"\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWithContext())\n",
    "        \"\"\"\n",
    " \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    " \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    " \n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    " \n",
    "        uit = K.tanh(uit)\n",
    " \n",
    "        a = K.exp(uit)\n",
    " \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    " \n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        # a = K.expand_dims(a)\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (-1, input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet-40-12 created.\n",
      "Train on 23986 samples, validate on 5997 samples\n",
      "Epoch 1/50\n",
      "23986/23986 [==============================] - 40s 2ms/step - loss: 0.8947 - acc: 0.5953 - val_loss: 0.8622 - val_acc: 0.6560\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.65599, saving model to DenseNet.h5\n",
      "Epoch 2/50\n",
      "23986/23986 [==============================] - 35s 1ms/step - loss: 0.8224 - acc: 0.6881 - val_loss: 0.8463 - val_acc: 0.6695\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.65599 to 0.66950, saving model to DenseNet.h5\n",
      "Epoch 3/50\n",
      "23986/23986 [==============================] - 35s 1ms/step - loss: 0.7762 - acc: 0.7262 - val_loss: 0.8230 - val_acc: 0.6887\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.66950 to 0.68868, saving model to DenseNet.h5\n",
      "Epoch 4/50\n",
      "23986/23986 [==============================] - 35s 1ms/step - loss: 0.7365 - acc: 0.7545 - val_loss: 0.8602 - val_acc: 0.6870\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.68868\n",
      "Epoch 5/50\n",
      "23986/23986 [==============================] - 34s 1ms/step - loss: 0.6983 - acc: 0.7778 - val_loss: 0.8279 - val_acc: 0.7014\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.68868 to 0.70135, saving model to DenseNet.h5\n",
      "Epoch 6/50\n",
      "23986/23986 [==============================] - 34s 1ms/step - loss: 0.6632 - acc: 0.7984 - val_loss: 0.8401 - val_acc: 0.6912\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.70135\n",
      "Epoch 7/50\n",
      "23986/23986 [==============================] - 34s 1ms/step - loss: 0.6319 - acc: 0.8170 - val_loss: 0.8734 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.70135\n",
      "Epoch 8/50\n",
      "23986/23986 [==============================] - 35s 1ms/step - loss: 0.5997 - acc: 0.8358 - val_loss: 0.8792 - val_acc: 0.6938\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.70135\n",
      "Epoch 9/50\n",
      "23986/23986 [==============================] - 36s 1ms/step - loss: 0.5720 - acc: 0.8457 - val_loss: 0.9199 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.70135\n",
      "Epoch 10/50\n",
      "23986/23986 [==============================] - 34s 1ms/step - loss: 0.5421 - acc: 0.8653 - val_loss: 0.9851 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.70135\n",
      "Epoch 11/50\n",
      "23986/23986 [==============================] - 32s 1ms/step - loss: 0.5208 - acc: 0.8732 - val_loss: 0.9927 - val_acc: 0.6815\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.70135\n",
      "Epoch 12/50\n",
      "23986/23986 [==============================] - 31s 1ms/step - loss: 0.4927 - acc: 0.8886 - val_loss: 1.0518 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.70135\n",
      "Epoch 13/50\n",
      "23986/23986 [==============================] - 33s 1ms/step - loss: 0.4685 - acc: 0.8989 - val_loss: 1.1131 - val_acc: 0.6742\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.70135\n",
      "Epoch 14/50\n",
      "23986/23986 [==============================] - 34s 1ms/step - loss: 0.4555 - acc: 0.9054 - val_loss: 1.0791 - val_acc: 0.6825\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.70135\n",
      "Epoch 15/50\n",
      "23986/23986 [==============================] - 35s 1ms/step - loss: 0.4324 - acc: 0.9175 - val_loss: 1.1952 - val_acc: 0.6740\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.70135\n",
      "Epoch 16/50\n",
      "23986/23986 [==============================] - 35s 1ms/step - loss: 0.4230 - acc: 0.9193 - val_loss: 1.2194 - val_acc: 0.6743\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.70135\n",
      "Epoch 17/50\n",
      "23986/23986 [==============================] - 35s 1ms/step - loss: 0.4023 - acc: 0.9274 - val_loss: 1.2094 - val_acc: 0.6760\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.70135\n",
      "Epoch 18/50\n",
      "23986/23986 [==============================] - 33s 1ms/step - loss: 0.3954 - acc: 0.9325 - val_loss: 1.2736 - val_acc: 0.6803\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.70135\n",
      "Epoch 19/50\n",
      "23986/23986 [==============================] - 33s 1ms/step - loss: 0.3820 - acc: 0.9382 - val_loss: 1.3248 - val_acc: 0.6773\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.70135\n",
      "Epoch 20/50\n",
      "23986/23986 [==============================] - 32s 1ms/step - loss: 0.3688 - acc: 0.9452 - val_loss: 1.3412 - val_acc: 0.6737\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.70135\n",
      "Epoch 21/50\n",
      "23986/23986 [==============================] - 35s 1ms/step - loss: 0.3569 - acc: 0.9475 - val_loss: 1.5012 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.70135\n",
      "Epoch 22/50\n",
      "23986/23986 [==============================] - 32s 1ms/step - loss: 0.3503 - acc: 0.9511 - val_loss: 1.4388 - val_acc: 0.6740\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.70135\n",
      "Epoch 23/50\n",
      "23986/23986 [==============================] - 33s 1ms/step - loss: 0.3412 - acc: 0.9544 - val_loss: 1.6516 - val_acc: 0.6668\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.70135\n",
      "Epoch 24/50\n",
      "23986/23986 [==============================] - 32s 1ms/step - loss: 0.3360 - acc: 0.9568 - val_loss: 1.5599 - val_acc: 0.6642\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.70135\n",
      "Epoch 25/50\n",
      "23986/23986 [==============================] - 31s 1ms/step - loss: 0.3269 - acc: 0.9611 - val_loss: 1.5930 - val_acc: 0.6727\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.70135\n",
      "Epoch 26/50\n",
      "23986/23986 [==============================] - 31s 1ms/step - loss: 0.3229 - acc: 0.9623 - val_loss: 1.5989 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.70135\n",
      "Epoch 27/50\n",
      "15200/23986 [==================>...........] - ETA: 10s - loss: 0.3104 - acc: 0.9663"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f829f59ef53c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m model.fit(xtrain, y_train, batch_size=32, epochs=50,\n\u001b[1;32m    106\u001b[0m          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m          callbacks = [checkpoint])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def conv_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    x = Activation('relu')(input)\n",
    "    x = Convolution1D(nb_filter, (9), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    feature_list = [x]\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        x = conv_block(x, growth_rate, dropout_rate, weight_decay)\n",
    "        feature_list.append(x)\n",
    "        x = Concatenate(axis=concat_axis)(feature_list)\n",
    "        nb_filter += growth_rate\n",
    "\n",
    "    return x, nb_filter\n",
    "\n",
    "def transition_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    x = Convolution1D(nb_filter, (1), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(input)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    x = AveragePooling1D((2), strides=(1))(x)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                           beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=16, dropout_rate=None,\n",
    "                     weight_decay=1E-4, verbose=True):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    assert (depth - 4) % 3 == 0, \"Depth must be 3 N + 4\"\n",
    "\n",
    "    # layers in each dense block\n",
    "    nb_layers = int((depth - 4) / 3)\n",
    "\n",
    "    # Initial convolution\n",
    "    x = Convolution1D(nb_filter, (9), kernel_initializer=\"he_uniform\", padding=\"same\", name=\"initial_conv2D\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(model_input)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                            beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                                   weight_decay=weight_decay)\n",
    "        # add transition_block\n",
    "        x = transition_block(x, nb_filter, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # The last dense_block does not have a transition_block\n",
    "    x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                               weight_decay=weight_decay)\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "#     print x.shape\n",
    "#     x = AveragePooling2D((1, 25), strides=(1, 25))(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "#     print x.shape\n",
    "#     x = Attention_layer()(x)\n",
    "    x = Dense(nb_classes, activation='softmax', kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    if verbose: \n",
    "        print(\"DenseNet-%d-%d created.\" % (depth, growth_rate))\n",
    "\n",
    "    return densenet\n",
    "\n",
    "\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 1700\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS)\n",
    "densenet_depth = 40\n",
    "densenet_growth_rate = 12\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim,depth=densenet_depth,\n",
    "                  growth_rate = densenet_growth_rate)\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('DenseNet.h5', monitor='val_acc', verbose=1, save_best_only=True)\n",
    "\n",
    "xtrain = x_train.reshape(-1,12,1700)\n",
    "xvalid = x_valid.reshape(-1,12,1700)\n",
    "model.fit(xtrain, y_train, batch_size=32, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet-40-12 created.\n",
      "Train on 23986 samples, validate on 5997 samples\n",
      "Epoch 1/50\n",
      "23986/23986 [==============================] - 691s 29ms/step - loss: 0.8168 - acc: 0.6798 - val_loss: 0.8734 - val_acc: 0.6223\n",
      "Epoch 2/50\n",
      "23986/23986 [==============================] - 687s 29ms/step - loss: 0.7484 - acc: 0.7192 - val_loss: 1.0302 - val_acc: 0.5793\n",
      "Epoch 3/50\n",
      "23986/23986 [==============================] - 686s 29ms/step - loss: 0.7131 - acc: 0.7332 - val_loss: 0.7198 - val_acc: 0.7310\n",
      "Epoch 4/50\n",
      "23986/23986 [==============================] - 683s 28ms/step - loss: 0.6843 - acc: 0.7398 - val_loss: 0.6623 - val_acc: 0.7620\n",
      "Epoch 5/50\n",
      "23986/23986 [==============================] - 685s 29ms/step - loss: 0.6616 - acc: 0.7468 - val_loss: 0.6137 - val_acc: 0.7826\n",
      "Epoch 6/50\n",
      "23986/23986 [==============================] - 685s 29ms/step - loss: 0.6355 - acc: 0.7578 - val_loss: 0.6015 - val_acc: 0.7784\n",
      "Epoch 7/50\n",
      "23986/23986 [==============================] - 684s 29ms/step - loss: 0.6135 - acc: 0.7646 - val_loss: 0.6599 - val_acc: 0.7355\n",
      "Epoch 8/50\n",
      "23986/23986 [==============================] - 684s 29ms/step - loss: 0.5995 - acc: 0.7686 - val_loss: 0.5870 - val_acc: 0.7736\n",
      "Epoch 9/50\n",
      "23986/23986 [==============================] - 684s 29ms/step - loss: 0.5827 - acc: 0.7726 - val_loss: 0.6206 - val_acc: 0.7612\n",
      "Epoch 10/50\n",
      "23986/23986 [==============================] - 685s 29ms/step - loss: 0.5721 - acc: 0.7755 - val_loss: 0.7228 - val_acc: 0.7005\n",
      "Epoch 11/50\n",
      "23986/23986 [==============================] - 687s 29ms/step - loss: 0.5590 - acc: 0.7838 - val_loss: 0.5864 - val_acc: 0.7687\n",
      "Epoch 12/50\n",
      "23986/23986 [==============================] - 687s 29ms/step - loss: 0.5498 - acc: 0.7835 - val_loss: 0.6332 - val_acc: 0.7340\n",
      "Epoch 13/50\n",
      "23986/23986 [==============================] - 686s 29ms/step - loss: 0.5414 - acc: 0.7819 - val_loss: 0.5287 - val_acc: 0.7894\n",
      "Epoch 14/50\n",
      "23986/23986 [==============================] - 685s 29ms/step - loss: 0.5344 - acc: 0.7848 - val_loss: 0.5277 - val_acc: 0.7914\n",
      "Epoch 15/50\n",
      "23986/23986 [==============================] - 685s 29ms/step - loss: 0.5249 - acc: 0.7883 - val_loss: 0.4872 - val_acc: 0.8129\n",
      "Epoch 16/50\n",
      "23986/23986 [==============================] - 686s 29ms/step - loss: 0.5149 - acc: 0.7908 - val_loss: 0.5140 - val_acc: 0.7947\n",
      "Epoch 17/50\n",
      "23986/23986 [==============================] - 688s 29ms/step - loss: 0.5052 - acc: 0.7959 - val_loss: 0.5230 - val_acc: 0.7832\n",
      "Epoch 18/50\n",
      "23986/23986 [==============================] - 685s 29ms/step - loss: 0.5020 - acc: 0.7954 - val_loss: 0.4797 - val_acc: 0.8074\n",
      "Epoch 19/50\n",
      "23986/23986 [==============================] - 684s 29ms/step - loss: 0.4931 - acc: 0.7997 - val_loss: 0.4848 - val_acc: 0.8072\n",
      "Epoch 20/50\n",
      "23986/23986 [==============================] - 686s 29ms/step - loss: 0.4894 - acc: 0.8002 - val_loss: 0.5042 - val_acc: 0.7961\n",
      "Epoch 21/50\n",
      "23986/23986 [==============================] - 685s 29ms/step - loss: 0.4848 - acc: 0.8021 - val_loss: 0.5381 - val_acc: 0.7729\n",
      "Epoch 22/50\n",
      "23986/23986 [==============================] - 687s 29ms/step - loss: 0.4835 - acc: 0.8033 - val_loss: 0.4899 - val_acc: 0.8044\n",
      "Epoch 23/50\n",
      "23986/23986 [==============================] - 687s 29ms/step - loss: 0.4761 - acc: 0.8058 - val_loss: 0.5227 - val_acc: 0.7792\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efa940bce50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "def conv_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    x = Activation('relu')(input)\n",
    "    x = Convolution2D(nb_filter, (1, 9), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    feature_list = [x]\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        x = conv_block(x, growth_rate, dropout_rate, weight_decay)\n",
    "        feature_list.append(x)\n",
    "        x = Concatenate(axis=concat_axis)(feature_list)\n",
    "        nb_filter += growth_rate\n",
    "\n",
    "    return x, nb_filter\n",
    "\n",
    "def transition_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    x = Convolution2D(nb_filter, (1, 1), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(input)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    x = AveragePooling2D((1, 2), strides=(1, 2))(x)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                           beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=16, dropout_rate=None,\n",
    "                     weight_decay=1E-4, verbose=True):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    assert (depth - 4) % 3 == 0, \"Depth must be 3 N + 4\"\n",
    "\n",
    "    # layers in each dense block\n",
    "    nb_layers = int((depth - 4) / 3)\n",
    "\n",
    "    # Initial convolution\n",
    "    x = Convolution2D(nb_filter, (1, 9), kernel_initializer=\"he_uniform\", padding=\"same\", name=\"initial_conv2D\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(model_input)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                            beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                                   weight_decay=weight_decay)\n",
    "        # add transition_block\n",
    "        x = transition_block(x, nb_filter, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # The last dense_block does not have a transition_block\n",
    "    x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                               weight_decay=weight_decay)\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(nb_classes, activation='softmax', kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    if verbose: \n",
    "        print(\"DenseNet-%d-%d created.\" % (depth, growth_rate))\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 1700\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "densenet_depth = 40\n",
    "densenet_growth_rate = 12\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim,depth=densenet_depth,\n",
    "                  growth_rate = densenet_growth_rate)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "xtrain = x_train.reshape(-1,12,1700,1)\n",
    "xvalid = x_valid.reshape(-1,12,1700,1)\n",
    "model.fit(xtrain, y_train, batch_size=10, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet-40-12 created.\n",
      "Train on 23986 samples, validate on 5997 samples\n",
      "Epoch 1/50\n",
      "23986/23986 [==============================] - 661s 28ms/step - loss: 0.8067 - acc: 0.6932 - val_loss: 1.0090 - val_acc: 0.5813\n",
      "Epoch 2/50\n",
      "23986/23986 [==============================] - 657s 27ms/step - loss: 0.7553 - acc: 0.7229 - val_loss: 0.7649 - val_acc: 0.7144\n",
      "Epoch 3/50\n",
      "23986/23986 [==============================] - 655s 27ms/step - loss: 0.7303 - acc: 0.7378 - val_loss: 0.8906 - val_acc: 0.6026\n",
      "Epoch 4/50\n",
      "23986/23986 [==============================] - 656s 27ms/step - loss: 0.7083 - acc: 0.7494 - val_loss: 0.8943 - val_acc: 0.6930\n",
      "Epoch 5/50\n",
      "23986/23986 [==============================] - 656s 27ms/step - loss: 0.6875 - acc: 0.7566 - val_loss: 0.6657 - val_acc: 0.7696\n",
      "Epoch 6/50\n",
      "23986/23986 [==============================] - 655s 27ms/step - loss: 0.6686 - acc: 0.7634 - val_loss: 0.6662 - val_acc: 0.7642\n",
      "Epoch 7/50\n",
      "23986/23986 [==============================] - 656s 27ms/step - loss: 0.6572 - acc: 0.7686 - val_loss: 0.6362 - val_acc: 0.7819\n",
      "Epoch 8/50\n",
      "23986/23986 [==============================] - 656s 27ms/step - loss: 0.6421 - acc: 0.7757 - val_loss: 0.6208 - val_acc: 0.7884\n",
      "Epoch 9/50\n",
      "23986/23986 [==============================] - 657s 27ms/step - loss: 0.6293 - acc: 0.7802 - val_loss: 0.6116 - val_acc: 0.7897\n",
      "Epoch 10/50\n",
      "23986/23986 [==============================] - 657s 27ms/step - loss: 0.6177 - acc: 0.7851 - val_loss: 0.7844 - val_acc: 0.7185\n",
      "Epoch 11/50\n",
      "23986/23986 [==============================] - 655s 27ms/step - loss: 0.6114 - acc: 0.7848 - val_loss: 0.6064 - val_acc: 0.7907\n",
      "Epoch 12/50\n",
      "23986/23986 [==============================] - 654s 27ms/step - loss: 0.6005 - acc: 0.7884 - val_loss: 0.6299 - val_acc: 0.7640\n",
      "Epoch 13/50\n",
      "23986/23986 [==============================] - 654s 27ms/step - loss: 0.5960 - acc: 0.7856 - val_loss: 0.5978 - val_acc: 0.7859\n",
      "Epoch 14/50\n",
      "23986/23986 [==============================] - 654s 27ms/step - loss: 0.5885 - acc: 0.7860 - val_loss: 0.5740 - val_acc: 0.7961\n",
      "Epoch 15/50\n",
      "23986/23986 [==============================] - 653s 27ms/step - loss: 0.5806 - acc: 0.7905 - val_loss: 0.6560 - val_acc: 0.7535\n",
      "Epoch 16/50\n",
      "23986/23986 [==============================] - 653s 27ms/step - loss: 0.5752 - acc: 0.7927 - val_loss: 0.5606 - val_acc: 0.8006\n",
      "Epoch 17/50\n",
      "23986/23986 [==============================] - 654s 27ms/step - loss: 0.5675 - acc: 0.7945 - val_loss: 0.5641 - val_acc: 0.7954\n",
      "Epoch 18/50\n",
      "23986/23986 [==============================] - 654s 27ms/step - loss: 0.5619 - acc: 0.7940 - val_loss: 0.5499 - val_acc: 0.8062\n",
      "Epoch 19/50\n",
      "23986/23986 [==============================] - 653s 27ms/step - loss: 0.5581 - acc: 0.7963 - val_loss: 0.6149 - val_acc: 0.7632\n",
      "Epoch 20/50\n",
      "23986/23986 [==============================] - 653s 27ms/step - loss: 0.5515 - acc: 0.7968 - val_loss: 0.6338 - val_acc: 0.7455\n",
      "Epoch 21/50\n",
      "23986/23986 [==============================] - 654s 27ms/step - loss: 0.5443 - acc: 0.7983 - val_loss: 0.5454 - val_acc: 0.7992\n",
      "Epoch 22/50\n",
      "23986/23986 [==============================] - 655s 27ms/step - loss: 0.5408 - acc: 0.7969 - val_loss: 0.5271 - val_acc: 0.8029\n",
      "Epoch 23/50\n",
      "23986/23986 [==============================] - 656s 27ms/step - loss: 0.5366 - acc: 0.7986 - val_loss: 0.6184 - val_acc: 0.7625\n",
      "Epoch 24/50\n",
      "23986/23986 [==============================] - 656s 27ms/step - loss: 0.5291 - acc: 0.8043 - val_loss: 0.5230 - val_acc: 0.8056\n",
      "Epoch 25/50\n",
      "23986/23986 [==============================] - 657s 27ms/step - loss: 0.5258 - acc: 0.8020 - val_loss: 0.5362 - val_acc: 0.7937\n",
      "Epoch 26/50\n",
      "23986/23986 [==============================] - 655s 27ms/step - loss: 0.5217 - acc: 0.8043 - val_loss: 0.5457 - val_acc: 0.7842\n",
      "Epoch 27/50\n",
      "23986/23986 [==============================] - 656s 27ms/step - loss: 0.5182 - acc: 0.8035 - val_loss: 0.5141 - val_acc: 0.8047\n",
      "Epoch 28/50\n",
      "23986/23986 [==============================] - 654s 27ms/step - loss: 0.5131 - acc: 0.8052 - val_loss: 0.5145 - val_acc: 0.8052\n",
      "Epoch 29/50\n",
      "23986/23986 [==============================] - 655s 27ms/step - loss: 0.5105 - acc: 0.8038 - val_loss: 0.5147 - val_acc: 0.7989\n",
      "Epoch 30/50\n",
      "23986/23986 [==============================] - 656s 27ms/step - loss: 0.5057 - acc: 0.8057 - val_loss: 0.5454 - val_acc: 0.7832\n",
      "Epoch 31/50\n",
      "23986/23986 [==============================] - 654s 27ms/step - loss: 0.5018 - acc: 0.8087 - val_loss: 0.4966 - val_acc: 0.8084\n",
      "Epoch 32/50\n",
      "23986/23986 [==============================] - 656s 27ms/step - loss: 0.4996 - acc: 0.8088 - val_loss: 0.4910 - val_acc: 0.8127\n",
      "Epoch 33/50\n",
      "23986/23986 [==============================] - 655s 27ms/step - loss: 0.4959 - acc: 0.8081 - val_loss: 0.4954 - val_acc: 0.8082\n",
      "Epoch 34/50\n",
      "23986/23986 [==============================] - 666s 28ms/step - loss: 0.4899 - acc: 0.8131 - val_loss: 0.5398 - val_acc: 0.7741\n",
      "Epoch 35/50\n",
      "23986/23986 [==============================] - 670s 28ms/step - loss: 0.4846 - acc: 0.8123 - val_loss: 0.4910 - val_acc: 0.8111\n",
      "Epoch 36/50\n",
      "23986/23986 [==============================] - 681s 28ms/step - loss: 0.4823 - acc: 0.8138 - val_loss: 0.4847 - val_acc: 0.8152\n",
      "Epoch 37/50\n",
      "23986/23986 [==============================] - 682s 28ms/step - loss: 0.4773 - acc: 0.8141 - val_loss: 0.4850 - val_acc: 0.8102\n",
      "Epoch 38/50\n",
      "23986/23986 [==============================] - 683s 28ms/step - loss: 0.4763 - acc: 0.8153 - val_loss: 0.4597 - val_acc: 0.8239\n",
      "Epoch 39/50\n",
      "23986/23986 [==============================] - 680s 28ms/step - loss: 0.4731 - acc: 0.8164 - val_loss: 0.5117 - val_acc: 0.7901\n",
      "Epoch 40/50\n",
      "23986/23986 [==============================] - 682s 28ms/step - loss: 0.4684 - acc: 0.8184 - val_loss: 0.4915 - val_acc: 0.8059\n",
      "Epoch 41/50\n",
      "23986/23986 [==============================] - 682s 28ms/step - loss: 0.4686 - acc: 0.8179 - val_loss: 0.4599 - val_acc: 0.8232\n",
      "Epoch 42/50\n",
      "23986/23986 [==============================] - 686s 29ms/step - loss: 0.4627 - acc: 0.8204 - val_loss: 0.4752 - val_acc: 0.8074\n",
      "Epoch 43/50\n",
      "23986/23986 [==============================] - 685s 29ms/step - loss: 0.4615 - acc: 0.8211 - val_loss: 0.4641 - val_acc: 0.8127\n",
      "Epoch 00043: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ef5aaf9ed50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "def conv_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    x = Activation('relu')(input)\n",
    "    x = Convolution2D(nb_filter, (1, 9), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    feature_list = [x]\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        x = conv_block(x, growth_rate, dropout_rate, weight_decay)\n",
    "        feature_list.append(x)\n",
    "        x = Concatenate(axis=concat_axis)(feature_list)\n",
    "        nb_filter += growth_rate\n",
    "\n",
    "    return x, nb_filter\n",
    "\n",
    "def transition_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    x = Convolution2D(nb_filter, (1, 1), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(input)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    x = AveragePooling2D((1, 2), strides=(1, 2))(x)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                           beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=16, dropout_rate=None,\n",
    "                     weight_decay=1E-4, verbose=True):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    assert (depth - 4) % 3 == 0, \"Depth must be 3 N + 4\"\n",
    "\n",
    "    # layers in each dense block\n",
    "    nb_layers = int((depth - 4) / 3)\n",
    "\n",
    "    # Initial convolution\n",
    "    x = Convolution2D(nb_filter, (1, 9), kernel_initializer=\"he_uniform\", padding=\"same\", name=\"initial_conv2D\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(model_input)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                            beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                                   weight_decay=weight_decay)\n",
    "        # add transition_block\n",
    "        x = transition_block(x, nb_filter, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # The last dense_block does not have a transition_block\n",
    "    x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                               weight_decay=weight_decay)\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(nb_classes, activation='softmax', kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    if verbose: \n",
    "        print(\"DenseNet-%d-%d created.\" % (depth, growth_rate))\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 1700\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "densenet_depth = 40\n",
    "densenet_growth_rate = 12\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim,depth=densenet_depth,\n",
    "                  growth_rate = densenet_growth_rate)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "xtrain = x_train.reshape(-1,12,1700,1)\n",
    "xvalid = x_valid.reshape(-1,12,1700,1)\n",
    "model.fit(xtrain, y_train, batch_size=24, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"attention_layer_7/div:0\", shape=(?, 77, 32), dtype=float32)\n",
      "Tensor(\"lstm_7/transpose_1:0\", shape=(?, ?, 32), dtype=float32)\n",
      "Tensor(\"attention_layer_7/mul:0\", shape=(?, 77, 32), dtype=float32)\n",
      "Train on 23986 samples, validate on 5997 samples\n",
      "Epoch 1/50\n",
      "23986/23986 [==============================] - 138s 6ms/step - loss: 0.6928 - acc: 0.5098 - val_loss: 0.6868 - val_acc: 0.5573\n",
      "Epoch 2/50\n",
      "23986/23986 [==============================] - 136s 6ms/step - loss: 0.6174 - acc: 0.6488 - val_loss: 0.5560 - val_acc: 0.7174\n",
      "Epoch 3/50\n",
      "23986/23986 [==============================] - 136s 6ms/step - loss: 0.5044 - acc: 0.7541 - val_loss: 0.4734 - val_acc: 0.7782\n",
      "Epoch 4/50\n",
      "23986/23986 [==============================] - 135s 6ms/step - loss: 0.4766 - acc: 0.7699 - val_loss: 0.5252 - val_acc: 0.7560\n",
      "Epoch 5/50\n",
      "23986/23986 [==============================] - 136s 6ms/step - loss: 0.4614 - acc: 0.7815 - val_loss: 0.4823 - val_acc: 0.7727\n",
      "Epoch 6/50\n",
      "23986/23986 [==============================] - 137s 6ms/step - loss: 0.4545 - acc: 0.7849 - val_loss: 0.4921 - val_acc: 0.7737\n",
      "Epoch 7/50\n",
      "23986/23986 [==============================] - 137s 6ms/step - loss: 0.4466 - acc: 0.7910 - val_loss: 0.4693 - val_acc: 0.7871\n",
      "Epoch 8/50\n",
      "23986/23986 [==============================] - 134s 6ms/step - loss: 0.4421 - acc: 0.7931 - val_loss: 0.5370 - val_acc: 0.7562\n",
      "Epoch 9/50\n",
      "23986/23986 [==============================] - 132s 5ms/step - loss: 0.4375 - acc: 0.7943 - val_loss: 0.4470 - val_acc: 0.7954\n",
      "Epoch 10/50\n",
      "23986/23986 [==============================] - 132s 6ms/step - loss: 0.4348 - acc: 0.7967 - val_loss: 0.4464 - val_acc: 0.7916\n",
      "Epoch 11/50\n",
      "23986/23986 [==============================] - 136s 6ms/step - loss: 0.4303 - acc: 0.7970 - val_loss: 0.4441 - val_acc: 0.7919\n",
      "Epoch 12/50\n",
      "23986/23986 [==============================] - 136s 6ms/step - loss: 0.4296 - acc: 0.8006 - val_loss: 0.4744 - val_acc: 0.7861\n",
      "Epoch 13/50\n",
      "23986/23986 [==============================] - 137s 6ms/step - loss: 0.4267 - acc: 0.8010 - val_loss: 0.4607 - val_acc: 0.7901\n",
      "Epoch 14/50\n",
      "23986/23986 [==============================] - 138s 6ms/step - loss: 0.4256 - acc: 0.8024 - val_loss: 0.4118 - val_acc: 0.8147\n",
      "Epoch 15/50\n",
      "23986/23986 [==============================] - 136s 6ms/step - loss: 0.4222 - acc: 0.8021 - val_loss: 0.4615 - val_acc: 0.7966\n",
      "Epoch 16/50\n",
      "23986/23986 [==============================] - 134s 6ms/step - loss: 0.4202 - acc: 0.8052 - val_loss: 0.4192 - val_acc: 0.8072\n",
      "Epoch 17/50\n",
      "23986/23986 [==============================] - 133s 6ms/step - loss: 0.4165 - acc: 0.8083 - val_loss: 0.4429 - val_acc: 0.8032\n",
      "Epoch 18/50\n",
      "23986/23986 [==============================] - 134s 6ms/step - loss: 0.4155 - acc: 0.8063 - val_loss: 0.4188 - val_acc: 0.8074\n",
      "Epoch 19/50\n",
      "23986/23986 [==============================] - 136s 6ms/step - loss: 0.4141 - acc: 0.8076 - val_loss: 0.4431 - val_acc: 0.7946\n",
      "Epoch 00019: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ef5a42e4bd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "batch_size = 32\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2), input_shape=(12* 1700,1)))\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(7)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               batch_input_shape=(batch_size, 12, 1700)))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Attention_layer())\n",
    "\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "xtrain = x_train.reshape(-1,12*1700,1)\n",
    "xvalid = x_valid.reshape(-1,12*1700,1)\n",
    "\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])\n",
    "# score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "# print model.predict(x_test)\n",
    "# print K.eval(categorical_accuracy(y_test, model.predict(x_test)))\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"attention_layer_9/div:0\", shape=(?, 77, 32), dtype=float32)\n",
      "Tensor(\"lstm_11/transpose_1:0\", shape=(?, ?, 32), dtype=float32)\n",
      "Tensor(\"attention_layer_9/mul:0\", shape=(?, 77, 32), dtype=float32)\n",
      "Train on 23986 samples, validate on 5997 samples\n",
      "Epoch 1/50\n",
      "23986/23986 [==============================] - 252s 10ms/step - loss: 0.6929 - acc: 0.5135 - val_loss: 0.6885 - val_acc: 0.5861\n",
      "Epoch 2/50\n",
      "23986/23986 [==============================] - 246s 10ms/step - loss: 0.6051 - acc: 0.6643 - val_loss: 0.5678 - val_acc: 0.6923\n",
      "Epoch 3/50\n",
      "23986/23986 [==============================] - 247s 10ms/step - loss: 0.5068 - acc: 0.7568 - val_loss: 0.5035 - val_acc: 0.7637\n",
      "Epoch 4/50\n",
      "23986/23986 [==============================] - 247s 10ms/step - loss: 0.4836 - acc: 0.7710 - val_loss: 0.4786 - val_acc: 0.7734\n",
      "Epoch 5/50\n",
      "23986/23986 [==============================] - 245s 10ms/step - loss: 0.4695 - acc: 0.7776 - val_loss: 0.4639 - val_acc: 0.7869\n",
      "Epoch 6/50\n",
      "23986/23986 [==============================] - 246s 10ms/step - loss: 0.4588 - acc: 0.7836 - val_loss: 0.4680 - val_acc: 0.7829\n",
      "Epoch 7/50\n",
      "23986/23986 [==============================] - 246s 10ms/step - loss: 0.4522 - acc: 0.7851 - val_loss: 0.4806 - val_acc: 0.7777\n",
      "Epoch 8/50\n",
      "23986/23986 [==============================] - 239s 10ms/step - loss: 0.4442 - acc: 0.7930 - val_loss: 0.4820 - val_acc: 0.7784\n",
      "Epoch 9/50\n",
      "23986/23986 [==============================] - 245s 10ms/step - loss: 0.4397 - acc: 0.7961 - val_loss: 0.5237 - val_acc: 0.7676\n",
      "Epoch 10/50\n",
      "23986/23986 [==============================] - 246s 10ms/step - loss: 0.4381 - acc: 0.7939 - val_loss: 0.4890 - val_acc: 0.7749\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ef599a79dd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "batch_size = 32\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2), input_shape=(12* 1700,1)))\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(7)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               batch_input_shape=(batch_size, 12, 1700)))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Attention_layer())\n",
    "\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "xtrain = x_train.reshape(-1,12*1700,1)\n",
    "xvalid = x_valid.reshape(-1,12*1700,1)\n",
    "\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])\n",
    "# score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "# print model.predict(x_test)\n",
    "# print K.eval(categorical_accuracy(y_test, model.predict(x_test)))\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"attention_layer_10/div:0\", shape=(?, 77, 32), dtype=float32)\n",
      "Tensor(\"lstm_14/transpose_1:0\", shape=(?, ?, 32), dtype=float32)\n",
      "Tensor(\"attention_layer_10/mul:0\", shape=(?, 77, 32), dtype=float32)\n",
      "Train on 23986 samples, validate on 5997 samples\n",
      "Epoch 1/50\n",
      "23986/23986 [==============================] - 361s 15ms/step - loss: 0.6939 - acc: 0.4967 - val_loss: 0.6932 - val_acc: 0.4989\n",
      "Epoch 2/50\n",
      "23986/23986 [==============================] - 356s 15ms/step - loss: 0.6937 - acc: 0.5023 - val_loss: 0.6932 - val_acc: 0.5011\n",
      "Epoch 3/50\n",
      "23986/23986 [==============================] - 362s 15ms/step - loss: 0.6937 - acc: 0.4978 - val_loss: 0.6935 - val_acc: 0.4989\n",
      "Epoch 4/50\n",
      "23986/23986 [==============================] - 364s 15ms/step - loss: 0.6938 - acc: 0.4974 - val_loss: 0.6948 - val_acc: 0.5011\n",
      "Epoch 5/50\n",
      "23986/23986 [==============================] - 354s 15ms/step - loss: 0.6935 - acc: 0.5039 - val_loss: 0.6934 - val_acc: 0.4991\n",
      "Epoch 6/50\n",
      "23986/23986 [==============================] - 357s 15ms/step - loss: 0.6899 - acc: 0.5270 - val_loss: 0.6721 - val_acc: 0.6003\n",
      "Epoch 7/50\n",
      "23986/23986 [==============================] - 364s 15ms/step - loss: 0.5764 - acc: 0.6992 - val_loss: 0.5963 - val_acc: 0.6782\n",
      "Epoch 8/50\n",
      "23986/23986 [==============================] - 357s 15ms/step - loss: 0.5187 - acc: 0.7466 - val_loss: 0.4863 - val_acc: 0.7711\n",
      "Epoch 9/50\n",
      "23986/23986 [==============================] - 365s 15ms/step - loss: 0.4927 - acc: 0.7627 - val_loss: 0.4637 - val_acc: 0.7839\n",
      "Epoch 10/50\n",
      "23986/23986 [==============================] - 366s 15ms/step - loss: 0.4755 - acc: 0.7741 - val_loss: 0.4591 - val_acc: 0.7836\n",
      "Epoch 11/50\n",
      "23986/23986 [==============================] - 357s 15ms/step - loss: 0.4664 - acc: 0.7786 - val_loss: 0.4996 - val_acc: 0.7686\n",
      "Epoch 12/50\n",
      "23986/23986 [==============================] - 365s 15ms/step - loss: 0.4593 - acc: 0.7849 - val_loss: 0.4792 - val_acc: 0.7771\n",
      "Epoch 13/50\n",
      "23986/23986 [==============================] - 362s 15ms/step - loss: 0.4521 - acc: 0.7865 - val_loss: 0.4453 - val_acc: 0.7911\n",
      "Epoch 14/50\n",
      "23986/23986 [==============================] - 359s 15ms/step - loss: 0.4492 - acc: 0.7883 - val_loss: 0.5017 - val_acc: 0.7691\n",
      "Epoch 15/50\n",
      "23986/23986 [==============================] - 369s 15ms/step - loss: 0.4452 - acc: 0.7924 - val_loss: 0.4487 - val_acc: 0.7957\n",
      "Epoch 16/50\n",
      "23986/23986 [==============================] - 368s 15ms/step - loss: 0.4399 - acc: 0.7928 - val_loss: 0.4397 - val_acc: 0.7992\n",
      "Epoch 17/50\n",
      "23986/23986 [==============================] - 356s 15ms/step - loss: 0.4383 - acc: 0.7977 - val_loss: 0.4246 - val_acc: 0.8044\n",
      "Epoch 18/50\n",
      "23986/23986 [==============================] - 364s 15ms/step - loss: 0.4341 - acc: 0.7987 - val_loss: 0.4417 - val_acc: 0.8029\n",
      "Epoch 19/50\n",
      "23986/23986 [==============================] - 361s 15ms/step - loss: 0.4335 - acc: 0.7954 - val_loss: 0.4631 - val_acc: 0.7857\n",
      "Epoch 20/50\n",
      "23986/23986 [==============================] - 357s 15ms/step - loss: 0.4300 - acc: 0.7999 - val_loss: 0.4325 - val_acc: 0.8027\n",
      "Epoch 21/50\n",
      "23986/23986 [==============================] - 366s 15ms/step - loss: 0.4288 - acc: 0.8002 - val_loss: 0.4285 - val_acc: 0.8064\n",
      "Epoch 22/50\n",
      "23986/23986 [==============================] - 365s 15ms/step - loss: 0.4271 - acc: 0.8011 - val_loss: 0.4129 - val_acc: 0.8084\n",
      "Epoch 23/50\n",
      "23986/23986 [==============================] - 357s 15ms/step - loss: 0.4269 - acc: 0.8017 - val_loss: 0.4485 - val_acc: 0.7946\n",
      "Epoch 24/50\n",
      "23986/23986 [==============================] - 365s 15ms/step - loss: 0.4219 - acc: 0.8060 - val_loss: 0.4811 - val_acc: 0.7851\n",
      "Epoch 25/50\n",
      "23986/23986 [==============================] - 362s 15ms/step - loss: 0.4202 - acc: 0.8044 - val_loss: 0.4252 - val_acc: 0.8051\n",
      "Epoch 26/50\n",
      "23986/23986 [==============================] - 355s 15ms/step - loss: 0.4178 - acc: 0.8082 - val_loss: 0.4574 - val_acc: 0.8004\n",
      "Epoch 27/50\n",
      "23986/23986 [==============================] - 361s 15ms/step - loss: 0.4184 - acc: 0.8060 - val_loss: 0.4052 - val_acc: 0.8181\n",
      "Epoch 28/50\n",
      "23986/23986 [==============================] - 361s 15ms/step - loss: 0.4172 - acc: 0.8068 - val_loss: 0.4279 - val_acc: 0.8079\n",
      "Epoch 29/50\n",
      "23986/23986 [==============================] - 353s 15ms/step - loss: 0.4142 - acc: 0.8078 - val_loss: 0.4220 - val_acc: 0.8091\n",
      "Epoch 30/50\n",
      "23986/23986 [==============================] - 355s 15ms/step - loss: 0.4132 - acc: 0.8059 - val_loss: 0.4508 - val_acc: 0.7986\n",
      "Epoch 31/50\n",
      "23986/23986 [==============================] - 358s 15ms/step - loss: 0.4128 - acc: 0.8098 - val_loss: 0.4136 - val_acc: 0.8126\n",
      "Epoch 32/50\n",
      "23986/23986 [==============================] - 350s 15ms/step - loss: 0.4107 - acc: 0.8095 - val_loss: 0.4145 - val_acc: 0.8124\n",
      "Epoch 00032: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ef594a9fd90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "batch_size = 32\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2), input_shape=(12* 1700,1)))\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(7)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               batch_input_shape=(batch_size, 12, 1700)))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "# model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Attention_layer())\n",
    "\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "xtrain = x_train.reshape(-1,12*1700,1)\n",
    "xvalid = x_valid.reshape(-1,12*1700,1)\n",
    "\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])\n",
    "# score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "# print model.predict(x_test)\n",
    "# print K.eval(categorical_accuracy(y_test, model.predict(x_test)))\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"attention_layer_11/div:0\", shape=(?, 77, 32), dtype=float32)\n",
      "Tensor(\"lstm_18/transpose_1:0\", shape=(?, ?, 32), dtype=float32)\n",
      "Tensor(\"attention_layer_11/mul:0\", shape=(?, 77, 32), dtype=float32)\n",
      "Train on 23986 samples, validate on 5997 samples\n",
      "Epoch 1/50\n",
      "23986/23986 [==============================] - 476s 20ms/step - loss: 0.6941 - acc: 0.4958 - val_loss: 0.6950 - val_acc: 0.4989\n",
      "Epoch 2/50\n",
      "23986/23986 [==============================] - 467s 19ms/step - loss: 0.6940 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5021\n",
      "Epoch 3/50\n",
      "23986/23986 [==============================] - 465s 19ms/step - loss: 0.6938 - acc: 0.5001 - val_loss: 0.6938 - val_acc: 0.4989\n",
      "Epoch 4/50\n",
      "23986/23986 [==============================] - 467s 19ms/step - loss: 0.6937 - acc: 0.5013 - val_loss: 0.6932 - val_acc: 0.5011\n",
      "Epoch 5/50\n",
      "23986/23986 [==============================] - 461s 19ms/step - loss: 0.6939 - acc: 0.4957 - val_loss: 0.6936 - val_acc: 0.4989\n",
      "Epoch 6/50\n",
      "23986/23986 [==============================] - 461s 19ms/step - loss: 0.6933 - acc: 0.5046 - val_loss: 0.6933 - val_acc: 0.4991\n",
      "Epoch 7/50\n",
      "23986/23986 [==============================] - 450s 19ms/step - loss: 0.6936 - acc: 0.5016 - val_loss: 0.6925 - val_acc: 0.5259\n",
      "Epoch 8/50\n",
      "23986/23986 [==============================] - 455s 19ms/step - loss: 0.6857 - acc: 0.5379 - val_loss: 0.6739 - val_acc: 0.5750\n",
      "Epoch 9/50\n",
      "23986/23986 [==============================] - 456s 19ms/step - loss: 0.5922 - acc: 0.6872 - val_loss: 0.5278 - val_acc: 0.7417\n",
      "Epoch 10/50\n",
      "23986/23986 [==============================] - 456s 19ms/step - loss: 0.5202 - acc: 0.7454 - val_loss: 0.4851 - val_acc: 0.7674\n",
      "Epoch 11/50\n",
      "23986/23986 [==============================] - 464s 19ms/step - loss: 0.4943 - acc: 0.7642 - val_loss: 0.4906 - val_acc: 0.7637\n",
      "Epoch 12/50\n",
      "23986/23986 [==============================] - 452s 19ms/step - loss: 0.4801 - acc: 0.7737 - val_loss: 0.5028 - val_acc: 0.7615\n",
      "Epoch 13/50\n",
      "23986/23986 [==============================] - 462s 19ms/step - loss: 0.4683 - acc: 0.7792 - val_loss: 0.4694 - val_acc: 0.7802\n",
      "Epoch 14/50\n",
      "23986/23986 [==============================] - 460s 19ms/step - loss: 0.4571 - acc: 0.7880 - val_loss: 0.4480 - val_acc: 0.7891\n",
      "Epoch 15/50\n",
      "23986/23986 [==============================] - 464s 19ms/step - loss: 0.4519 - acc: 0.7893 - val_loss: 0.4494 - val_acc: 0.7899\n",
      "Epoch 16/50\n",
      "23986/23986 [==============================] - 467s 19ms/step - loss: 0.4473 - acc: 0.7916 - val_loss: 0.4888 - val_acc: 0.7741\n",
      "Epoch 17/50\n",
      "23986/23986 [==============================] - 464s 19ms/step - loss: 0.4434 - acc: 0.7928 - val_loss: 0.4905 - val_acc: 0.7776\n",
      "Epoch 18/50\n",
      "23986/23986 [==============================] - 473s 20ms/step - loss: 0.4383 - acc: 0.7961 - val_loss: 0.4300 - val_acc: 0.8084\n",
      "Epoch 19/50\n",
      "23986/23986 [==============================] - 462s 19ms/step - loss: 0.4322 - acc: 0.8013 - val_loss: 0.4827 - val_acc: 0.7804\n",
      "Epoch 20/50\n",
      "23986/23986 [==============================] - 468s 20ms/step - loss: 0.4309 - acc: 0.8001 - val_loss: 0.4336 - val_acc: 0.7999\n",
      "Epoch 21/50\n",
      "23986/23986 [==============================] - 473s 20ms/step - loss: 0.4268 - acc: 0.8029 - val_loss: 0.4259 - val_acc: 0.8072\n",
      "Epoch 22/50\n",
      "23986/23986 [==============================] - 462s 19ms/step - loss: 0.4277 - acc: 0.8023 - val_loss: 0.4065 - val_acc: 0.8151\n",
      "Epoch 23/50\n",
      "23986/23986 [==============================] - 474s 20ms/step - loss: 0.4266 - acc: 0.8030 - val_loss: 0.4437 - val_acc: 0.7966\n",
      "Epoch 24/50\n",
      "23986/23986 [==============================] - 475s 20ms/step - loss: 0.4210 - acc: 0.8071 - val_loss: 0.4136 - val_acc: 0.8132\n",
      "Epoch 25/50\n",
      "23986/23986 [==============================] - 468s 20ms/step - loss: 0.4209 - acc: 0.8049 - val_loss: 0.4371 - val_acc: 0.8002\n",
      "Epoch 26/50\n",
      "23986/23986 [==============================] - 466s 19ms/step - loss: 0.4198 - acc: 0.8078 - val_loss: 0.4268 - val_acc: 0.8067\n",
      "Epoch 27/50\n",
      "23986/23986 [==============================] - 478s 20ms/step - loss: 0.4152 - acc: 0.8096 - val_loss: 0.4387 - val_acc: 0.8017\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ef59d33be90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "batch_size = 32\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2), input_shape=(12* 1700,1)))\n",
    "model.add(Convolution1D(6, (21), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(7)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(7, (13), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(Convolution1D(5, (9), kernel_initializer='glorot_uniform', activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2)))\n",
    "model.add(MaxPooling1D(pool_size=(6)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               batch_input_shape=(batch_size, 12, 1700)))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Attention_layer())\n",
    "\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "xtrain = x_train.reshape(-1,12*1700,1)\n",
    "xvalid = x_valid.reshape(-1,12*1700,1)\n",
    "\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])\n",
    "# score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "# print model.predict(x_test)\n",
    "# print K.eval(categorical_accuracy(y_test, model.predict(x_test)))\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
