{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Convolution1D, Conv2D, MaxPooling2D, MaxPooling1D, LSTM, Embedding\n",
    "from keras.optimizers import SGD\n",
    "import keras.backend as K\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "config_proto = tf.ConfigProto(log_device_placement=0,allow_soft_placement=0)\n",
    "config_proto.gpu_options.allow_growth = True\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../../20_Galaxy/ANGRY/2HXT-train.npy')\n",
    "Y_train = np.load('../../20_Galaxy/ANGRY/2HXT-train_label.npy')\n",
    "\n",
    "X_valid = np.load('../../20_Galaxy/ANGRY/2HXT-val.npy')\n",
    "Y_valid = np.load('../../20_Galaxy/ANGRY/2HXT-val_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "one_hot = preprocessing.OneHotEncoder(sparse = False)\n",
    "y_train = one_hot.fit_transform(Y_train)\n",
    "y_valid = one_hot.fit_transform(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet-40-12 created.\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 933s 60ms/step - loss: 0.8302 - acc: 0.6554 - val_loss: 0.7641 - val_acc: 0.7398\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 927s 59ms/step - loss: 0.7789 - acc: 0.6979 - val_loss: 0.8154 - val_acc: 0.7255\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 924s 59ms/step - loss: 0.7597 - acc: 0.7019 - val_loss: 0.7210 - val_acc: 0.7658\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 922s 59ms/step - loss: 0.7371 - acc: 0.7171 - val_loss: 0.7049 - val_acc: 0.7978\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 921s 59ms/step - loss: 0.7197 - acc: 0.7237 - val_loss: 0.6813 - val_acc: 0.7615\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 920s 59ms/step - loss: 0.6979 - acc: 0.7300 - val_loss: 2.6567 - val_acc: 0.5000\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 920s 59ms/step - loss: 0.6859 - acc: 0.7362 - val_loss: 1.2178 - val_acc: 0.5465\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 921s 59ms/step - loss: 0.6696 - acc: 0.7396 - val_loss: 0.6015 - val_acc: 0.8175\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 917s 59ms/step - loss: 0.6542 - acc: 0.7493 - val_loss: 0.6027 - val_acc: 0.7988\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 917s 59ms/step - loss: 0.6420 - acc: 0.7539 - val_loss: 0.5412 - val_acc: 0.8295\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 918s 59ms/step - loss: 0.6301 - acc: 0.7547 - val_loss: 0.5552 - val_acc: 0.8290\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 916s 59ms/step - loss: 0.6170 - acc: 0.7592 - val_loss: 0.5619 - val_acc: 0.8153\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 916s 59ms/step - loss: 0.6049 - acc: 0.7693 - val_loss: 0.6991 - val_acc: 0.7425\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 919s 59ms/step - loss: 0.5931 - acc: 0.7657 - val_loss: 0.6055 - val_acc: 0.8255\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 919s 59ms/step - loss: 0.5826 - acc: 0.7791 - val_loss: 0.6233 - val_acc: 0.7967\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6df05f9790>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "def conv_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    x = Activation('relu')(input)\n",
    "    x = Convolution2D(nb_filter, (1, 9), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    feature_list = [x]\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        x = conv_block(x, growth_rate, dropout_rate, weight_decay)\n",
    "        feature_list.append(x)\n",
    "        x = Concatenate(axis=concat_axis)(feature_list)\n",
    "        nb_filter += growth_rate\n",
    "\n",
    "    return x, nb_filter\n",
    "\n",
    "def transition_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    x = Convolution2D(nb_filter, (1, 1), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(input)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    x = AveragePooling2D((1, 2), strides=(1, 2))(x)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                           beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=16, dropout_rate=None,\n",
    "                     weight_decay=1E-4, verbose=True):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    assert (depth - 4) % 3 == 0, \"Depth must be 3 N + 4\"\n",
    "\n",
    "    # layers in each dense block\n",
    "    nb_layers = int((depth - 4) / 3)\n",
    "\n",
    "    # Initial convolution\n",
    "    x = Convolution2D(nb_filter, (1, 9), kernel_initializer=\"he_uniform\", padding=\"same\", name=\"initial_conv2D\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(model_input)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                            beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                                   weight_decay=weight_decay)\n",
    "        # add transition_block\n",
    "        x = transition_block(x, nb_filter, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # The last dense_block does not have a transition_block\n",
    "    x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                               weight_decay=weight_decay)\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(nb_classes, activation='softmax', kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    if verbose: \n",
    "        print(\"DenseNet-%d-%d created.\" % (depth, growth_rate))\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "densenet_depth = 40\n",
    "densenet_growth_rate = 12\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim,depth=densenet_depth,\n",
    "                  growth_rate = densenet_growth_rate)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=12, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet-40-8 created.\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 941s 60ms/step - loss: 0.7439 - acc: 0.6722 - val_loss: 0.7206 - val_acc: 0.7945\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 941s 60ms/step - loss: 0.6799 - acc: 0.7253 - val_loss: 0.7019 - val_acc: 0.7370\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 940s 60ms/step - loss: 0.6520 - acc: 0.7397 - val_loss: 0.6777 - val_acc: 0.7975\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 942s 60ms/step - loss: 0.6269 - acc: 0.7565 - val_loss: 0.6694 - val_acc: 0.8205\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 942s 60ms/step - loss: 0.6044 - acc: 0.7698 - val_loss: 0.7361 - val_acc: 0.8093\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 937s 60ms/step - loss: 0.5881 - acc: 0.7772 - val_loss: 0.5123 - val_acc: 0.8620\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 940s 60ms/step - loss: 0.5719 - acc: 0.7878 - val_loss: 0.6164 - val_acc: 0.8498\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 939s 60ms/step - loss: 0.5640 - acc: 0.7880 - val_loss: 0.6215 - val_acc: 0.8450\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 936s 60ms/step - loss: 0.5489 - acc: 0.7948 - val_loss: 0.6153 - val_acc: 0.8280\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 935s 60ms/step - loss: 0.5297 - acc: 0.8044 - val_loss: 0.5459 - val_acc: 0.8158\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 936s 60ms/step - loss: 0.5294 - acc: 0.8020 - val_loss: 0.6098 - val_acc: 0.8745\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6dbbbc4a10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "def conv_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    x = Activation('relu')(input)\n",
    "    x = Convolution2D(nb_filter, (1, 21), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    feature_list = [x]\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        x = conv_block(x, growth_rate, dropout_rate, weight_decay)\n",
    "        feature_list.append(x)\n",
    "        x = Concatenate(axis=concat_axis)(feature_list)\n",
    "        nb_filter += growth_rate\n",
    "\n",
    "    return x, nb_filter\n",
    "\n",
    "def transition_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    x = Convolution2D(nb_filter, (1, 1), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(input)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    x = AveragePooling2D((1, 2), strides=(1, 2))(x)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                           beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=16, dropout_rate=None,\n",
    "                     weight_decay=1E-4, verbose=True):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    assert (depth - 4) % 3 == 0, \"Depth must be 3 N + 4\"\n",
    "\n",
    "    # layers in each dense block\n",
    "    nb_layers = int((depth - 4) / 3)\n",
    "\n",
    "    # Initial convolution\n",
    "    x = Convolution2D(nb_filter, (1, 21), kernel_initializer=\"he_uniform\", padding=\"same\", name=\"initial_conv2D\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(model_input)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                            beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                                   weight_decay=weight_decay)\n",
    "        # add transition_block\n",
    "        x = transition_block(x, nb_filter, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # The last dense_block does not have a transition_block\n",
    "    x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                               weight_decay=weight_decay)\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(nb_classes, activation='softmax', kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    if verbose: \n",
    "        print(\"DenseNet-%d-%d created.\" % (depth, growth_rate))\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "densenet_depth = 40\n",
    "densenet_growth_rate = 8\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim,depth=densenet_depth,\n",
    "                  growth_rate = densenet_growth_rate)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=12, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet-40-8 created.\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 917s 59ms/step - loss: 0.7422 - acc: 0.6827 - val_loss: 0.6711 - val_acc: 0.7595\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 918s 59ms/step - loss: 0.6803 - acc: 0.7279 - val_loss: 0.7303 - val_acc: 0.7645\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 917s 59ms/step - loss: 0.6629 - acc: 0.7408 - val_loss: 0.6969 - val_acc: 0.7673\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 920s 59ms/step - loss: 0.6383 - acc: 0.7576 - val_loss: 0.5879 - val_acc: 0.8315\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 911s 58ms/step - loss: 0.6125 - acc: 0.7726 - val_loss: 0.5980 - val_acc: 0.8320\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 917s 59ms/step - loss: 0.5932 - acc: 0.7862 - val_loss: 0.6111 - val_acc: 0.8410\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 913s 58ms/step - loss: 0.5721 - acc: 0.7991 - val_loss: 0.5732 - val_acc: 0.8603\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 921s 59ms/step - loss: 0.5553 - acc: 0.8064 - val_loss: 0.8350 - val_acc: 0.7725\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 920s 59ms/step - loss: 0.5423 - acc: 0.8140 - val_loss: 0.6287 - val_acc: 0.8508\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 925s 59ms/step - loss: 0.5396 - acc: 0.8150 - val_loss: 0.9699 - val_acc: 0.6850\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 923s 59ms/step - loss: 0.5221 - acc: 0.8221 - val_loss: 0.6613 - val_acc: 0.8388\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 923s 59ms/step - loss: 0.5167 - acc: 0.8237 - val_loss: 0.5850 - val_acc: 0.8375\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 922s 59ms/step - loss: 0.5092 - acc: 0.8248 - val_loss: 0.5124 - val_acc: 0.8323\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 920s 59ms/step - loss: 0.5106 - acc: 0.8228 - val_loss: 0.6522 - val_acc: 0.8565\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 917s 59ms/step - loss: 0.4987 - acc: 0.8264 - val_loss: 0.5653 - val_acc: 0.8685\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 922s 59ms/step - loss: 0.4920 - acc: 0.8300 - val_loss: 0.7891 - val_acc: 0.8093\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 922s 59ms/step - loss: 0.4970 - acc: 0.8304 - val_loss: 0.5115 - val_acc: 0.8638\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 926s 59ms/step - loss: 0.4836 - acc: 0.8332 - val_loss: 0.5138 - val_acc: 0.8705\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 924s 59ms/step - loss: 0.4814 - acc: 0.8346 - val_loss: 0.5154 - val_acc: 0.8560\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 928s 59ms/step - loss: 0.4755 - acc: 0.8371 - val_loss: 0.5536 - val_acc: 0.8735\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 926s 59ms/step - loss: 0.4740 - acc: 0.8371 - val_loss: 0.5496 - val_acc: 0.8745\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 929s 59ms/step - loss: 0.4690 - acc: 0.8362 - val_loss: 0.4979 - val_acc: 0.8758\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 925s 59ms/step - loss: 0.4668 - acc: 0.8382 - val_loss: 0.4994 - val_acc: 0.8743\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 922s 59ms/step - loss: 0.4679 - acc: 0.8357 - val_loss: 0.5650 - val_acc: 0.8745\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 923s 59ms/step - loss: 0.4575 - acc: 0.8424 - val_loss: 0.5592 - val_acc: 0.8800\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 924s 59ms/step - loss: 0.4552 - acc: 0.8428 - val_loss: 0.4889 - val_acc: 0.8573\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 927s 59ms/step - loss: 0.4585 - acc: 0.8358 - val_loss: 0.6832 - val_acc: 0.8488\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 926s 59ms/step - loss: 0.4524 - acc: 0.8414 - val_loss: 0.5046 - val_acc: 0.8795\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 928s 59ms/step - loss: 0.4484 - acc: 0.8403 - val_loss: 0.5650 - val_acc: 0.8733\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 927s 59ms/step - loss: 0.4424 - acc: 0.8454 - val_loss: 0.5904 - val_acc: 0.8633\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 929s 59ms/step - loss: 0.4422 - acc: 0.8439 - val_loss: 0.5201 - val_acc: 0.8758\n",
      "Epoch 32/50\n",
      "15658/15658 [==============================] - 926s 59ms/step - loss: 0.4400 - acc: 0.8452 - val_loss: 0.5578 - val_acc: 0.8575\n",
      "Epoch 33/50\n",
      "15658/15658 [==============================] - 923s 59ms/step - loss: 0.4378 - acc: 0.8430 - val_loss: 0.6803 - val_acc: 0.8583\n",
      "Epoch 34/50\n",
      "15658/15658 [==============================] - 928s 59ms/step - loss: 0.4378 - acc: 0.8430 - val_loss: 0.4757 - val_acc: 0.8768\n",
      "Epoch 35/50\n",
      "15658/15658 [==============================] - 925s 59ms/step - loss: 0.4370 - acc: 0.8440 - val_loss: 0.5633 - val_acc: 0.8790\n",
      "Epoch 36/50\n",
      "15658/15658 [==============================] - 928s 59ms/step - loss: 0.4337 - acc: 0.8446 - val_loss: 0.4695 - val_acc: 0.8835\n",
      "Epoch 37/50\n",
      "15658/15658 [==============================] - 929s 59ms/step - loss: 0.4332 - acc: 0.8426 - val_loss: 0.6420 - val_acc: 0.8655\n",
      "Epoch 38/50\n",
      "15658/15658 [==============================] - 928s 59ms/step - loss: 0.4249 - acc: 0.8453 - val_loss: 0.7010 - val_acc: 0.8155\n",
      "Epoch 39/50\n",
      "15658/15658 [==============================] - 925s 59ms/step - loss: 0.4258 - acc: 0.8458 - val_loss: 0.5208 - val_acc: 0.8848\n",
      "Epoch 40/50\n",
      "15658/15658 [==============================] - 923s 59ms/step - loss: 0.4214 - acc: 0.8487 - val_loss: 0.5338 - val_acc: 0.8655\n",
      "Epoch 41/50\n",
      "15658/15658 [==============================] - 929s 59ms/step - loss: 0.4220 - acc: 0.8480 - val_loss: 0.5184 - val_acc: 0.8868\n",
      "Epoch 42/50\n",
      "15658/15658 [==============================] - 926s 59ms/step - loss: 0.4269 - acc: 0.8438 - val_loss: 0.5400 - val_acc: 0.8798\n",
      "Epoch 43/50\n",
      "15658/15658 [==============================] - 929s 59ms/step - loss: 0.4206 - acc: 0.8463 - val_loss: 0.4235 - val_acc: 0.8850\n",
      "Epoch 44/50\n",
      "15658/15658 [==============================] - 925s 59ms/step - loss: 0.4289 - acc: 0.8417 - val_loss: 0.6001 - val_acc: 0.8638\n",
      "Epoch 45/50\n",
      "15658/15658 [==============================] - 929s 59ms/step - loss: 0.4185 - acc: 0.8481 - val_loss: 0.6528 - val_acc: 0.7328\n",
      "Epoch 46/50\n",
      "15658/15658 [==============================] - 929s 59ms/step - loss: 0.4159 - acc: 0.8455 - val_loss: 0.5352 - val_acc: 0.8595\n",
      "Epoch 47/50\n",
      "15658/15658 [==============================] - 932s 60ms/step - loss: 0.4189 - acc: 0.8433 - val_loss: 0.4796 - val_acc: 0.8505\n",
      "Epoch 48/50\n",
      "15658/15658 [==============================] - 931s 59ms/step - loss: 0.4160 - acc: 0.8481 - val_loss: 0.5132 - val_acc: 0.8700\n",
      "Epoch 49/50\n",
      "15658/15658 [==============================] - 930s 59ms/step - loss: 0.4084 - acc: 0.8506 - val_loss: 0.5681 - val_acc: 0.8393\n",
      "Epoch 50/50\n",
      "15658/15658 [==============================] - 925s 59ms/step - loss: 0.4074 - acc: 0.8484 - val_loss: 0.4944 - val_acc: 0.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79dcb6d310>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "def conv_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    x = Activation('relu')(input)\n",
    "    x = Convolution2D(nb_filter, (1, 21), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    feature_list = [x]\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        x = conv_block(x, growth_rate, dropout_rate, weight_decay)\n",
    "        feature_list.append(x)\n",
    "        x = Concatenate(axis=concat_axis)(feature_list)\n",
    "        nb_filter += growth_rate\n",
    "\n",
    "    return x, nb_filter\n",
    "\n",
    "def transition_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    x = Convolution2D(nb_filter, (1, 1), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(input)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    x = AveragePooling2D((1, 2), strides=(1, 2))(x)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                           beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=16, dropout_rate=None,\n",
    "                     weight_decay=1E-4, verbose=True):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    assert (depth - 4) % 3 == 0, \"Depth must be 3 N + 4\"\n",
    "\n",
    "    # layers in each dense block\n",
    "    nb_layers = int((depth - 4) / 3)\n",
    "\n",
    "    # Initial convolution\n",
    "    x = Convolution2D(nb_filter, (1, 21), kernel_initializer=\"he_uniform\", padding=\"same\", name=\"initial_conv2D\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(model_input)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                            beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                                   weight_decay=weight_decay)\n",
    "        # add transition_block\n",
    "        x = transition_block(x, nb_filter, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # The last dense_block does not have a transition_block\n",
    "    x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                               weight_decay=weight_decay)\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(nb_classes, activation='softmax', kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    if verbose: \n",
    "        print(\"DenseNet-%d-%d created.\" % (depth, growth_rate))\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 12\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "densenet_depth = 40\n",
    "densenet_growth_rate = 8\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim,depth=densenet_depth,\n",
    "                  growth_rate = densenet_growth_rate)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=12, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet-40-10 created.\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 1142s 73ms/step - loss: 0.7970 - acc: 0.6624 - val_loss: 1.0330 - val_acc: 0.6465\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 1143s 73ms/step - loss: 0.7361 - acc: 0.7155 - val_loss: 0.7228 - val_acc: 0.8063\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 1148s 73ms/step - loss: 0.7119 - acc: 0.7325 - val_loss: 0.6772 - val_acc: 0.8150\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 1149s 73ms/step - loss: 0.6929 - acc: 0.7497 - val_loss: 0.7958 - val_acc: 0.7313\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 1145s 73ms/step - loss: 0.6739 - acc: 0.7565 - val_loss: 0.6723 - val_acc: 0.8265\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 1146s 73ms/step - loss: 0.6547 - acc: 0.7739 - val_loss: 0.6267 - val_acc: 0.8045\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 1144s 73ms/step - loss: 0.6289 - acc: 0.7941 - val_loss: 0.6246 - val_acc: 0.8638\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 1142s 73ms/step - loss: 0.6179 - acc: 0.8004 - val_loss: 0.8375 - val_acc: 0.6808\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 1144s 73ms/step - loss: 0.6048 - acc: 0.8089 - val_loss: 0.7090 - val_acc: 0.8415\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 1142s 73ms/step - loss: 0.5996 - acc: 0.8114 - val_loss: 0.7012 - val_acc: 0.8085\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 1147s 73ms/step - loss: 0.5878 - acc: 0.8182 - val_loss: 0.6554 - val_acc: 0.8508\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 1138s 73ms/step - loss: 0.5846 - acc: 0.8188 - val_loss: 0.5952 - val_acc: 0.8680\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 1136s 73ms/step - loss: 0.5764 - acc: 0.8214 - val_loss: 0.6187 - val_acc: 0.8693\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 1138s 73ms/step - loss: 0.5733 - acc: 0.8221 - val_loss: 0.5472 - val_acc: 0.8765\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 1142s 73ms/step - loss: 0.5655 - acc: 0.8250 - val_loss: 0.5671 - val_acc: 0.8683\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 1140s 73ms/step - loss: 0.5683 - acc: 0.8231 - val_loss: 0.5147 - val_acc: 0.8793\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 1142s 73ms/step - loss: 0.5630 - acc: 0.8262 - val_loss: 0.5563 - val_acc: 0.8785\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 1142s 73ms/step - loss: 0.5574 - acc: 0.8274 - val_loss: 0.6743 - val_acc: 0.8295\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 1143s 73ms/step - loss: 0.5565 - acc: 0.8288 - val_loss: 0.6241 - val_acc: 0.8750\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 1146s 73ms/step - loss: 0.5558 - acc: 0.8292 - val_loss: 0.5907 - val_acc: 0.8740\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 1139s 73ms/step - loss: 0.5473 - acc: 0.8336 - val_loss: 0.6292 - val_acc: 0.8675\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 1139s 73ms/step - loss: 0.5466 - acc: 0.8332 - val_loss: 0.7314 - val_acc: 0.7878\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 1142s 73ms/step - loss: 0.5448 - acc: 0.8322 - val_loss: 0.6277 - val_acc: 0.8278\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 1134s 72ms/step - loss: 0.5451 - acc: 0.8325 - val_loss: 0.6079 - val_acc: 0.8633\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 1143s 73ms/step - loss: 0.5402 - acc: 0.8342 - val_loss: 0.6337 - val_acc: 0.8760\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 1140s 73ms/step - loss: 0.5413 - acc: 0.8354 - val_loss: 0.6135 - val_acc: 0.8823\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 1142s 73ms/step - loss: 0.5354 - acc: 0.8387 - val_loss: 0.5691 - val_acc: 0.8410\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 1136s 73ms/step - loss: 0.5352 - acc: 0.8368 - val_loss: 0.6143 - val_acc: 0.8528\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 1133s 72ms/step - loss: 0.5357 - acc: 0.8365 - val_loss: 0.6307 - val_acc: 0.8385\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 1137s 73ms/step - loss: 0.5316 - acc: 0.8385 - val_loss: 0.5290 - val_acc: 0.8858\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 1143s 73ms/step - loss: 0.5279 - acc: 0.8407 - val_loss: 0.4896 - val_acc: 0.8905\n",
      "Epoch 32/50\n",
      "15658/15658 [==============================] - 1140s 73ms/step - loss: 0.5241 - acc: 0.8409 - val_loss: 0.8251 - val_acc: 0.8133\n",
      "Epoch 33/50\n",
      "15658/15658 [==============================] - 1145s 73ms/step - loss: 0.5241 - acc: 0.8420 - val_loss: 0.7416 - val_acc: 0.8835\n",
      "Epoch 34/50\n",
      "15658/15658 [==============================] - 1143s 73ms/step - loss: 0.5239 - acc: 0.8440 - val_loss: 0.5234 - val_acc: 0.8815\n",
      "Epoch 35/50\n",
      "15658/15658 [==============================] - 1140s 73ms/step - loss: 0.5217 - acc: 0.8416 - val_loss: 0.6850 - val_acc: 0.8508\n",
      "Epoch 36/50\n",
      "15658/15658 [==============================] - 1140s 73ms/step - loss: 0.5226 - acc: 0.8422 - val_loss: 0.5400 - val_acc: 0.8790\n",
      "Epoch 37/50\n",
      "15658/15658 [==============================] - 1138s 73ms/step - loss: 0.5169 - acc: 0.8460 - val_loss: 0.5620 - val_acc: 0.8820\n",
      "Epoch 38/50\n",
      "15658/15658 [==============================] - 1140s 73ms/step - loss: 0.5185 - acc: 0.8428 - val_loss: 0.6797 - val_acc: 0.8770\n",
      "Epoch 39/50\n",
      "15658/15658 [==============================] - 1142s 73ms/step - loss: 0.5168 - acc: 0.8447 - val_loss: 0.6189 - val_acc: 0.8828\n",
      "Epoch 40/50\n",
      "15658/15658 [==============================] - 1142s 73ms/step - loss: 0.5127 - acc: 0.8467 - val_loss: 0.6320 - val_acc: 0.8773\n",
      "Epoch 41/50\n",
      "15658/15658 [==============================] - 1145s 73ms/step - loss: 0.5102 - acc: 0.8453 - val_loss: 0.5495 - val_acc: 0.8765\n",
      "Epoch 00041: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79ccd75190>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "def conv_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    x = Activation('relu')(input)\n",
    "    x = Convolution2D(nb_filter, (1, 21), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    feature_list = [x]\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        x = conv_block(x, growth_rate, dropout_rate, weight_decay)\n",
    "        feature_list.append(x)\n",
    "        x = Concatenate(axis=concat_axis)(feature_list)\n",
    "        nb_filter += growth_rate\n",
    "\n",
    "    return x, nb_filter\n",
    "\n",
    "def transition_block(input, nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    x = Convolution2D(nb_filter, (1, 1), kernel_initializer=\"he_uniform\", padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(input)\n",
    "    if dropout_rate is not None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    x = AveragePooling2D((1, 2), strides=(1, 2))(x)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                           beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=16, dropout_rate=None,\n",
    "                     weight_decay=1E-4, verbose=True):\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    assert (depth - 4) % 3 == 0, \"Depth must be 3 N + 4\"\n",
    "\n",
    "    # layers in each dense block\n",
    "    nb_layers = int((depth - 4) / 3)\n",
    "\n",
    "    # Initial convolution\n",
    "    x = Convolution2D(nb_filter, (1, 21), kernel_initializer=\"he_uniform\", padding=\"same\", name=\"initial_conv2D\", use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay))(model_input)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                            beta_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                                   weight_decay=weight_decay)\n",
    "        # add transition_block\n",
    "        x = transition_block(x, nb_filter, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # The last dense_block does not have a transition_block\n",
    "    x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                               weight_decay=weight_decay)\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(nb_classes, activation='softmax', kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    if verbose: \n",
    "        print(\"DenseNet-%d-%d created.\" % (depth, growth_rate))\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 12\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "densenet_depth = 40\n",
    "densenet_growth_rate = 10\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim,depth=densenet_depth,\n",
    "                  growth_rate = densenet_growth_rate)\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=12, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_259/concat:0\", shape=(?, 10, 480), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 115s 7ms/step - loss: 0.6069 - acc: 0.6500 - val_loss: 0.4529 - val_acc: 0.8217\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 103s 7ms/step - loss: 0.4697 - acc: 0.7731 - val_loss: 0.4463 - val_acc: 0.8605\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 105s 7ms/step - loss: 0.4135 - acc: 0.8106 - val_loss: 0.4503 - val_acc: 0.8698\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 106s 7ms/step - loss: 0.3823 - acc: 0.8303 - val_loss: 0.4869 - val_acc: 0.8867\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 106s 7ms/step - loss: 0.3686 - acc: 0.8389 - val_loss: 0.4642 - val_acc: 0.8992\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 106s 7ms/step - loss: 0.3573 - acc: 0.8458 - val_loss: 0.4401 - val_acc: 0.8902\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 106s 7ms/step - loss: 0.3452 - acc: 0.8536 - val_loss: 0.3691 - val_acc: 0.8845\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 106s 7ms/step - loss: 0.3377 - acc: 0.8530 - val_loss: 0.3949 - val_acc: 0.8950\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 106s 7ms/step - loss: 0.3378 - acc: 0.8559 - val_loss: 0.3716 - val_acc: 0.9025\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 106s 7ms/step - loss: 0.3323 - acc: 0.8586 - val_loss: 0.3628 - val_acc: 0.8858\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 107s 7ms/step - loss: 0.3300 - acc: 0.8613 - val_loss: 0.3877 - val_acc: 0.9020\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 106s 7ms/step - loss: 0.3245 - acc: 0.8606 - val_loss: 0.3614 - val_acc: 0.8920\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 105s 7ms/step - loss: 0.3216 - acc: 0.8631 - val_loss: 0.3680 - val_acc: 0.9050\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 105s 7ms/step - loss: 0.3220 - acc: 0.8645 - val_loss: 0.3610 - val_acc: 0.8945\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 105s 7ms/step - loss: 0.3184 - acc: 0.8675 - val_loss: 0.3421 - val_acc: 0.8998\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 105s 7ms/step - loss: 0.3150 - acc: 0.8675 - val_loss: 0.3557 - val_acc: 0.9002\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 105s 7ms/step - loss: 0.3135 - acc: 0.8659 - val_loss: 0.4142 - val_acc: 0.9045\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 106s 7ms/step - loss: 0.3120 - acc: 0.8674 - val_loss: 0.3852 - val_acc: 0.8945\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79cbb5fd50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(ROWS):\n",
    "        inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(48, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(48, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(56, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(56, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(40, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(40, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS,COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12,3640,1)\n",
    "xvalid = X_valid.reshape(-1,12,3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_267/concat:0\", shape=(?, 507, 30), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 633s 40ms/step - loss: 0.5566 - acc: 0.7000 - val_loss: 0.4535 - val_acc: 0.8482\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 611s 39ms/step - loss: 0.4418 - acc: 0.7940 - val_loss: 0.4424 - val_acc: 0.8642\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 612s 39ms/step - loss: 0.4237 - acc: 0.8046 - val_loss: 0.4328 - val_acc: 0.8682\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 611s 39ms/step - loss: 0.3799 - acc: 0.8317 - val_loss: 0.4040 - val_acc: 0.8980\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 610s 39ms/step - loss: 0.3572 - acc: 0.8469 - val_loss: 0.4547 - val_acc: 0.8640\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 606s 39ms/step - loss: 0.3450 - acc: 0.8534 - val_loss: 0.3834 - val_acc: 0.9045\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 605s 39ms/step - loss: 0.3401 - acc: 0.8559 - val_loss: 0.3019 - val_acc: 0.9023\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 609s 39ms/step - loss: 0.3308 - acc: 0.8575 - val_loss: 0.3464 - val_acc: 0.9048\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 604s 39ms/step - loss: 0.3253 - acc: 0.8621 - val_loss: 0.2834 - val_acc: 0.9083\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 607s 39ms/step - loss: 0.3207 - acc: 0.8635 - val_loss: 0.3238 - val_acc: 0.9065\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 609s 39ms/step - loss: 0.3160 - acc: 0.8641 - val_loss: 0.2819 - val_acc: 0.9085\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 608s 39ms/step - loss: 0.3111 - acc: 0.8683 - val_loss: 0.2870 - val_acc: 0.9070\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 617s 39ms/step - loss: 0.3064 - acc: 0.8697 - val_loss: 0.2561 - val_acc: 0.9100\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 631s 40ms/step - loss: 0.3052 - acc: 0.8698 - val_loss: 0.3214 - val_acc: 0.9015\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 630s 40ms/step - loss: 0.3000 - acc: 0.8725 - val_loss: 0.3523 - val_acc: 0.8968\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 610s 39ms/step - loss: 0.2961 - acc: 0.8742 - val_loss: 0.2526 - val_acc: 0.9067\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 615s 39ms/step - loss: 0.2937 - acc: 0.8757 - val_loss: 0.3326 - val_acc: 0.9030\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 614s 39ms/step - loss: 0.2916 - acc: 0.8746 - val_loss: 0.2612 - val_acc: 0.9120\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 611s 39ms/step - loss: 0.2854 - acc: 0.8783 - val_loss: 0.3001 - val_acc: 0.9075\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 610s 39ms/step - loss: 0.2819 - acc: 0.8801 - val_loss: 0.3073 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 613s 39ms/step - loss: 0.2789 - acc: 0.8816 - val_loss: 0.2663 - val_acc: 0.9107\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 627s 40ms/step - loss: 0.2768 - acc: 0.8816 - val_loss: 0.3156 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 612s 39ms/step - loss: 0.2749 - acc: 0.8811 - val_loss: 0.2825 - val_acc: 0.9055\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7959264e90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(3):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=1)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_269/concat:0\", shape=(?, 338, 30), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 429s 27ms/step - loss: 0.6121 - acc: 0.6426 - val_loss: 0.4704 - val_acc: 0.8307\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 412s 26ms/step - loss: 0.4764 - acc: 0.7688 - val_loss: 0.4895 - val_acc: 0.8622\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 416s 27ms/step - loss: 0.4344 - acc: 0.7980 - val_loss: 0.4776 - val_acc: 0.8700\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 414s 26ms/step - loss: 0.3993 - acc: 0.8209 - val_loss: 0.4889 - val_acc: 0.8762\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 408s 26ms/step - loss: 0.3717 - acc: 0.8396 - val_loss: 0.4835 - val_acc: 0.8908\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 417s 27ms/step - loss: 0.3597 - acc: 0.8463 - val_loss: 0.4740 - val_acc: 0.8945\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 416s 27ms/step - loss: 0.3513 - acc: 0.8486 - val_loss: 0.3877 - val_acc: 0.9023\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 415s 27ms/step - loss: 0.3421 - acc: 0.8560 - val_loss: 0.4226 - val_acc: 0.8915\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 414s 26ms/step - loss: 0.3369 - acc: 0.8552 - val_loss: 0.3273 - val_acc: 0.8953\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 411s 26ms/step - loss: 0.3315 - acc: 0.8580 - val_loss: 0.3468 - val_acc: 0.8888\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 411s 26ms/step - loss: 0.3301 - acc: 0.8587 - val_loss: 0.3349 - val_acc: 0.9080\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 418s 27ms/step - loss: 0.3223 - acc: 0.8625 - val_loss: 0.3339 - val_acc: 0.9002\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 418s 27ms/step - loss: 0.3198 - acc: 0.8638 - val_loss: 0.3261 - val_acc: 0.9018\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 418s 27ms/step - loss: 0.3145 - acc: 0.8672 - val_loss: 0.3810 - val_acc: 0.8982\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 418s 27ms/step - loss: 0.3117 - acc: 0.8658 - val_loss: 0.3940 - val_acc: 0.8935\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 412s 26ms/step - loss: 0.3072 - acc: 0.8712 - val_loss: 0.2489 - val_acc: 0.9147\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 414s 26ms/step - loss: 0.3021 - acc: 0.8715 - val_loss: 0.2953 - val_acc: 0.9062\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 417s 27ms/step - loss: 0.2972 - acc: 0.8745 - val_loss: 0.3131 - val_acc: 0.9065\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 418s 27ms/step - loss: 0.2979 - acc: 0.8712 - val_loss: 0.3371 - val_acc: 0.9050\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 416s 27ms/step - loss: 0.2948 - acc: 0.8733 - val_loss: 0.3571 - val_acc: 0.8985\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 410s 26ms/step - loss: 0.2912 - acc: 0.8769 - val_loss: 0.3066 - val_acc: 0.9092\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 426s 27ms/step - loss: 0.2907 - acc: 0.8751 - val_loss: 0.2984 - val_acc: 0.9048\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 425s 27ms/step - loss: 0.2839 - acc: 0.8794 - val_loss: 0.3317 - val_acc: 0.9030\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 416s 27ms/step - loss: 0.2850 - acc: 0.8767 - val_loss: 0.2775 - val_acc: 0.9117\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 409s 26ms/step - loss: 0.2791 - acc: 0.8833 - val_loss: 0.3452 - val_acc: 0.9080\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 413s 26ms/step - loss: 0.2761 - acc: 0.8800 - val_loss: 0.3631 - val_acc: 0.9027\n",
      "Epoch 00026: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f794f84d510>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=1)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_270/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.5975 - acc: 0.6592 - val_loss: 0.5557 - val_acc: 0.7522\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 276s 18ms/step - loss: 0.4691 - acc: 0.7760 - val_loss: 0.5070 - val_acc: 0.8680\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.4291 - acc: 0.8023 - val_loss: 0.4427 - val_acc: 0.8602\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.4036 - acc: 0.8198 - val_loss: 0.4648 - val_acc: 0.8808\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 281s 18ms/step - loss: 0.3848 - acc: 0.8295 - val_loss: 0.4018 - val_acc: 0.8797\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.3695 - acc: 0.8385 - val_loss: 0.4047 - val_acc: 0.8995\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 275s 18ms/step - loss: 0.3581 - acc: 0.8456 - val_loss: 0.4077 - val_acc: 0.9010\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 275s 18ms/step - loss: 0.3473 - acc: 0.8515 - val_loss: 0.4439 - val_acc: 0.8858\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3408 - acc: 0.8533 - val_loss: 0.3298 - val_acc: 0.9012\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.3318 - acc: 0.8580 - val_loss: 0.2683 - val_acc: 0.9083\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 278s 18ms/step - loss: 0.3269 - acc: 0.8613 - val_loss: 0.3216 - val_acc: 0.8947\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 276s 18ms/step - loss: 0.3222 - acc: 0.8645 - val_loss: 0.2735 - val_acc: 0.9105\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 276s 18ms/step - loss: 0.3209 - acc: 0.8641 - val_loss: 0.2489 - val_acc: 0.9127\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 277s 18ms/step - loss: 0.3145 - acc: 0.8674 - val_loss: 0.2784 - val_acc: 0.9100\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3111 - acc: 0.8684 - val_loss: 0.2807 - val_acc: 0.9110\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 278s 18ms/step - loss: 0.3102 - acc: 0.8673 - val_loss: 0.2881 - val_acc: 0.9052\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 277s 18ms/step - loss: 0.3057 - acc: 0.8716 - val_loss: 0.2485 - val_acc: 0.9150\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.3039 - acc: 0.8716 - val_loss: 0.3060 - val_acc: 0.9100\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.3015 - acc: 0.8725 - val_loss: 0.2467 - val_acc: 0.9225\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.2968 - acc: 0.8742 - val_loss: 0.3198 - val_acc: 0.9083\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 281s 18ms/step - loss: 0.2939 - acc: 0.8739 - val_loss: 0.3358 - val_acc: 0.9060\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 281s 18ms/step - loss: 0.2939 - acc: 0.8757 - val_loss: 0.2700 - val_acc: 0.9178\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.2892 - acc: 0.8757 - val_loss: 0.2936 - val_acc: 0.9100\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 277s 18ms/step - loss: 0.2829 - acc: 0.8785 - val_loss: 0.2724 - val_acc: 0.9153\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 274s 17ms/step - loss: 0.2835 - acc: 0.8787 - val_loss: 0.3283 - val_acc: 0.9042\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 277s 18ms/step - loss: 0.2802 - acc: 0.8802 - val_loss: 0.3162 - val_acc: 0.9060\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 278s 18ms/step - loss: 0.2776 - acc: 0.8817 - val_loss: 0.3667 - val_acc: 0.8832\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 278s 18ms/step - loss: 0.2743 - acc: 0.8836 - val_loss: 0.2640 - val_acc: 0.9163\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.2727 - acc: 0.8808 - val_loss: 0.3150 - val_acc: 0.9110\n",
      "Epoch 00029: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f794b0604d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_281/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Tensor(\"lstm_18/transpose_1:0\", shape=(?, ?, 64), dtype=float32)\n",
      "Tensor(\"attention_layer_10/div:0\", shape=(?, 169, 64), dtype=float32)\n",
      "Tensor(\"lstm_18/transpose_1:0\", shape=(?, ?, 64), dtype=float32)\n",
      "Tensor(\"attention_layer_10/mul:0\", shape=(?, 169, 64), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 357s 23ms/step - loss: 0.6843 - acc: 0.5393 - val_loss: 0.6439 - val_acc: 0.6643\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 325s 21ms/step - loss: 0.6268 - acc: 0.6360 - val_loss: 0.5360 - val_acc: 0.7758\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 327s 21ms/step - loss: 0.5172 - acc: 0.7419 - val_loss: 0.5028 - val_acc: 0.8150\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 328s 21ms/step - loss: 0.4672 - acc: 0.7739 - val_loss: 0.4911 - val_acc: 0.8467\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 324s 21ms/step - loss: 0.4377 - acc: 0.7948 - val_loss: 0.5479 - val_acc: 0.7967\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 326s 21ms/step - loss: 0.4168 - acc: 0.8076 - val_loss: 0.4864 - val_acc: 0.8652\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 327s 21ms/step - loss: 0.4000 - acc: 0.8189 - val_loss: 0.4633 - val_acc: 0.8732\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 326s 21ms/step - loss: 0.3862 - acc: 0.8280 - val_loss: 0.4459 - val_acc: 0.8740\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 329s 21ms/step - loss: 0.3784 - acc: 0.8359 - val_loss: 0.4527 - val_acc: 0.8768\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 320s 20ms/step - loss: 0.3660 - acc: 0.8416 - val_loss: 0.4452 - val_acc: 0.8888\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 322s 21ms/step - loss: 0.3615 - acc: 0.8433 - val_loss: 0.4724 - val_acc: 0.8832\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 327s 21ms/step - loss: 0.3608 - acc: 0.8423 - val_loss: 0.4302 - val_acc: 0.8918\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 327s 21ms/step - loss: 0.3531 - acc: 0.8495 - val_loss: 0.4319 - val_acc: 0.8902\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 325s 21ms/step - loss: 0.3489 - acc: 0.8509 - val_loss: 0.4167 - val_acc: 0.8890\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 327s 21ms/step - loss: 0.3466 - acc: 0.8522 - val_loss: 0.4151 - val_acc: 0.8962\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 325s 21ms/step - loss: 0.3417 - acc: 0.8544 - val_loss: 0.4059 - val_acc: 0.8890\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 322s 21ms/step - loss: 0.3418 - acc: 0.8556 - val_loss: 0.4207 - val_acc: 0.8870\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 313s 20ms/step - loss: 0.3377 - acc: 0.8570 - val_loss: 0.3938 - val_acc: 0.8938\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 310s 20ms/step - loss: 0.3375 - acc: 0.8571 - val_loss: 0.3935 - val_acc: 0.8940\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 311s 20ms/step - loss: 0.3387 - acc: 0.8566 - val_loss: 0.3642 - val_acc: 0.8863\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 310s 20ms/step - loss: 0.3349 - acc: 0.8593 - val_loss: 0.3799 - val_acc: 0.8965\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 307s 20ms/step - loss: 0.3335 - acc: 0.8603 - val_loss: 0.3929 - val_acc: 0.8995\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 308s 20ms/step - loss: 0.3306 - acc: 0.8601 - val_loss: 0.3647 - val_acc: 0.8920\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 309s 20ms/step - loss: 0.3304 - acc: 0.8607 - val_loss: 0.4073 - val_acc: 0.8910\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 309s 20ms/step - loss: 0.3284 - acc: 0.8603 - val_loss: 0.3530 - val_acc: 0.8942\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 308s 20ms/step - loss: 0.3248 - acc: 0.8640 - val_loss: 0.3786 - val_acc: 0.8867\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 309s 20ms/step - loss: 0.3258 - acc: 0.8629 - val_loss: 0.4153 - val_acc: 0.8865\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 309s 20ms/step - loss: 0.3268 - acc: 0.8614 - val_loss: 0.3376 - val_acc: 0.8928\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 309s 20ms/step - loss: 0.3254 - acc: 0.8640 - val_loss: 0.3837 - val_acc: 0.8980\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 310s 20ms/step - loss: 0.3233 - acc: 0.8629 - val_loss: 0.4012 - val_acc: 0.8985\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 310s 20ms/step - loss: 0.3227 - acc: 0.8660 - val_loss: 0.3420 - val_acc: 0.8905\n",
      "Epoch 32/50\n",
      "15658/15658 [==============================] - 309s 20ms/step - loss: 0.3213 - acc: 0.8659 - val_loss: 0.3468 - val_acc: 0.8960\n",
      "Epoch 00032: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7922ea6f50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "class Attention_layer(Layer): \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        super(Attention_layer, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        a = K.exp(uit)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        print a\n",
    "        print x\n",
    "        weighted_input = x * a\n",
    "        print weighted_input\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    print lstm_out1\n",
    "    \n",
    "    flatten1 = Attention_layer()(lstm_out1)\n",
    "\n",
    "#     flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_1/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 400s 26ms/step - loss: 0.5868 - acc: 0.6702 - val_loss: 0.5257 - val_acc: 0.8240\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 385s 25ms/step - loss: 0.4574 - acc: 0.7845 - val_loss: 0.5044 - val_acc: 0.8790\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 385s 25ms/step - loss: 0.4181 - acc: 0.8126 - val_loss: 0.4078 - val_acc: 0.8785\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 384s 25ms/step - loss: 0.3869 - acc: 0.8313 - val_loss: 0.4251 - val_acc: 0.8928\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 383s 24ms/step - loss: 0.3670 - acc: 0.8444 - val_loss: 0.3778 - val_acc: 0.9005\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 385s 25ms/step - loss: 0.3561 - acc: 0.8492 - val_loss: 0.3733 - val_acc: 0.9038\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 384s 25ms/step - loss: 0.3457 - acc: 0.8521 - val_loss: 0.3333 - val_acc: 0.9033\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 388s 25ms/step - loss: 0.3401 - acc: 0.8541 - val_loss: 0.3524 - val_acc: 0.9020\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 387s 25ms/step - loss: 0.3331 - acc: 0.8585 - val_loss: 0.3651 - val_acc: 0.8925\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 379s 24ms/step - loss: 0.3290 - acc: 0.8608 - val_loss: 0.2886 - val_acc: 0.9040\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 376s 24ms/step - loss: 0.3260 - acc: 0.8601 - val_loss: 0.3056 - val_acc: 0.9025\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 377s 24ms/step - loss: 0.3237 - acc: 0.8624 - val_loss: 0.2911 - val_acc: 0.9030\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 380s 24ms/step - loss: 0.3207 - acc: 0.8646 - val_loss: 0.2816 - val_acc: 0.9123\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 377s 24ms/step - loss: 0.3171 - acc: 0.8661 - val_loss: 0.3017 - val_acc: 0.9113\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 377s 24ms/step - loss: 0.3145 - acc: 0.8688 - val_loss: 0.3523 - val_acc: 0.8882\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 379s 24ms/step - loss: 0.3112 - acc: 0.8656 - val_loss: 0.3647 - val_acc: 0.8755\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 379s 24ms/step - loss: 0.3090 - acc: 0.8680 - val_loss: 0.2985 - val_acc: 0.9073\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 380s 24ms/step - loss: 0.3043 - acc: 0.8712 - val_loss: 0.2908 - val_acc: 0.9105\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 377s 24ms/step - loss: 0.3034 - acc: 0.8703 - val_loss: 0.3219 - val_acc: 0.8975\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 380s 24ms/step - loss: 0.3019 - acc: 0.8684 - val_loss: 0.3103 - val_acc: 0.9113\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 379s 24ms/step - loss: 0.2993 - acc: 0.8737 - val_loss: 0.3121 - val_acc: 0.9092\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 377s 24ms/step - loss: 0.2985 - acc: 0.8711 - val_loss: 0.2945 - val_acc: 0.9115\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 372s 24ms/step - loss: 0.2925 - acc: 0.8753 - val_loss: 0.3029 - val_acc: 0.9085\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f213e506e10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = Bidirectional(LSTM(64,return_sequences=True))(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_10/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.6134 - acc: 0.6389 - val_loss: 0.5179 - val_acc: 0.7525\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75250, saving model to model.h5\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 283s 18ms/step - loss: 0.4745 - acc: 0.7666 - val_loss: 0.4506 - val_acc: 0.8482\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75250 to 0.84825, saving model to model.h5\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.4330 - acc: 0.7987 - val_loss: 0.3936 - val_acc: 0.8810\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.84825 to 0.88100, saving model to model.h5\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.4052 - acc: 0.8186 - val_loss: 0.4071 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.88100 to 0.88400, saving model to model.h5\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.3800 - acc: 0.8329 - val_loss: 0.3919 - val_acc: 0.8898\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.88400 to 0.88975, saving model to model.h5\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 283s 18ms/step - loss: 0.3654 - acc: 0.8415 - val_loss: 0.3348 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.88975 to 0.89000, saving model to model.h5\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.3497 - acc: 0.8490 - val_loss: 0.2966 - val_acc: 0.9058\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.89000 to 0.90575, saving model to model.h5\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.3409 - acc: 0.8529 - val_loss: 0.3868 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.90575\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 281s 18ms/step - loss: 0.3357 - acc: 0.8617 - val_loss: 0.3912 - val_acc: 0.9045\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.90575\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 278s 18ms/step - loss: 0.3326 - acc: 0.8583 - val_loss: 0.3310 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.90575 to 0.90675, saving model to model.h5\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.3254 - acc: 0.8630 - val_loss: 0.3081 - val_acc: 0.9073\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.90675 to 0.90725, saving model to model.h5\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.3236 - acc: 0.8616 - val_loss: 0.3351 - val_acc: 0.9002\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.90725\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.3207 - acc: 0.8617 - val_loss: 0.3485 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.90725\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 281s 18ms/step - loss: 0.3154 - acc: 0.8644 - val_loss: 0.3229 - val_acc: 0.9040\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.90725\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.3113 - acc: 0.8680 - val_loss: 0.2832 - val_acc: 0.9048\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.90725\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 283s 18ms/step - loss: 0.3083 - acc: 0.8689 - val_loss: 0.2929 - val_acc: 0.9102\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.90725 to 0.91025, saving model to model.h5\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.3052 - acc: 0.8693 - val_loss: 0.2789 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.91025 to 0.91125, saving model to model.h5\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.3023 - acc: 0.8710 - val_loss: 0.3007 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.91125\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.2994 - acc: 0.8720 - val_loss: 0.3597 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.91125\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.2983 - acc: 0.8725 - val_loss: 0.3017 - val_acc: 0.9042\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.91125\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.2967 - acc: 0.8732 - val_loss: 0.3268 - val_acc: 0.9048\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.91125\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 283s 18ms/step - loss: 0.2925 - acc: 0.8769 - val_loss: 0.3330 - val_acc: 0.9098\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.91125\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 283s 18ms/step - loss: 0.2905 - acc: 0.8744 - val_loss: 0.3090 - val_acc: 0.9008\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.91125\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.2890 - acc: 0.8761 - val_loss: 0.3344 - val_acc: 0.9020\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.91125\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.2850 - acc: 0.8792 - val_loss: 0.3114 - val_acc: 0.9102\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.91125\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.2808 - acc: 0.8808 - val_loss: 0.3832 - val_acc: 0.8855\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.91125\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.2816 - acc: 0.8800 - val_loss: 0.2626 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.91125 to 0.91700, saving model to model.h5\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.2788 - acc: 0.8824 - val_loss: 0.3646 - val_acc: 0.9077\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.91700\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.2773 - acc: 0.8824 - val_loss: 0.3226 - val_acc: 0.9075\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.91700\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.2720 - acc: 0.8851 - val_loss: 0.3923 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.91700\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.2714 - acc: 0.8827 - val_loss: 0.3092 - val_acc: 0.9095\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.91700\n",
      "Epoch 32/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.2677 - acc: 0.8852 - val_loss: 0.3092 - val_acc: 0.9075\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.91700\n",
      "Epoch 33/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.2630 - acc: 0.8879 - val_loss: 0.2967 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.91700\n",
      "Epoch 34/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.2634 - acc: 0.8860 - val_loss: 0.3046 - val_acc: 0.9058\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.91700\n",
      "Epoch 35/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.2612 - acc: 0.8903 - val_loss: 0.3349 - val_acc: 0.9033\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.91700\n",
      "Epoch 36/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.2603 - acc: 0.8883 - val_loss: 0.3097 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.91700\n",
      "Epoch 37/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.2537 - acc: 0.8923 - val_loss: 0.4358 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.91700\n",
      "Epoch 00037: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7a8b48ef90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('model.h5', monitor=\"val_acc\", save_best_only=True,save_weights_only=True, verbose=1)\n",
    "\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop,model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_20/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.6106 - acc: 0.6484 - val_loss: 0.4968 - val_acc: 0.7775\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.77750, saving model to model.h5\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 283s 18ms/step - loss: 0.4702 - acc: 0.7727 - val_loss: 0.4317 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.77750 to 0.85575, saving model to model.h5\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.4234 - acc: 0.8068 - val_loss: 0.4516 - val_acc: 0.8825\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.85575 to 0.88250, saving model to model.h5\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 283s 18ms/step - loss: 0.3990 - acc: 0.8212 - val_loss: 0.5414 - val_acc: 0.8445\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.88250\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.3805 - acc: 0.8351 - val_loss: 0.4359 - val_acc: 0.8863\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.88250 to 0.88625, saving model to model.h5\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.3627 - acc: 0.8430 - val_loss: 0.4043 - val_acc: 0.8865\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.88625 to 0.88650, saving model to model.h5\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.3550 - acc: 0.8497 - val_loss: 0.3518 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.88650 to 0.90225, saving model to model.h5\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.3485 - acc: 0.8468 - val_loss: 0.3282 - val_acc: 0.8995\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.90225\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.3376 - acc: 0.8546 - val_loss: 0.3234 - val_acc: 0.9027\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.90225 to 0.90275, saving model to model.h5\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.3363 - acc: 0.8581 - val_loss: 0.3768 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.90275 to 0.90675, saving model to model.h5\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.3297 - acc: 0.8593 - val_loss: 0.3440 - val_acc: 0.9018\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.90675\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 281s 18ms/step - loss: 0.3237 - acc: 0.8612 - val_loss: 0.3811 - val_acc: 0.8982\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.90675\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 280s 18ms/step - loss: 0.3192 - acc: 0.8637 - val_loss: 0.3481 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.90675 to 0.90825, saving model to model.h5\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3139 - acc: 0.8694 - val_loss: 0.3671 - val_acc: 0.9058\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.90825\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3106 - acc: 0.8684 - val_loss: 0.2953 - val_acc: 0.9077\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.90825\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.3071 - acc: 0.8700 - val_loss: 0.3362 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.90825\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.3049 - acc: 0.8705 - val_loss: 0.3267 - val_acc: 0.9065\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.90825\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 279s 18ms/step - loss: 0.3027 - acc: 0.8705 - val_loss: 0.3057 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.90825 to 0.91000, saving model to model.h5\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 278s 18ms/step - loss: 0.2992 - acc: 0.8726 - val_loss: 0.4142 - val_acc: 0.8690\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.91000\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.2967 - acc: 0.8714 - val_loss: 0.3684 - val_acc: 0.9085\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.91000\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.2942 - acc: 0.8758 - val_loss: 0.3132 - val_acc: 0.9120\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.91000 to 0.91200, saving model to model.h5\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.2883 - acc: 0.8769 - val_loss: 0.3445 - val_acc: 0.9040\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.91200\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.2863 - acc: 0.8766 - val_loss: 0.3523 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.91200\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.2852 - acc: 0.8766 - val_loss: 0.3261 - val_acc: 0.9120\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.91200\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 283s 18ms/step - loss: 0.2809 - acc: 0.8826 - val_loss: 0.4432 - val_acc: 0.9090\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.91200\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.2769 - acc: 0.8825 - val_loss: 0.3667 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.91200\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.2758 - acc: 0.8817 - val_loss: 0.4431 - val_acc: 0.8957\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.91200\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.2723 - acc: 0.8814 - val_loss: 0.3868 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.91200\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.2715 - acc: 0.8869 - val_loss: 0.4068 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.91200\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.2673 - acc: 0.8843 - val_loss: 0.3512 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.91200\n",
      "Epoch 31/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.2654 - acc: 0.8893 - val_loss: 0.4146 - val_acc: 0.9048\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.91200\n",
      "Epoch 00031: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7a61ca4e50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('model.h5', monitor=\"val_acc\", save_best_only=True,save_weights_only=True, verbose=1)\n",
    "\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop,model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
