{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.4463e+02 3.5672e-02 5.8109e+00 ... 9.0493e-01 7.8880e-01 3.8257e-01]\n",
      " [3.2388e+02 5.5815e-03 5.5475e+00 ... 6.5424e-01 1.2726e+00 1.1577e+00]\n",
      " [3.5204e+02 1.5560e-02 5.5999e+00 ... 8.7005e-01 7.9320e-01 8.2680e-01]\n",
      " ...\n",
      " [2.7530e+02 8.9307e-02 5.4169e+00 ... 2.5477e+00 2.0142e+00 1.6806e+00]\n",
      " [5.0100e+02 2.8378e-01 6.0655e+00 ... 3.2340e+00 4.7615e+00 3.7687e+00]\n",
      " [4.0921e+02 9.0453e-03 5.8048e+00 ... 1.1404e+00 2.9595e-01 1.2543e+00]]\n"
     ]
    }
   ],
   "source": [
    "dir_name = glob.glob(r'../../00_Kaneki/results/*.csv')\n",
    "all_data=np.zeros((len(dir_name),491))\n",
    "\n",
    "for index, name in enumerate(dir_name):\n",
    "    path = '../../00_Kaneki/results/' + str(index) + '.csv'\n",
    "    data = pd.read_csv(path,header = None)\n",
    "    data1=np.array(data)\n",
    "    all_data[index,:]=data1\n",
    "print all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19658, 491)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('features.npy',all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Convolution1D, Conv2D, MaxPooling2D, MaxPooling1D, LSTM, Embedding\n",
    "from keras.optimizers import SGD\n",
    "import keras.backend as K\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "config_proto = tf.ConfigProto(log_device_placement=0,allow_soft_placement=0)\n",
    "config_proto.gpu_options.allow_growth = True\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../../20_Galaxy/ANGRY/2HXT-train.npy')\n",
    "Y_train = np.load('../../20_Galaxy/ANGRY/2HXT-train_label.npy')\n",
    "\n",
    "X_valid = np.load('../../20_Galaxy/ANGRY/2HXT-val.npy')\n",
    "Y_valid = np.load('../../20_Galaxy/ANGRY/2HXT-val_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "one_hot = preprocessing.OneHotEncoder(sparse = False)\n",
    "y_train = one_hot.fit_transform(Y_train)\n",
    "y_valid = one_hot.fit_transform(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_3/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.6121 - acc: 0.6450 - val_loss: 0.4631 - val_acc: 0.8322\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.4749 - acc: 0.7724 - val_loss: 0.4843 - val_acc: 0.8698\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.4307 - acc: 0.8014 - val_loss: 0.5738 - val_acc: 0.8002\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.3994 - acc: 0.8211 - val_loss: 0.4110 - val_acc: 0.8950\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.3737 - acc: 0.8385 - val_loss: 0.4265 - val_acc: 0.9002\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.3598 - acc: 0.8455 - val_loss: 0.4294 - val_acc: 0.8990\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.3440 - acc: 0.8532 - val_loss: 0.3534 - val_acc: 0.9012\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 283s 18ms/step - loss: 0.3399 - acc: 0.8545 - val_loss: 0.3164 - val_acc: 0.9060\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.3361 - acc: 0.8560 - val_loss: 0.3317 - val_acc: 0.9062\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.3271 - acc: 0.8633 - val_loss: 0.3180 - val_acc: 0.9075\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.3247 - acc: 0.8596 - val_loss: 0.4542 - val_acc: 0.8790\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.3188 - acc: 0.8632 - val_loss: 0.3140 - val_acc: 0.9140\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.3157 - acc: 0.8666 - val_loss: 0.3516 - val_acc: 0.9087\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.3129 - acc: 0.8686 - val_loss: 0.2945 - val_acc: 0.9075\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.3098 - acc: 0.8658 - val_loss: 0.2877 - val_acc: 0.9143\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.3058 - acc: 0.8693 - val_loss: 0.3272 - val_acc: 0.9030\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.3028 - acc: 0.8734 - val_loss: 0.3349 - val_acc: 0.9052\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 283s 18ms/step - loss: 0.3008 - acc: 0.8715 - val_loss: 0.3012 - val_acc: 0.9140\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 283s 18ms/step - loss: 0.2987 - acc: 0.8744 - val_loss: 0.3271 - val_acc: 0.9020\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.2932 - acc: 0.8755 - val_loss: 0.3228 - val_acc: 0.9100\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 284s 18ms/step - loss: 0.2920 - acc: 0.8767 - val_loss: 0.3133 - val_acc: 0.9025\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 283s 18ms/step - loss: 0.2917 - acc: 0.8769 - val_loss: 0.3667 - val_acc: 0.9100\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.2864 - acc: 0.8780 - val_loss: 0.3006 - val_acc: 0.9067\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 282s 18ms/step - loss: 0.2860 - acc: 0.8781 - val_loss: 0.3017 - val_acc: 0.9075\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 281s 18ms/step - loss: 0.2822 - acc: 0.8799 - val_loss: 0.2888 - val_acc: 0.9052\n",
      "Epoch 00025: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e3bd76ad0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut(x, index):\n",
    "    x = x[:,index]\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "#         inputs = Lambda(cut, arguments={'index':i})(model_input)\n",
    "#         print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(model_input)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "xtrain = X_train.reshape(-1,12*3640,1)\n",
    "xvalid = X_valid.reshape(-1,12*3640,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lambda_73/strided_slice:0\", shape=(?, 43680, 1), dtype=float32)\n",
      "Tensor(\"lambda_74/strided_slice:0\", shape=(?, 43680, 1), dtype=float32)\n",
      "Tensor(\"concatenate_51/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Tensor(\"flatten_30/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"flatten_29/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "(15658, 12, 3640)\n",
      "(15658, 43680)\n",
      "(15658, 44171)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 298s 19ms/step - loss: 0.6233 - acc: 0.6280 - val_loss: 0.4762 - val_acc: 0.8135\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 294s 19ms/step - loss: 0.4768 - acc: 0.7706 - val_loss: 0.4614 - val_acc: 0.8450\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 294s 19ms/step - loss: 0.4279 - acc: 0.8046 - val_loss: 0.4517 - val_acc: 0.8530\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 295s 19ms/step - loss: 0.4009 - acc: 0.8208 - val_loss: 0.4494 - val_acc: 0.8900\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 296s 19ms/step - loss: 0.3792 - acc: 0.8358 - val_loss: 0.4605 - val_acc: 0.8640\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 296s 19ms/step - loss: 0.3653 - acc: 0.8439 - val_loss: 0.4493 - val_acc: 0.8660\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 298s 19ms/step - loss: 0.3502 - acc: 0.8504 - val_loss: 0.3193 - val_acc: 0.8975\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 299s 19ms/step - loss: 0.3431 - acc: 0.8539 - val_loss: 0.3561 - val_acc: 0.9065\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.3340 - acc: 0.8566 - val_loss: 0.3591 - val_acc: 0.9035\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.3310 - acc: 0.8571 - val_loss: 0.3591 - val_acc: 0.8972\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 295s 19ms/step - loss: 0.3272 - acc: 0.8606 - val_loss: 0.4314 - val_acc: 0.9045\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 296s 19ms/step - loss: 0.3215 - acc: 0.8629 - val_loss: 0.3705 - val_acc: 0.9075\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 295s 19ms/step - loss: 0.3190 - acc: 0.8646 - val_loss: 0.3742 - val_acc: 0.8965\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.3135 - acc: 0.8655 - val_loss: 0.3869 - val_acc: 0.9075\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 292s 19ms/step - loss: 0.3124 - acc: 0.8686 - val_loss: 0.2992 - val_acc: 0.9117\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.3091 - acc: 0.8691 - val_loss: 0.3423 - val_acc: 0.9092\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 298s 19ms/step - loss: 0.3040 - acc: 0.8730 - val_loss: 0.2738 - val_acc: 0.9115\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 298s 19ms/step - loss: 0.3037 - acc: 0.8700 - val_loss: 0.3833 - val_acc: 0.8882\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.3019 - acc: 0.8707 - val_loss: 0.3258 - val_acc: 0.9048\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.2973 - acc: 0.8763 - val_loss: 0.2879 - val_acc: 0.9127\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.2949 - acc: 0.8742 - val_loss: 0.3435 - val_acc: 0.9083\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.2920 - acc: 0.8745 - val_loss: 0.3614 - val_acc: 0.9055\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.2888 - acc: 0.8781 - val_loss: 0.4007 - val_acc: 0.9075\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.2855 - acc: 0.8794 - val_loss: 0.3082 - val_acc: 0.9055\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 292s 19ms/step - loss: 0.2826 - acc: 0.8795 - val_loss: 0.3785 - val_acc: 0.9062\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 292s 19ms/step - loss: 0.2806 - acc: 0.8804 - val_loss: 0.3571 - val_acc: 0.8880\n",
      "Epoch 27/50\n",
      "15658/15658 [==============================] - 292s 19ms/step - loss: 0.2764 - acc: 0.8827 - val_loss: 0.3920 - val_acc: 0.9070\n",
      "Epoch 28/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.2765 - acc: 0.8802 - val_loss: 0.3492 - val_acc: 0.9067\n",
      "Epoch 29/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.2735 - acc: 0.8848 - val_loss: 0.3439 - val_acc: 0.9095\n",
      "Epoch 30/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.2690 - acc: 0.8843 - val_loss: 0.3592 - val_acc: 0.9095\n",
      "Epoch 00030: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e19141ed0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut1(x):\n",
    "    x = x[:,:12*3640]\n",
    "    return x\n",
    "\n",
    "def cut2(x):\n",
    "    x = x[:,12*3640:]\n",
    "    x = x.reshape(491,1)\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "        inputs = Lambda(cut1)(model_input)\n",
    "        print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    \n",
    "#     feature_list2 = []\n",
    "#     feature_list2.append(flatten1)\n",
    "#     inputs2 = Lambda(cut1)(model_input)\n",
    "#     inputs3 = Flatten()(inputs2)\n",
    "    \n",
    "#     print inputs3\n",
    "#     print flatten1\n",
    "#     feature_list2.append(inputs3)\n",
    "    \n",
    "#     flatten2 = Concatenate(axis=1)(feature_list2)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten1)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS+491,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "print X_train.shape\n",
    "xtrain = X_train.reshape(-1,12*3640)\n",
    "xvalid = X_valid.reshape(-1,12*3640)\n",
    "print xtrain.shape\n",
    "xtrain = pd.concat([pd.DataFrame(xtrain),pd.DataFrame(all_data[:15658])], axis=1)\n",
    "xvalid = pd.concat([pd.DataFrame(xvalid),pd.DataFrame(all_data[15658:])], axis=1)\n",
    "xtrain = np.array(xtrain)\n",
    "xvalid = np.array(xvalid)\n",
    "print xtrain.shape\n",
    "xtrain = xtrain.reshape(-1,12*3640+491,1)\n",
    "xvalid = xvalid.reshape(-1,12*3640+491,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lambda_100/strided_slice:0\", shape=(?, 43680, 1), dtype=float32)\n",
      "Tensor(\"lambda_101/strided_slice:0\", shape=(?, 43680, 1), dtype=float32)\n",
      "Tensor(\"concatenate_69/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Tensor(\"concatenate_70/concat:0\", shape=(?, ?), dtype=float32)\n",
      "(15658, 12, 3640)\n",
      "(15658, 43680)\n",
      "(15658, 44171)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 302s 19ms/step - loss: 0.6915 - acc: 0.5197 - val_loss: 0.6964 - val_acc: 0.5040\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.6717 - acc: 0.5855 - val_loss: 0.6489 - val_acc: 0.6285\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.6041 - acc: 0.6736 - val_loss: 0.5435 - val_acc: 0.7688\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 294s 19ms/step - loss: 0.5060 - acc: 0.7581 - val_loss: 0.4470 - val_acc: 0.8322\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.4447 - acc: 0.7974 - val_loss: 0.4351 - val_acc: 0.8365\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.4020 - acc: 0.8254 - val_loss: 0.4280 - val_acc: 0.8423\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.3656 - acc: 0.8447 - val_loss: 0.4399 - val_acc: 0.8387\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.3356 - acc: 0.8628 - val_loss: 0.3812 - val_acc: 0.8782\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.3104 - acc: 0.8737 - val_loss: 0.4412 - val_acc: 0.8343\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.2896 - acc: 0.8863 - val_loss: 0.3764 - val_acc: 0.8822\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.2755 - acc: 0.8939 - val_loss: 0.3886 - val_acc: 0.8825\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.2636 - acc: 0.9001 - val_loss: 0.4216 - val_acc: 0.8620\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.2462 - acc: 0.9092 - val_loss: 0.4139 - val_acc: 0.8742\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 294s 19ms/step - loss: 0.2365 - acc: 0.9144 - val_loss: 0.4281 - val_acc: 0.8722\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.2254 - acc: 0.9207 - val_loss: 0.4818 - val_acc: 0.8340\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.2136 - acc: 0.9267 - val_loss: 0.4512 - val_acc: 0.8590\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 294s 19ms/step - loss: 0.1987 - acc: 0.9343 - val_loss: 0.4730 - val_acc: 0.8543\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 297s 19ms/step - loss: 0.1880 - acc: 0.9385 - val_loss: 0.5094 - val_acc: 0.8300\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 298s 19ms/step - loss: 0.1807 - acc: 0.9414 - val_loss: 0.5308 - val_acc: 0.8253\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 294s 19ms/step - loss: 0.1662 - acc: 0.9497 - val_loss: 0.4841 - val_acc: 0.8553\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 294s 19ms/step - loss: 0.1583 - acc: 0.9528 - val_loss: 0.5460 - val_acc: 0.8310\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1dfa07a3d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut1(x):\n",
    "    x = x[:,:12*3640]\n",
    "    return x\n",
    "\n",
    "def cut2(x):\n",
    "    x = x[:,12*3640:]\n",
    "    x = x.reshape(491,1)\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "        inputs = Lambda(cut1)(model_input)\n",
    "        print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    \n",
    "    feature_list2 = []\n",
    "    feature_list2.append(flatten1)\n",
    "    inputs2 = Lambda(cut1)(model_input)\n",
    "    inputs3 = Flatten()(inputs2)\n",
    "    \n",
    "#     print inputs3\n",
    "#     print flatten1\n",
    "    feature_list2.append(inputs3)\n",
    "    \n",
    "    flatten2 = Concatenate(axis=1)(feature_list2)\n",
    "    \n",
    "    print flatten2\n",
    "    flatten3 = Dense(8, kernel_initializer='glorot_uniform', activation='softmax')(flatten2)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten3)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS+491,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "print X_train.shape\n",
    "xtrain = X_train.reshape(-1,12*3640)\n",
    "xvalid = X_valid.reshape(-1,12*3640)\n",
    "print xtrain.shape\n",
    "xtrain = pd.concat([pd.DataFrame(xtrain),pd.DataFrame(all_data[:15658])], axis=1)\n",
    "xvalid = pd.concat([pd.DataFrame(xvalid),pd.DataFrame(all_data[15658:])], axis=1)\n",
    "xtrain = np.array(xtrain)\n",
    "xvalid = np.array(xvalid)\n",
    "print xtrain.shape\n",
    "xtrain = xtrain.reshape(-1,12*3640+491,1)\n",
    "xvalid = xvalid.reshape(-1,12*3640+491,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lambda_94/strided_slice:0\", shape=(?, 43680, 1), dtype=float32)\n",
      "Tensor(\"lambda_95/strided_slice:0\", shape=(?, 43680, 1), dtype=float32)\n",
      "Tensor(\"concatenate_65/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Tensor(\"concatenate_66/concat:0\", shape=(?, ?), dtype=float32)\n",
      "(15658, 12, 3640)\n",
      "(15658, 43680)\n",
      "(15658, 44171)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 301s 19ms/step - loss: 0.6917 - acc: 0.5276 - val_loss: 0.6959 - val_acc: 0.5000\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 296s 19ms/step - loss: 0.6900 - acc: 0.5293 - val_loss: 0.6950 - val_acc: 0.4998\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 295s 19ms/step - loss: 0.6880 - acc: 0.5317 - val_loss: 0.6943 - val_acc: 0.4990\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.6842 - acc: 0.5423 - val_loss: 0.6887 - val_acc: 0.5105\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.6640 - acc: 0.5994 - val_loss: 0.6259 - val_acc: 0.6757\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.6193 - acc: 0.6531 - val_loss: 0.5628 - val_acc: 0.7340\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.5437 - acc: 0.7260 - val_loss: 0.4764 - val_acc: 0.8162\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.4668 - acc: 0.7833 - val_loss: 0.4260 - val_acc: 0.8408\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.4162 - acc: 0.8142 - val_loss: 0.4503 - val_acc: 0.8203\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.3771 - acc: 0.8347 - val_loss: 0.3922 - val_acc: 0.8635\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.3424 - acc: 0.8553 - val_loss: 0.3825 - val_acc: 0.8680\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.3192 - acc: 0.8687 - val_loss: 0.3803 - val_acc: 0.8750\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.2995 - acc: 0.8782 - val_loss: 0.3803 - val_acc: 0.8795\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 292s 19ms/step - loss: 0.2896 - acc: 0.8825 - val_loss: 0.4401 - val_acc: 0.8538\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 292s 19ms/step - loss: 0.2704 - acc: 0.8944 - val_loss: 0.4344 - val_acc: 0.8413\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.2596 - acc: 0.8992 - val_loss: 0.4433 - val_acc: 0.8553\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.2447 - acc: 0.9071 - val_loss: 0.4272 - val_acc: 0.8582\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 295s 19ms/step - loss: 0.2265 - acc: 0.9156 - val_loss: 0.5282 - val_acc: 0.8177\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 295s 19ms/step - loss: 0.2186 - acc: 0.9207 - val_loss: 0.4390 - val_acc: 0.8627\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.1996 - acc: 0.9304 - val_loss: 0.4347 - val_acc: 0.8713\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.1898 - acc: 0.9351 - val_loss: 0.4703 - val_acc: 0.8520\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.1778 - acc: 0.9400 - val_loss: 0.4547 - val_acc: 0.8602\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 294s 19ms/step - loss: 0.1658 - acc: 0.9448 - val_loss: 0.5148 - val_acc: 0.8477\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1df7efbd50>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut1(x):\n",
    "    x = x[:,:12*3640]\n",
    "    return x\n",
    "\n",
    "def cut2(x):\n",
    "    x = x[:,12*3640:]\n",
    "    x = x.reshape(491,1)\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "        inputs = Lambda(cut1)(model_input)\n",
    "        print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    \n",
    "    feature_list2 = []\n",
    "    feature_list2.append(flatten1)\n",
    "    inputs2 = Lambda(cut1)(model_input)\n",
    "    inputs3 = Flatten()(inputs2)\n",
    "    \n",
    "#     print inputs3\n",
    "#     print flatten1\n",
    "    feature_list2.append(inputs3)\n",
    "    \n",
    "    flatten2 = Concatenate(axis=1)(feature_list2)\n",
    "    \n",
    "    print flatten2\n",
    "    flatten3 = Dense(32, kernel_initializer='glorot_uniform', activation='softmax')(flatten2)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten3)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS+491,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "print X_train.shape\n",
    "xtrain = X_train.reshape(-1,12*3640)\n",
    "xvalid = X_valid.reshape(-1,12*3640)\n",
    "print xtrain.shape\n",
    "xtrain = pd.concat([pd.DataFrame(xtrain),pd.DataFrame(all_data[:15658])], axis=1)\n",
    "xvalid = pd.concat([pd.DataFrame(xvalid),pd.DataFrame(all_data[15658:])], axis=1)\n",
    "xtrain = np.array(xtrain)\n",
    "xvalid = np.array(xvalid)\n",
    "print xtrain.shape\n",
    "xtrain = xtrain.reshape(-1,12*3640+491,1)\n",
    "xvalid = xvalid.reshape(-1,12*3640+491,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lambda_103/strided_slice:0\", shape=(?, 43680, 1), dtype=float32)\n",
      "Tensor(\"lambda_104/strided_slice:0\", shape=(?, 43680, 1), dtype=float32)\n",
      "Tensor(\"concatenate_71/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Tensor(\"concatenate_72/concat:0\", shape=(?, ?), dtype=float32)\n",
      "(15658, 12, 3640)\n",
      "(15658, 43680)\n",
      "(15658, 44171)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 302s 19ms/step - loss: 0.6959 - acc: 0.5206 - val_loss: 0.6891 - val_acc: 0.5160\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.6616 - acc: 0.5823 - val_loss: 0.6770 - val_acc: 0.5595\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 294s 19ms/step - loss: 0.5841 - acc: 0.6966 - val_loss: 0.5201 - val_acc: 0.7863\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 294s 19ms/step - loss: 0.4919 - acc: 0.7696 - val_loss: 0.4896 - val_acc: 0.8008\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.4426 - acc: 0.7975 - val_loss: 0.4696 - val_acc: 0.8147\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 292s 19ms/step - loss: 0.4064 - acc: 0.8241 - val_loss: 0.4490 - val_acc: 0.8402\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.3783 - acc: 0.8401 - val_loss: 0.4333 - val_acc: 0.8482\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 292s 19ms/step - loss: 0.3451 - acc: 0.8598 - val_loss: 0.4129 - val_acc: 0.8618\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.3224 - acc: 0.8706 - val_loss: 0.4213 - val_acc: 0.8562\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.3098 - acc: 0.8792 - val_loss: 0.3942 - val_acc: 0.8735\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.2887 - acc: 0.8895 - val_loss: 0.4169 - val_acc: 0.8637\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.2815 - acc: 0.8929 - val_loss: 0.4313 - val_acc: 0.8602\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.2636 - acc: 0.9033 - val_loss: 0.4308 - val_acc: 0.8618\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.2594 - acc: 0.9048 - val_loss: 0.4419 - val_acc: 0.8622\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.2484 - acc: 0.9105 - val_loss: 0.4424 - val_acc: 0.8598\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.2401 - acc: 0.9166 - val_loss: 0.4676 - val_acc: 0.8608\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.2334 - acc: 0.9188 - val_loss: 0.4782 - val_acc: 0.8553\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.2272 - acc: 0.9213 - val_loss: 0.4830 - val_acc: 0.8562\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 295s 19ms/step - loss: 0.2205 - acc: 0.9241 - val_loss: 0.4670 - val_acc: 0.8602\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.2132 - acc: 0.9285 - val_loss: 0.5183 - val_acc: 0.8435\n",
      "Epoch 00020: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e36f331d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut1(x):\n",
    "    x = x[:,:12*3640]\n",
    "    return x\n",
    "\n",
    "def cut2(x):\n",
    "    x = x[:,12*3640:]\n",
    "    x = x.reshape(491,1)\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "        inputs = Lambda(cut1)(model_input)\n",
    "        print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    \n",
    "    feature_list2 = []\n",
    "    feature_list2.append(flatten1)\n",
    "    inputs2 = Lambda(cut1)(model_input)\n",
    "    inputs3 = Flatten()(inputs2)\n",
    "    \n",
    "#     print inputs3\n",
    "#     print flatten1\n",
    "    feature_list2.append(inputs3)\n",
    "    \n",
    "    flatten2 = Concatenate(axis=1)(feature_list2)\n",
    "    \n",
    "    print flatten2\n",
    "    flatten3 = Dense(4, kernel_initializer='glorot_uniform', activation='softmax')(flatten2)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten3)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS+491,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "print X_train.shape\n",
    "xtrain = X_train.reshape(-1,12*3640)\n",
    "xvalid = X_valid.reshape(-1,12*3640)\n",
    "print xtrain.shape\n",
    "xtrain = pd.concat([pd.DataFrame(xtrain),pd.DataFrame(all_data[:15658])], axis=1)\n",
    "xvalid = pd.concat([pd.DataFrame(xvalid),pd.DataFrame(all_data[15658:])], axis=1)\n",
    "xtrain = np.array(xtrain)\n",
    "xvalid = np.array(xvalid)\n",
    "print xtrain.shape\n",
    "xtrain = xtrain.reshape(-1,12*3640+491,1)\n",
    "xvalid = xvalid.reshape(-1,12*3640+491,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lambda_127/strided_slice:0\", shape=(?, 43680, 1), dtype=float32)\n",
      "Tensor(\"lambda_128/strided_slice:0\", shape=(?, 43680, 1), dtype=float32)\n",
      "Tensor(\"concatenate_87/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Tensor(\"concatenate_88/concat:0\", shape=(?, ?), dtype=float32)\n",
      "(15658, 12, 3640)\n",
      "(15658, 43680)\n",
      "(15658, 44171)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 300s 19ms/step - loss: 0.6917 - acc: 0.5273 - val_loss: 0.6936 - val_acc: 0.4947\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.6789 - acc: 0.5605 - val_loss: 0.6866 - val_acc: 0.5312\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.6405 - acc: 0.6282 - val_loss: 0.6037 - val_acc: 0.6777\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.5614 - acc: 0.7112 - val_loss: 0.4961 - val_acc: 0.8037\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.4816 - acc: 0.7730 - val_loss: 0.4593 - val_acc: 0.8143\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.4228 - acc: 0.8094 - val_loss: 0.4908 - val_acc: 0.7943\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.3894 - acc: 0.8260 - val_loss: 0.4237 - val_acc: 0.8390\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.3564 - acc: 0.8449 - val_loss: 0.3833 - val_acc: 0.8718\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.3252 - acc: 0.8650 - val_loss: 0.3983 - val_acc: 0.8648\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.3119 - acc: 0.8711 - val_loss: 0.3789 - val_acc: 0.8793\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.2958 - acc: 0.8804 - val_loss: 0.3759 - val_acc: 0.8808\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.2807 - acc: 0.8856 - val_loss: 0.3950 - val_acc: 0.8688\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 292s 19ms/step - loss: 0.2634 - acc: 0.8972 - val_loss: 0.4218 - val_acc: 0.8652\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 292s 19ms/step - loss: 0.2475 - acc: 0.9055 - val_loss: 0.4206 - val_acc: 0.8598\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.2280 - acc: 0.9158 - val_loss: 0.4677 - val_acc: 0.8297\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.2123 - acc: 0.9239 - val_loss: 0.4410 - val_acc: 0.8560\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 292s 19ms/step - loss: 0.1979 - acc: 0.9310 - val_loss: 0.4617 - val_acc: 0.8462\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.1835 - acc: 0.9368 - val_loss: 0.4759 - val_acc: 0.8475\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.1763 - acc: 0.9400 - val_loss: 0.4889 - val_acc: 0.8322\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.1676 - acc: 0.9446 - val_loss: 0.5129 - val_acc: 0.8273\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.1578 - acc: 0.9492 - val_loss: 0.4824 - val_acc: 0.8505\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d2161cd50>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut1(x):\n",
    "    x = x[:,:12*3640]\n",
    "    return x\n",
    "\n",
    "def cut2(x):\n",
    "    x = x[:,12*3640:]\n",
    "    x = x.reshape(491,1)\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "        inputs = Lambda(cut1)(model_input)\n",
    "        print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    \n",
    "    feature_list2 = []\n",
    "    feature_list2.append(flatten1)\n",
    "    inputs2 = Lambda(cut1)(model_input)\n",
    "    inputs3 = Flatten()(inputs2)\n",
    "    \n",
    "#     print inputs3\n",
    "#     print flatten1\n",
    "    feature_list2.append(inputs3)\n",
    "    \n",
    "    flatten2 = Concatenate(axis=1)(feature_list2)\n",
    "    \n",
    "    print flatten2\n",
    "    flatten3 = Dense(12, kernel_initializer='glorot_uniform', activation='softmax')(flatten2)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten3)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS+491,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "print X_train.shape\n",
    "xtrain = X_train.reshape(-1,12*3640)\n",
    "xvalid = X_valid.reshape(-1,12*3640)\n",
    "print xtrain.shape\n",
    "xtrain = pd.concat([pd.DataFrame(xtrain),pd.DataFrame(all_data[:15658])], axis=1)\n",
    "xvalid = pd.concat([pd.DataFrame(xvalid),pd.DataFrame(all_data[15658:])], axis=1)\n",
    "xtrain = np.array(xtrain)\n",
    "xvalid = np.array(xvalid)\n",
    "print xtrain.shape\n",
    "xtrain = xtrain.reshape(-1,12*3640+491,1)\n",
    "xvalid = xvalid.reshape(-1,12*3640+491,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lambda_130/strided_slice:0\", shape=(?, 43680, 1), dtype=float32)\n",
      "Tensor(\"lambda_131/strided_slice:0\", shape=(?, 43680, 1), dtype=float32)\n",
      "Tensor(\"concatenate_89/concat:0\", shape=(?, 169, 60), dtype=float32)\n",
      "Tensor(\"concatenate_90/concat:0\", shape=(?, ?), dtype=float32)\n",
      "(15658, 12, 3640)\n",
      "(15658, 43680)\n",
      "(15658, 44171)\n",
      "Train on 15658 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15658/15658 [==============================] - 296s 19ms/step - loss: 0.6917 - acc: 0.5213 - val_loss: 0.6950 - val_acc: 0.4938\n",
      "Epoch 2/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.6867 - acc: 0.5334 - val_loss: 0.6945 - val_acc: 0.4925\n",
      "Epoch 3/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.6819 - acc: 0.5526 - val_loss: 0.6939 - val_acc: 0.5035\n",
      "Epoch 4/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.6755 - acc: 0.5730 - val_loss: 0.6893 - val_acc: 0.5108\n",
      "Epoch 5/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.6639 - acc: 0.6025 - val_loss: 0.6767 - val_acc: 0.5587\n",
      "Epoch 6/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.6400 - acc: 0.6415 - val_loss: 0.6322 - val_acc: 0.6492\n",
      "Epoch 7/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.5848 - acc: 0.6989 - val_loss: 0.5198 - val_acc: 0.7837\n",
      "Epoch 8/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.5070 - acc: 0.7613 - val_loss: 0.4716 - val_acc: 0.8130\n",
      "Epoch 9/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.4646 - acc: 0.7890 - val_loss: 0.4501 - val_acc: 0.8233\n",
      "Epoch 10/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.4333 - acc: 0.8087 - val_loss: 0.4594 - val_acc: 0.8123\n",
      "Epoch 11/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.4056 - acc: 0.8246 - val_loss: 0.4404 - val_acc: 0.8260\n",
      "Epoch 12/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.3830 - acc: 0.8368 - val_loss: 0.5177 - val_acc: 0.7995\n",
      "Epoch 13/50\n",
      "15658/15658 [==============================] - 292s 19ms/step - loss: 0.3603 - acc: 0.8497 - val_loss: 0.3932 - val_acc: 0.8615\n",
      "Epoch 14/50\n",
      "15658/15658 [==============================] - 293s 19ms/step - loss: 0.3396 - acc: 0.8624 - val_loss: 0.3855 - val_acc: 0.8668\n",
      "Epoch 15/50\n",
      "15658/15658 [==============================] - 291s 19ms/step - loss: 0.3251 - acc: 0.8693 - val_loss: 0.3941 - val_acc: 0.8620\n",
      "Epoch 16/50\n",
      "15658/15658 [==============================] - 290s 19ms/step - loss: 0.3072 - acc: 0.8818 - val_loss: 0.3878 - val_acc: 0.8672\n",
      "Epoch 17/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.2955 - acc: 0.8868 - val_loss: 0.3914 - val_acc: 0.8665\n",
      "Epoch 18/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.2803 - acc: 0.8955 - val_loss: 0.3964 - val_acc: 0.8633\n",
      "Epoch 19/50\n",
      "15658/15658 [==============================] - 285s 18ms/step - loss: 0.2685 - acc: 0.8992 - val_loss: 0.4046 - val_acc: 0.8575\n",
      "Epoch 20/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.2586 - acc: 0.9055 - val_loss: 0.4021 - val_acc: 0.8627\n",
      "Epoch 21/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.2491 - acc: 0.9108 - val_loss: 0.3956 - val_acc: 0.8655\n",
      "Epoch 22/50\n",
      "15658/15658 [==============================] - 287s 18ms/step - loss: 0.2378 - acc: 0.9172 - val_loss: 0.4225 - val_acc: 0.8540\n",
      "Epoch 23/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.2282 - acc: 0.9195 - val_loss: 0.4070 - val_acc: 0.8637\n",
      "Epoch 24/50\n",
      "15658/15658 [==============================] - 286s 18ms/step - loss: 0.2218 - acc: 0.9239 - val_loss: 0.4236 - val_acc: 0.8590\n",
      "Epoch 25/50\n",
      "15658/15658 [==============================] - 289s 18ms/step - loss: 0.2120 - acc: 0.9275 - val_loss: 0.4521 - val_acc: 0.8425\n",
      "Epoch 26/50\n",
      "15658/15658 [==============================] - 288s 18ms/step - loss: 0.2085 - acc: 0.9311 - val_loss: 0.4377 - val_acc: 0.8510\n",
      "Epoch 00026: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f22ec21a950>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def cut1(x):\n",
    "    x = x[:,:12*3640]\n",
    "    return x\n",
    "\n",
    "def cut2(x):\n",
    "    x = x[:,12*3640:]\n",
    "    x = x.reshape(491,1)\n",
    "    return x\n",
    "\n",
    "def createDenseNet(nb_classes, img_dim):\n",
    "\n",
    "    feature_list = []\n",
    "    \n",
    "    model_input = Input(shape=img_dim)\n",
    "    \n",
    "    for i in range(2):\n",
    "        inputs = Lambda(cut1)(model_input)\n",
    "        print inputs\n",
    "#         print model_input\n",
    "\n",
    "        cnn1 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(inputs)\n",
    "        cnn2 = Convolution1D(36, (21), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn1)\n",
    "        pool1 = MaxPooling1D(pool_size=(7))(cnn2)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "\n",
    "#         drop1 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop1)\n",
    "        \n",
    "        cnn3 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop1)\n",
    "        cnn4 = Convolution1D(42, (13), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn3)\n",
    "        pool2 = MaxPooling1D(pool_size=(6))(cnn4)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "#         drop2 = BatchNormalization(axis=1, gamma_regularizer=l2(1E-4),beta_regularizer=l2(1E-4))(drop2)\n",
    "        \n",
    "        cnn5 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(drop2)\n",
    "        cnn6 = Convolution1D(30, (9), kernel_initializer='glorot_uniform', \n",
    "                      activation=keras.layers.advanced_activations.LeakyReLU(alpha=0.2))(cnn5)\n",
    "        pool3 = MaxPooling1D(pool_size=(6))(cnn6)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "#         print drop3\n",
    "        feature_list.append(drop3)\n",
    "        \n",
    "    concat = Concatenate(axis=2)(feature_list)\n",
    "    print concat\n",
    "    \n",
    "    lstm_out1 = LSTM(64,return_sequences=True)(concat)\n",
    "    \n",
    "#     flatten1 = Attention_layer(lstm_out1)\n",
    "    \n",
    "    flatten1 = Flatten()(lstm_out1)\n",
    "    \n",
    "    \n",
    "    feature_list2 = []\n",
    "    feature_list2.append(flatten1)\n",
    "    inputs2 = Lambda(cut1)(model_input)\n",
    "    inputs3 = Flatten()(inputs2)\n",
    "    \n",
    "#     print inputs3\n",
    "#     print flatten1\n",
    "    feature_list2.append(inputs3)\n",
    "    \n",
    "    flatten2 = Concatenate(axis=1)(feature_list2)\n",
    "    \n",
    "    print flatten2\n",
    "    flatten3 = Dense(12, kernel_initializer='glorot_uniform', activation='softmax')(flatten2)\n",
    "    \n",
    "    x = Dense(nb_classes, kernel_initializer='glorot_uniform', activation='softmax')(flatten3)\n",
    "\n",
    "    densenet = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "    return densenet\n",
    "\n",
    "#define DenseNet parms\n",
    "ROWS = 12\n",
    "COLS = 3640\n",
    "CHANNELS = 1\n",
    "nb_classes = 2\n",
    "batch_size = 32\n",
    "nb_epoch = 40\n",
    "img_dim = (ROWS*COLS+491,CHANNELS)\n",
    "\n",
    "model = createDenseNet(nb_classes=nb_classes,img_dim=img_dim)\n",
    "\n",
    "sgd = SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
    "print X_train.shape\n",
    "xtrain = X_train.reshape(-1,12*3640)\n",
    "xvalid = X_valid.reshape(-1,12*3640)\n",
    "print xtrain.shape\n",
    "xtrain = pd.concat([pd.DataFrame(xtrain),pd.DataFrame(all_data[:15658])], axis=1)\n",
    "xvalid = pd.concat([pd.DataFrame(xvalid),pd.DataFrame(all_data[15658:])], axis=1)\n",
    "xtrain = np.array(xtrain)\n",
    "xvalid = np.array(xvalid)\n",
    "print xtrain.shape\n",
    "xtrain = xtrain.reshape(-1,12*3640+491,1)\n",
    "xvalid = xvalid.reshape(-1,12*3640+491,1)\n",
    "model.fit(xtrain, y_train, batch_size=batch_size, epochs=50,\n",
    "         validation_data=(xvalid,y_valid),\n",
    "         callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
